{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter: RAGAS: Automated Evaluation of Retrieval Augmented Generation\n",
      "Shahul Es † , Jithin James † , Luis Espinosa-Anke ∗♢ , Steven Schockaert ∗ † Exploding Gradients ∗ CardiffNLP, Cardiff University, United Kingdom ♢ AMPLYFI, United Kingdom shahules786@gmail.com,jamesjithin97@gmail.com {espinosa-ankel,schockaerts1}@cardiff.ac.uk\n",
      "\n",
      "Chapter: Abstract\n",
      "We introduce RAGA S ( R etrieval A ugmented G eneration As sessment), a framework for reference-free evaluation of Retrieval Aug- mented Generation (RAG) pipelines.\n",
      "RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natu- ral language layer between a user and textual databases, reducing the risk of hallucinations.\n",
      "Evaluating RAG architectures is, however, chal- lenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the gener- ation itself.\n",
      "\n",
      "Chapter: Abstract\n",
      "With RAGA S , we put forward a suite of metrics which can be used to evaluate these different dimensions without having to rely on ground truth human annotations .\n",
      "\n",
      "Chapter: Abstract\n",
      "We posit that such a framework can crucially con- tribute to faster evaluation cycles of RAG archi- tectures, which is especially important given the fast adoption of LLMs.\n",
      "1\n",
      "\n",
      "Chapter: Introduction\n",
      "Language Models (LMs) capture a vast amount of knowledge about the world, which allows them to answer questions without accessing any exter- nal sources.\n",
      "\n",
      "Chapter: Introduction\n",
      "This idea of LMs as repositories of knowledge emerged shortly after the introduction of BERT ( Devlin et al.\n",
      ", 2019 ) and became more firmly established with the introduction of ever larger LMs ( Roberts et al.\n",
      ", 2020 ).\n",
      "\n",
      "Chapter: Introduction\n",
      "While the most recent Large Language Models (LLMs) capture enough knowledge to rival human performance across a wide variety of question answering bench- marks ( Bubeck et al.\n",
      ", 2023 ), the idea of using LLMs as knowledge bases still has two fundamen- tal limitations.\n",
      "\n",
      "Chapter: Introduction\n",
      "First, LLMs are not able to answer questions about events that have happened after they were trained.\n",
      "\n",
      "Chapter: Introduction\n",
      "Second, even the largest models struggle to memorise knowledge that is only rarely mentioned in the training corpus ( Kandpal et al.\n",
      ", 2022 ; Mallen et al.\n",
      ", 2023 ).\n",
      "\n",
      "Chapter: Introduction\n",
      "The standard solution to these issues is to rely on Retrieval Augmented Generation (RAG) ( Lee et al.\n",
      ", 2019 ; Lewis et al.\n",
      ", 2020 ; Guu et al.\n",
      ", 2020 ).\n",
      "\n",
      "Chapter: Introduction\n",
      "Answering a question then essentially involves retrieving relevant pas- sages from a corpus and feeding these passages, along with the original question, to the LM.\n",
      "\n",
      "Chapter: Introduction\n",
      "While initial approaches relied on specialised LMs for retrieval-augmented language modelling ( Khandel- wal et al.\n",
      ", 2020 ; Borgeaud et al.\n",
      ", 2022 ), recent work has suggested that simply adding retrieved docu- ments to the input of a standard LM can also work well ( Khattab et al.\n",
      ", 2022 ; Ram et al.\n",
      ", 2023 ; Shi et al.\n",
      ", 2023 ), thus making it possible to use retrieval- augmented strategies in combination with LLMs that are only available through APIs.\n",
      "While the usefulness of retrieval-augmented strategies is clear, their implementation requires a significant amount of tuning, as the overall per- formance will be affected by the retrieval model, the considered corpus, the LM, or the prompt for- mulation, among others.\n",
      "\n",
      "Chapter: Introduction\n",
      "Automated evaluation of retrieval-augmented systems is thus paramount.\n",
      "\n",
      "Chapter: Introduction\n",
      "In practice, RAG systems are often evaluated in terms of the language modelling task itself, i.e.\n",
      "\n",
      "Chapter: Introduction\n",
      "by mea- suring perplexity on some reference corpus.\n",
      "\n",
      "Chapter: Introduction\n",
      "How- ever, such evaluations are not always predictive of downstream performance ( Wang et al.\n",
      ", 2023c ).\n",
      "Moreover, this evaluation strategy relies on the LM probabilities, which are not accessible for some closed models (e.g.\n",
      "\n",
      "Chapter: Introduction\n",
      "ChatGPT and GPT-4).\n",
      "\n",
      "Chapter: Introduction\n",
      "Ques- tion answering is another common evaluation task, but usually only datasets with short extractive an- swers are considered, which may not be represen- tative of how the system will be used.\n",
      "To address these issues, in this paper we present RAGA S 1 , a framework for the automated assess- 1 RAGA S is available at https://github.com/ explodinggradients/ragas .\n",
      "# arXiv:2309.15217v1  [cs.CL]  26 Sep 2023ment of retrieval augmented generation systems.\n",
      "We focus on settings where reference answers may not be available, and where we want to estimate different proxies for correctness, in addition to the usefulness of the retrieved passages.\n",
      "\n",
      "Chapter: Introduction\n",
      "The RAGA S framework provides an integration with both llama- index and Langchain , the most widely used frame- works for building RAG solutions, thus enabling developers to easily integrate RAGA S into their standard workflow.\n",
      "\n",
      "Chapter: 2 Related Work\n",
      "Estimating faithfulness using LLMs The prob- lem of detecting hallucinations in LLM generated responses has been extensively studied ( Ji et al.\n",
      ", 2023 ).\n",
      "\n",
      "Chapter: 2 Related Work\n",
      "Several authors have suggested the idea of predicting factuality using a few-shot prompt- ing strategy ( Zhang et al.\n",
      ", 2023 ).\n",
      "\n",
      "Chapter: 2 Related Work\n",
      "Recent analy- ses, however, suggest that existing models struggle with detecting hallucination when using standard prompting strategies ( Li et al.\n",
      ", 2023 ; Azaria and Mitchell , 2023 ).\n",
      "\n",
      "Chapter: 2 Related Work\n",
      "Other approaches rely on linking the generated responses to facts from an external knowledge base ( Min et al.\n",
      ", 2023 ), but this is not always possible.\n",
      "Yet another strategy is to inspect the probabili- ties assigned to individual tokens, where we would expect the model to be less confident in halluci- nated answers than in factual ones.\n",
      "\n",
      "Chapter: 2 Related Work\n",
      "For instance, BARTScore ( Yuan et al.\n",
      ", 2021 ) estimates factuality by looking at the conditional probability of the gen- erated text given the input.\n",
      "Kadavath et al.\n",
      "( 2022 ) use a variation of this idea.\n",
      "\n",
      "Chapter: 2 Related Work\n",
      "Starting from the ob- servation that LLMs provide well-calibrated proba- bilities when answering multiple-choice questions, they essentially convert the problem of validating model generated answers into a multiple-choice question which asks whether the answer is true or false.\n",
      "\n",
      "Chapter: 2 Related Work\n",
      "Rather than looking at the output probabil- ities, Azaria and Mitchell ( 2023 ) propose to train a supervised classifier on the weights from one of the hidden layers of the LLM, to predict whether a given statement is true or not.\n",
      "\n",
      "Chapter: 2 Related Work\n",
      "While the approach performs well, the need to access the hidden states of the model makes it unsuitable for systems that access LLMs through an API.\n",
      "For models that do not provide access to token probabilities, such as ChatGPT and GPT-4, differ- ent methods are needed.\n",
      "\n",
      "Chapter: 2 Related Work\n",
      "SelfCheckGPT ( Manakul et al.\n",
      ", 2023 ) addresses this problem by instead sam- pling multiple answers.\n",
      "\n",
      "Chapter: 2 Related Work\n",
      "Their core idea is that factual answers are more stable: when an answer is factual, we can expect that different samples will tend to be semantically similar, whereas this is less likely to be the case for hallucinated answers.\n",
      "Automated evaluation of text generation systems LLMs have also been leveraged to automatically evaluate other aspects of generated text fragments, beyond factuality.\n",
      "\n",
      "Chapter: 2 Related Work\n",
      "For instance, GPTScore ( Fu et al.\n",
      ", 2023 ) uses a prompt that specifies the consid- ered aspect (e.g.\n",
      "\n",
      "Chapter: 2 Related Work\n",
      "fluency) and then scores passages based on the average probability of the generated tokens, according to a given autoregressive LM.\n",
      "This idea of using prompts was previously also considered by Yuan et al.\n",
      "( 2021 ), although they used a smaller fine-tuned LM (i.e.\n",
      "\n",
      "Chapter: 2 Related Work\n",
      "BART) and did not observe a clear benefit from using prompts.\n",
      "\n",
      "Chapter: 2 Related Work\n",
      "An- other approach directly asks ChatGPT to evaluate a particular aspect of the given answer by provid- ing a score between 0 and 100, or by providing a rating on a 5-star scale ( Wang et al.\n",
      ", 2023a ).\n",
      "\n",
      "Chapter: 2 Related Work\n",
      "Re- markably, strong results can be obtained in this way, although it comes with the limitation of being sensitive to the design of the prompt.\n",
      "\n",
      "Chapter: 2 Related Work\n",
      "Rather than scoring individual answers, some authors have also focused on using an LLM to select the best answer among a number of candidates ( Wang et al.\n",
      ", 2023b ), typically to compare the performance of different LLMs.\n",
      "\n",
      "Chapter: 2 Related Work\n",
      "However, care is needed with this approach, as the order in which the answers is presented can influence the result ( Wang et al.\n",
      ", 2023b ).\n",
      "In terms of how ground truth answers or, more generally, generations, have been typically used in the literature, most approaches have relied on the availability of one or more reference answers.\n",
      "For instance, BERTScore ( Zhang et al.\n",
      ", 2020 ) and MoverScore ( Zhao et al.\n",
      ", 2019 ) use contex- tualised embeddings, produced by a pre-trained BERT model, to compare the similarity between the generated answer and the reference answers.\n",
      "BARTScore ( Yuan et al.\n",
      ", 2021 ) similarly uses refer- ence answers to compute aspects such as precision (estimated as the probability of generating the gen- erated answer given the reference) and recall (esti- mated as the probability of generating the reference given the generated answer).\n",
      "\n",
      "Chapter: 3 Evaluation Strategies\n",
      "We consider a standard RAG setting, where given a question q , the system first retrieves some context c ( q ) and then uses the retrieved context to generate an answer a s ( q ) .\n",
      "\n",
      "Chapter: 3 Evaluation Strategies\n",
      "When building a RAG system,we usually do not have access to human-annotated datasets or reference answers.\n",
      "\n",
      "Chapter: 3 Evaluation Strategies\n",
      "We therefore fo- cus on metrics that are fully self-contained and reference-free.\n",
      "\n",
      "Chapter: 3 Evaluation Strategies\n",
      "We focus in particular three quality aspects, which we argue are of central importance.\n",
      "First, Faithfulness refers to the idea that the an- swer should be grounded in the given context.\n",
      "\n",
      "Chapter: 3 Evaluation Strategies\n",
      "This is important to avoid hallucinations, and to ensure that the retrieved context can act as a justification for the generated answer.\n",
      "\n",
      "Chapter: 3 Evaluation Strategies\n",
      "Indeed, RAG systems are often used in applications where the factual con- sistency of the generated text w.r.t.\n",
      "\n",
      "Chapter: 3 Evaluation Strategies\n",
      "the grounded sources is highly important, e.g.\n",
      "\n",
      "Chapter: 3 Evaluation Strategies\n",
      "in domains such as law, where information is constantly evolving.\n",
      "\n",
      "Chapter: 3 Evaluation Strategies\n",
      "Sec- ond, Answer Relevance refers to the idea that the generated answer should address the actual ques- tion that was provided.\n",
      "\n",
      "Chapter: 3 Evaluation Strategies\n",
      "Finally,\n",
      "\n",
      "Chapter: 4 The WikiEval Dataset\n",
      "Relevance refers to the idea that the retrieved context should be focused, containing as little irrelevant informa- tion as possible.\n",
      "\n",
      "Chapter: 4 The WikiEval Dataset\n",
      "This is important given the cost associated with feeding long context passages to LLMs.\n",
      "\n",
      "Chapter: 4 The WikiEval Dataset\n",
      "Moreover, when context passages are too long, LLMs are often less effective in exploiting that context, especially for information that is pro- vided in the middle of the context passage ( Liu et al.\n",
      ", 2023 ).\n",
      "We now explain how these three quality aspects can be measured in a fully automated way, by prompting an LLM.\n",
      "\n",
      "Chapter: 4 The WikiEval Dataset\n",
      "In our implementation and experiments, all prompts are evaluated using the gpt-3.5-turbo-16k model, which is available through the OpenAI API 2 .\n",
      "Faithfulness We say that the answer a s ( q ) is faithful to the context c ( q ) if the claims that are made in the answer can be inferred from the con- text.\n",
      "\n",
      "Chapter: 4 The WikiEval Dataset\n",
      "To estimate faithfulness, we first use an LLM to extract a set of statements, S ( a s ( q )) .\n",
      "\n",
      "Chapter: 4 The WikiEval Dataset\n",
      "The aim of this step is to decompose longer sentences into shorter and more focused assertions.\n",
      "\n",
      "Chapter: 4 The WikiEval Dataset\n",
      "We use the following prompt for this step 3 : Given a question and answer, create one or more statements from each sentence in the given answer.\n",
      "question: [question] answer: [answer] where [question] and [answer] refer to the given question and answer.\n",
      "\n",
      "Chapter: 4 The WikiEval Dataset\n",
      "For each statement s i 2 https://platform.openai.com 3 To help clarify the task, we include a demonstration as part of the prompt.\n",
      "\n",
      "Chapter: 4 The WikiEval Dataset\n",
      "This demonstration is not explicitly shown in the listing of the prompts throughout this paper.\n",
      "in S , the LLM determines if s i can be inferred from c ( q ) using a verification function v ( s i , c ( q )) .\n",
      "\n",
      "Chapter: 4 The WikiEval Dataset\n",
      "This verification step is carried out using the following prompt: Consider the given context and following statements, then determine whether they are supported by the information present in the context.\n",
      "\n",
      "Chapter: 4 The WikiEval Dataset\n",
      "Provide a brief explana- tion for each statement before arriving at the verdict (Yes/No).\n",
      "\n",
      "Chapter: 4 The WikiEval Dataset\n",
      "Provide a final verdict for each statement in order at the end in the given format.\n",
      "\n",
      "Chapter: 4 The WikiEval Dataset\n",
      "Do not deviate from the specified format.\n",
      "statement: [statement 1] ...\n",
      "statement: [statement n ] The final faithfulness score, F , is then computed as F = | V | | S | , where | V | is the number of statements that were supported according to the LLM and | S | is the total number of statements.\n",
      "Answer relevance We say that the answer a s ( q ) is relevant if it directly addresses the question in an appropriate way.\n",
      "\n",
      "Chapter: 4 The WikiEval Dataset\n",
      "In particular, our assessment of answer relevance does not take into account fac- tuality, but penalises cases where the answer is incomplete or where it contains redundant informa- tion.\n",
      "\n",
      "Chapter: 4 The WikiEval Dataset\n",
      "To estimate answer relevance, for the given answer a s ( q ) , we prompt the LLM to generate n potential questions q i based on a s ( q ) , as follows: Generate a question for the given answer.\n",
      "answer : [answer] We then obtain embeddings for all questions us- ing the text-embedding-ada-002 model, avail- able from the OpenAI API.\n",
      "\n",
      "Chapter: 4 The WikiEval Dataset\n",
      "For each q i , we cal- culate the similarity sim ( q, q i ) with the original question q , as the cosine between the correspond- ing embeddings.\n",
      "\n",
      "Chapter: 4 The WikiEval Dataset\n",
      "The answer relevance score, AR , for question q is then computed as: AR = 1 n n X i =1 sim ( q, q i ) (1) This metric evaluates how closely the generated answer aligns with the initial question or instruc- tion.\n",
      "\n",
      "Chapter: 5 Experiments\n",
      "relevance The context c ( q ) is consid- ered relevant to the extent that it exclusively con- tains information that is needed to answer the ques- tion.\n",
      "\n",
      "Chapter: 5 Experiments\n",
      "In particular, this metric aims to penalise theinclusion of redundant information.\n",
      "\n",
      "Chapter: 5 Experiments\n",
      "To estimate context relevance, given a question q and its con- text c ( q ) , the LLM extracts a subset of sentences, S ext , from c ( q ) that are crucial to answer q , using the following prompt: Please extract relevant sentences from the provided context that can potentially help answer the following question.\n",
      "\n",
      "Chapter: 5 Experiments\n",
      "If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return the phrase \"Insufficient Information\".\n",
      "\n",
      "Chapter: 5 Experiments\n",
      "While extract- ing candidate sentences you’re not al- lowed to make any changes to sentences from given context.\n",
      "The context relevance score is then computed as: CR = number of extracted sentences total number of sentences in c ( q ) (2)\n",
      "\n",
      "Chapter: 6 Conclusions\n",
      "To evaluate the proposed framework, we ideally need examples of question-context-answer triples which are annotated with human judgments.\n",
      "\n",
      "Chapter: 6 Conclusions\n",
      "We can then verify to what extent our metrics agree with human assessments of faithfulness, answer relevance and context relevance.\n",
      "\n",
      "Chapter: 6 Conclusions\n",
      "Since we are not aware of any publicly available datasets that could be used for this purpose, we created a new dataset, which we refer to as WikiEval 4 .\n",
      "\n",
      "Chapter: 6 Conclusions\n",
      "To construct the dataset, we first selected 50 Wikipedia pages cov- ering events that have happened since the start of 2022 5 .\n",
      "\n",
      "Chapter: 6 Conclusions\n",
      "In selecting these pages, we prioritised those with recent edits.\n",
      "\n",
      "Chapter: 6 Conclusions\n",
      "For each of the 50 pages, we then asked ChatGPT to suggest a question that can be answered based on the introductory section of the page, using the following prompt: Your task is to formulate a question from given context satisfying the rules given below: 1.\n",
      "\n",
      "Chapter: 6 Conclusions\n",
      "The question should be fully answered from the given context.\n",
      "2.\n",
      "\n",
      "Chapter: 6 Conclusions\n",
      "The question should be framed from a part that contains non-trivial informa- tion.\n",
      "3.\n",
      "\n",
      "Chapter: 6 Conclusions\n",
      "The answer should not contain any 4 https://huggingface.co/datasets/ explodinggradients/WikiEval 5 That is, beyond the reported training cutoff of the model we used in our experiments.\n",
      "links.\n",
      "4.\n",
      "\n",
      "Chapter: 6 Conclusions\n",
      "The question should be of moderate difficulty.\n",
      "5.\n",
      "\n",
      "Chapter: 6 Conclusions\n",
      "The question must be reasonable and must be understood and responded to by humans.\n",
      "6.\n",
      "\n",
      "Chapter: 6 Conclusions\n",
      "Do not use phrases that ’provided con- text’, etc in the question context: We also used ChatGPT to answer the generated question, when given the corresponding introduc- tory section as context, using the following prompt: Answer the question using the informa- tion from the given context.\n",
      "question: [question] context: [context] All questions were annotated along the three con- sidered quality dimensions by two annotators.\n",
      "\n",
      "Chapter: 6 Conclusions\n",
      "Both annotators were fluent in English and were given clear instructions about the meaning of the three considered quality dimensions.\n",
      "\n",
      "Chapter: 6 Conclusions\n",
      "For faithfulness and context relevance, the two annotators agreed in around 95% of cases.\n",
      "\n",
      "Chapter: 6 Conclusions\n",
      "For answer relevance, they agreed in around 90% of the cases.\n",
      "\n",
      "Chapter: 6 Conclusions\n",
      "Disagreements were resolved after a discussion between the anno- tators.\n",
      "Faithfulness To obtain human judgements about faithfulness, we first used ChatGPT to answer the question without access to any additional context.\n",
      "We then asked the annotators to judge which of the two answers was the most faithful (i.e.\n",
      "\n",
      "Chapter: 6 Conclusions\n",
      "the standard one or the one generated without context), given the question and corresponding Wikipedia page.\n",
      "Answer relevance We first used ChatGPT to obtain candidate answers with lower answer rel- evance, using the following prompt: Answer the given question in an incom- plete manner.\n",
      "question: [question] We then asked human annotators to compare this answer, and indicate which of the two answers had the highest answer relevance.\n",
      "\n",
      "Chapter: References\n",
      "relevance To measure this aspect, we first added additional sentences to the context by scraping back-links to the corresponding Wikipedia page.\n",
      "\n",
      "Chapter: References\n",
      "In this way, we were able to add information to the context that was related but less relevant forFaith.\n",
      "Ans.\n",
      "\n",
      "Chapter: References\n",
      "Rel.\n",
      "Cont.\n",
      "\n",
      "Chapter: References\n",
      "Rel.\n",
      "RAGAs 0.95 0.78 0.70 GPT Score 0.72 0.52 0.63 GPT Ranking 0.54 0.40 0.52 Table 1: Agreement with human annotators in pairwise comparisons of faithfulness, answer relevance and con- text relevance, using the WikEval dataset (accuracy).\n",
      "answering the question.\n",
      "\n",
      "Chapter: References\n",
      "For the few pages with- out any back-links, we instead used ChatGPT to complete the given context.\n",
      "\n",
      "Chapter: A Examples from WikiEval\n",
      "Table 1 analyses the agreement between the met- rics proposed in Section 3 and the human assess- ments from the proposed WikiEval dataset.\n",
      "\n",
      "Chapter: A Examples from WikiEval\n",
      "Each WikiEval instance requires the model to compare two answers or two context fragments.\n",
      "\n",
      "Chapter: A Examples from WikiEval\n",
      "We count how often the answer/context preferred by the model (i.e.\n",
      "\n",
      "Chapter: A Examples from WikiEval\n",
      "with highest estimated faithfulness, an- swer relevance, or context relevance) coincides with the answer/context preferred by the human annotators.\n",
      "\n",
      "Chapter: A Examples from WikiEval\n",
      "We report the results in terms of ac- curacy (i.e.\n",
      "\n",
      "Chapter: A Examples from WikiEval\n",
      "the fraction of instances on which the model agrees with the annotators).\n",
      "To put the results in context, we compare our proposed metrics (shown as RAGAs in Table 1 ) with two baseline methods.\n",
      "\n",
      "Chapter: A Examples from WikiEval\n",
      "For the first method, shown as GPT Score , we ask ChatGPT to assign a score between 0 and 10 for the three quality dimensions.\n",
      "To this end, we use a prompt that describes the meaning of the quality metric and then asks to score the given answer/context in line with that definition.\n",
      "\n",
      "Chapter: A Examples from WikiEval\n",
      "For instance, for evaluating faithfulness, we used the following prompt: Faithfulness measures the information consistency of the answer against the given context.\n",
      "\n",
      "Chapter: A Examples from WikiEval\n",
      "Any claims that are made in the answer that cannot be deduced from context should be penalized.\n",
      "Given an answer and context, assign a score for faithfulness in the range 0-10.\n",
      "context : [context] answer : [answer] Ties, where the same score is assigned by the LLM to both answer candidates, were broken randomly.\n",
      "The second baseline, shown as GPT Ranking , in- stead asks ChatGPT to select the preferred answer/- context.\n",
      "\n",
      "Chapter: A Examples from WikiEval\n",
      "In this case, the prompt again includes a definition of the considered quality metric.\n",
      "\n",
      "Chapter: A Examples from WikiEval\n",
      "For instance, for evaluating answer relevance, we used the following prompt: Answer Relevancy measures the degree to which a response directly addresses and is appropriate for a given question.\n",
      "It penalizes the present of redundant in- formation or incomplete answers given a question.\n",
      "\n",
      "Chapter: A Examples from WikiEval\n",
      "Given an question and answer, rank each answer based on Answer Rele- vancy.\n",
      "question : [question] answer 1 : [answer 1] answer 2 : [answer 2] The results in Table 1 show that our proposed metrics are much closer aligned with the human judgements than the predictions from the two base- lines.\n",
      "\n",
      "Chapter: A Examples from WikiEval\n",
      "For faithfulness, the RAGAs prediction are in general highly accurate.\n",
      "\n",
      "Chapter: A Examples from WikiEval\n",
      "For answer relevance, the agreement is lower, but this is largely due to the fact that the differences between the two candidate answers are often very subtle.\n",
      "\n",
      "Chapter: A Examples from WikiEval\n",
      "We found context relevance to be the hardest quality dimension to evaluate.\n",
      "\n",
      "Chapter: A Examples from WikiEval\n",
      "In particular, we observed that ChatGPT often struggles with the task of selecting the sen- tences from the context that are crucial, especially for longer contexts.\n",
      "\n",
      "Chapter: Question\n",
      "We have highlighted the need for automated reference-free evaluation of RAG systems.\n",
      "\n",
      "Chapter: Question\n",
      "In par- ticular, we have argued the need for an evaluation framework that can assess faithfulness (i.e.\n",
      "\n",
      "Chapter: Question\n",
      "is the answer grounded in the retrieved context), answer relevance (i.e.\n",
      "\n",
      "Chapter: Question\n",
      "does the answer address the ques- tion) and context relevance (i.e.\n",
      "\n",
      "Chapter: Question\n",
      "is the retrieved context sufficiently focused).\n",
      "\n",
      "Chapter: Question\n",
      "To support the devel- opment of such a framework, we have introduced WikiEval , a dataset which human judgements of these three different aspects.\n",
      "\n",
      "Chapter: Question\n",
      "Finally, we have also described RAGAs, our implementation of the three considered quality aspects.\n",
      "\n",
      "Chapter: Question\n",
      "This framework is easy to use and can provide deverlopers of RAG sys- tems with valuable insights, even in the absence of any ground truth.\n",
      "\n",
      "Chapter: Question\n",
      "Our evaluation on WikiEval has shown that the predictions from RAGAs are closely aligned with human predictions, especially for faithfulness and answer relevance.\n",
      "\n",
      "Chapter: Context\n",
      "Amos Azaria and Tom M.\n",
      "\n",
      "Chapter: Context\n",
      "Mitchell.\n",
      "\n",
      "Chapter: Context\n",
      "2023.\n",
      "The inter- nal state of an LLM knows when its lying .\n",
      "CoRR , abs/2304.13734.\n",
      "Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Si- monyan, Jack W.\n",
      "\n",
      "Chapter: Context\n",
      "Rae, Erich Elsen, and Laurent Sifre.\n",
      "2022.\n",
      "Improving language models by retrieving from trillions of tokens .\n",
      "\n",
      "Chapter: Context\n",
      "In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Bal- timore, Maryland, USA , volume 162 of Proceedings of Machine Learning Research , pages 2206–2240.\n",
      "PMLR.\n",
      "Sébastien Bubeck, Varun Chandrasekaran, Ronen El- dan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund- berg, et al.\n",
      "\n",
      "Chapter: Context\n",
      "2023.\n",
      "\n",
      "Chapter: Context\n",
      "Sparks of artificial general intelli- gence: Early experiments with gpt-4.\n",
      "arXiv preprint arXiv:2303.12712 .\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n",
      "\n",
      "Chapter: Context\n",
      "2019.\n",
      "BERT: Pre-training of deep bidirectional transformers for language under- standing .\n",
      "\n",
      "Chapter: Context\n",
      "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers) , pages 4171–4186, Minneapolis, Minnesota.\n",
      "\n",
      "Chapter: Context\n",
      "Association for Computational Linguistics.\n",
      "Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu.\n",
      "\n",
      "Chapter: Context\n",
      "2023.\n",
      "Gptscore: Evaluate as you desire .\n",
      "CoRR , abs/2302.04166.\n",
      "Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu- pat, and Mingwei Chang.\n",
      "\n",
      "Chapter: Context\n",
      "2020.\n",
      "\n",
      "Chapter: Context\n",
      "Retrieval augmented language model pre-training.\n",
      "\n",
      "Chapter: Context\n",
      "In International confer- ence on machine learning , pages 3929–3938.\n",
      "\n",
      "Chapter: Context\n",
      "PMLR.\n",
      "Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung.\n",
      "\n",
      "Chapter: Context\n",
      "2023.\n",
      "\n",
      "Chapter: Context\n",
      "Survey of halluci- nation in natural language generation.\n",
      "ACM Comput- ing Surveys , 55(12):1–38.\n",
      "Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jack- son Kernion, Shauna Kravec, Liane Lovitt, Ka- mal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan.\n",
      "\n",
      "Chapter: Context\n",
      "2022.\n",
      "Language models (mostly) know what they know .\n",
      "CoRR , abs/2207.05221.\n",
      "Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel.\n",
      "\n",
      "Chapter: Context\n",
      "2022.\n",
      "Large language models struggle to learn long-tail knowledge .\n",
      "CoRR , abs/2211.08411.\n",
      "Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis.\n",
      "\n",
      "Chapter: Context\n",
      "2020.\n",
      "Generalization through memorization: Nearest neighbor language models .\n",
      "\n",
      "Chapter: Context\n",
      "In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 .\n",
      "\n",
      "Chapter: Context\n",
      "OpenReview.net.\n",
      "Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia.\n",
      "\n",
      "Chapter: Context\n",
      "2022.\n",
      "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive NLP .\n",
      "CoRR , abs/2212.14024.\n",
      "Kenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n",
      "2019.\n",
      "\n",
      "Chapter: Context\n",
      "Latent retrieval for weakly supervised open do- main question answering.\n",
      "\n",
      "Chapter: Context\n",
      "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 6086–6096.\n",
      "Patrick S.\n",
      "\n",
      "Chapter: Context\n",
      "H.\n",
      "\n",
      "Chapter: Context\n",
      "Lewis, Ethan Perez, Aleksandra Pik- tus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela.\n",
      "\n",
      "Chapter: Context\n",
      "2020.\n",
      "Retrieval-augmented generation for knowledge-intensive NLP tasks .\n",
      "\n",
      "Chapter: Context\n",
      "In Advances in Neu- ral Information Processing Systems 33: Annual Con- ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual .\n",
      "Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen.\n",
      "\n",
      "Chapter: Context\n",
      "2023.\n",
      "Halueval: A large- scale hallucination evaluation benchmark for large language models .\n",
      "CoRR , abs/2305.11747.\n",
      "Nelson F.\n",
      "\n",
      "Chapter: Context\n",
      "Liu, Kevin Lin, John Hewitt, Ashwin Paran- jape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.\n",
      "\n",
      "Chapter: Context\n",
      "2023.\n",
      "Lost in the middle: How language models use long contexts .\n",
      "Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi.\n",
      "\n",
      "Chapter: Context\n",
      "2023.\n",
      "When not to trust language models: Investigating effectiveness of parametric and non-parametric mem- ories .\n",
      "\n",
      "Chapter: Context\n",
      "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers) , pages 9802–9822, Toronto, Canada.\n",
      "\n",
      "Chapter: Context\n",
      "Association for Computational Linguistics.\n",
      "Potsawee Manakul, Adian Liusie, and Mark J.\n",
      "\n",
      "Chapter: Context\n",
      "F.\n",
      "\n",
      "Chapter: Context\n",
      "Gales.\n",
      "2023.\n",
      "Selfcheckgpt: Zero-resource black-box hal- lucination detection for generative large language models .\n",
      "CoRR , abs/2303.08896.\n",
      "Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi.\n",
      "\n",
      "Chapter: Context\n",
      "2023.\n",
      "Factscore: Fine-grained atomic evaluation of fac- tual precision in long form text generation .\n",
      "CoRR , abs/2305.14251.Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham.\n",
      "\n",
      "Chapter: Context\n",
      "2023.\n",
      "In-context retrieval-augmented lan- guage models .\n",
      "CoRR , abs/2302.00083.\n",
      "Adam Roberts, Colin Raffel, and Noam Shazeer.\n",
      "\n",
      "Chapter: Context\n",
      "2020.\n",
      "How much knowledge can you pack into the param- eters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 5418–5426, Online.\n",
      "\n",
      "Chapter: Context\n",
      "Association for Computational Linguistics.\n",
      "Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih.\n",
      "\n",
      "Chapter: Context\n",
      "2023.\n",
      "REPLUG: retrieval-augmented black-box language models .\n",
      "CoRR , abs/2301.12652.\n",
      "Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxi- ang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.\n",
      "\n",
      "Chapter: Context\n",
      "2023a.\n",
      "Is chatgpt a good NLG evaluator? A preliminary study .\n",
      "CoRR , abs/2303.04048.\n",
      "Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui.\n",
      "2023b.\n",
      "Large language models are not fair evaluators .\n",
      "CoRR , abs/2305.17926.\n",
      "Shufan Wang, Yixiao Song, Andrew Drozdov, Aparna Garimella, Varun Manjunatha, and Mohit Iyyer.\n",
      "2023c.\n",
      "KNN-LM does not improve open-ended text generation .\n",
      "CoRR , abs/2305.14625.\n",
      "Weizhe Yuan, Graham Neubig, and Pengfei Liu.\n",
      "\n",
      "Chapter: Context\n",
      "2021.\n",
      "Bartscore: Evaluating generated text as text genera- tion .\n",
      "\n",
      "Chapter: Context\n",
      "In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Informa- tion Processing Systems 2021, NeurIPS 2021, De- cember 6-14, 2021, virtual , pages 27263–27277.\n",
      "Tianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei Fang, Luc Gaitskell, Thomas Hartvigsen, Xixin Wu, Danny Fox, Helen Meng, and James R.\n",
      "\n",
      "Chapter: Context\n",
      "Glass.\n",
      "\n",
      "Chapter: Context\n",
      "2023.\n",
      "Interpretable unified language checking .\n",
      "CoRR , abs/2304.03728.\n",
      "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\n",
      "Weinberger, and Yoav Artzi.\n",
      "\n",
      "Chapter: Context\n",
      "2020.\n",
      "Bertscore: Evalu- ating text generation with BERT .\n",
      "\n",
      "Chapter: Context\n",
      "In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 .\n",
      "\n",
      "Chapter: Context\n",
      "OpenRe- view.net.\n",
      "Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris- tian M.\n",
      "\n",
      "Chapter: Context\n",
      "Meyer, and Steffen Eger.\n",
      "\n",
      "Chapter: Context\n",
      "2019.\n",
      "MoverScore: Text generation evaluating with contextualized em- beddings and earth mover distance .\n",
      "\n",
      "Chapter: Context\n",
      "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna- tional Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP) , pages 563–578, Hong Kong, China.\n",
      "\n",
      "Chapter: Context\n",
      "Association for Computational Lin- guistics.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "import unstructured\n",
    "\n",
    "\n",
    "class PDFProcessor:\n",
    "    def __init__(self, pdf_path):\n",
    "        self.pdf_path = pdf_path\n",
    "        self.full_text = \"\"\n",
    "        self.chapters = []\n",
    "        self.section_texts = {}\n",
    "\n",
    "    def extract_and_clean_text(self):\n",
    "        doc = fitz.open(self.pdf_path)\n",
    "\n",
    "        for page in doc:\n",
    "            text = self._extract_text_from_page(page)\n",
    "            cleaned_text = self._clean_extracted_text(text)\n",
    "            self.full_text += cleaned_text\n",
    "\n",
    "        return self.full_text\n",
    "\n",
    "    def _extract_text_from_page(self, page):\n",
    "        text = \"\"\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        for block in blocks:\n",
    "            if \"lines\" in block:\n",
    "                for line in block[\"lines\"]:\n",
    "                    for span in line[\"spans\"]:\n",
    "                        is_bold = span['flags'] & 16  # Bold flag\n",
    "                        is_italic = span['flags'] & 2  # Italic flag\n",
    "                        if span['size'] >= 14.0:  # Title\n",
    "                            text += \"# \" + span['text'] + \"\\n\"\n",
    "                        else:\n",
    "                            text += span['text'] + \"\\n\"\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def _clean_extracted_text(text):\n",
    "        lines = text.split('\\n')\n",
    "        cleaned_lines = []\n",
    "        last_line_was_heading = False\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            current_line_is_heading = line.startswith(\"#\")\n",
    "            if current_line_is_heading and last_line_was_heading:\n",
    "                line = line.replace('#', '')\n",
    "            last_line_was_heading = current_line_is_heading\n",
    "\n",
    "            if re.match(r'^#\\s+CHAPITRE', line):\n",
    "                line = \"\\n\\n\" + line\n",
    "\n",
    "            if line.endswith('.'):\n",
    "                cleaned_lines.append(line + \"\\n\")\n",
    "            else:\n",
    "                cleaned_lines.append(line + \" \")\n",
    "\n",
    "        return ''.join(cleaned_lines).strip()\n",
    "\n",
    "    def partition_pdf(self):\n",
    "        elements = partition_pdf(self.pdf_path, strategy=\"hi_res\")\n",
    "        self.chapters = [\n",
    "            element.text for element in elements if isinstance(element, unstructured.documents.elements.Title)\n",
    "        ]\n",
    "\n",
    "    def split_text_by_chapters(self):\n",
    "        pattern = '|'.join([re.escape(section) for section in self.chapters])\n",
    "        split_text = re.split(pattern, self.full_text)\n",
    "        split_text = [section for section in split_text if section.strip()]\n",
    "\n",
    "        for i, section in enumerate(self.chapters):\n",
    "            if i < len(split_text):\n",
    "                self.section_texts[section] = split_text[i + 1].strip()\n",
    "\n",
    "    def transform_sections_to_array(self):\n",
    "        structured_text = []\n",
    "        for section, content in self.section_texts.items():\n",
    "            structured_text.append(f\"Chapter: {section}\\nContent: {content}\")\n",
    "        return structured_text\n",
    "\n",
    "    @staticmethod\n",
    "    def transform_array(array):\n",
    "        transformed_array = []\n",
    "\n",
    "        for entry in array:\n",
    "            chapter_title_match = re.match(r'Chapter: ([^\\n]+)', entry)\n",
    "            if not chapter_title_match:\n",
    "                continue\n",
    "            chapter_title = chapter_title_match.group(1)\n",
    "\n",
    "            content_match = re.search(r'Content: (.+)', entry, re.DOTALL)\n",
    "            if not content_match:\n",
    "                continue\n",
    "            content = content_match.group(1)\n",
    "\n",
    "            sentences = re.split(r'(?<=\\.) ', content)\n",
    "            for sentence in sentences:\n",
    "                transformed_array.append(f'Chapter: {chapter_title}\\n{sentence.strip()}')\n",
    "\n",
    "        return transformed_array\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"hackaton-mistral-studai/data/RAGAS_09_2023.pdf\"\n",
    "    processor = PDFProcessor(pdf_path)\n",
    "\n",
    "    text = processor.extract_and_clean_text()\n",
    "    processor.partition_pdf()\n",
    "    processor.split_text_by_chapters()\n",
    "    structured_text = processor.transform_sections_to_array()\n",
    "    transformed_text = PDFProcessor.transform_array(structured_text)\n",
    "\n",
    "    for line in transformed_text:\n",
    "        print(line)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# RAGAS: Automated Evaluation of Retrieval Augmented Generation Shahul Es † , Jithin James † , Luis Espinosa-Anke ∗♢ , Steven Schockaert ∗ † Exploding Gradients ∗ CardiffNLP, Cardiff University, United Kingdom ♢ AMPLYFI, United Kingdom shahules786@gmail.com,jamesjithin97@gmail.com {espinosa-ankel,schockaerts1}@cardiff.ac.uk Abstract We introduce RAGA S ( R etrieval A ugmented G eneration As sessment), a framework for reference-free evaluation of Retrieval Aug- mented Generation (RAG) pipelines.\n",
      "RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natu- ral language layer between a user and textual databases, reducing the risk of hallucinations.\n",
      "Evaluating RAG architectures is, however, chal- lenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the gener- ation itself. With RAGA S , we put forward a suite of metrics which can be used to evaluate these different dimensions without having to rely on ground truth human annotations . We posit that such a framework can crucially con- tribute to faster evaluation cycles of RAG archi- tectures, which is especially important given the fast adoption of LLMs.\n",
      "1 Introduction Language Models (LMs) capture a vast amount of knowledge about the world, which allows them to answer questions without accessing any exter- nal sources. This idea of LMs as repositories of knowledge emerged shortly after the introduction of BERT ( Devlin et al.\n",
      ", 2019 ) and became more firmly established with the introduction of ever larger LMs ( Roberts et al.\n",
      ", 2020 ). While the most recent Large Language Models (LLMs) capture enough knowledge to rival human performance across a wide variety of question answering bench- marks ( Bubeck et al.\n",
      ", 2023 ), the idea of using LLMs as knowledge bases still has two fundamen- tal limitations. First, LLMs are not able to answer questions about events that have happened after they were trained. Second, even the largest models struggle to memorise knowledge that is only rarely mentioned in the training corpus ( Kandpal et al.\n",
      ", 2022 ; Mallen et al.\n",
      ", 2023 ). The standard solution to these issues is to rely on Retrieval Augmented Generation (RAG) ( Lee et al.\n",
      ", 2019 ; Lewis et al.\n",
      ", 2020 ; Guu et al.\n",
      ", 2020 ). Answering a question then essentially involves retrieving relevant pas- sages from a corpus and feeding these passages, along with the original question, to the LM. While initial approaches relied on specialised LMs for retrieval-augmented language modelling ( Khandel- wal et al.\n",
      ", 2020 ; Borgeaud et al.\n",
      ", 2022 ), recent work has suggested that simply adding retrieved docu- ments to the input of a standard LM can also work well ( Khattab et al.\n",
      ", 2022 ; Ram et al.\n",
      ", 2023 ; Shi et al.\n",
      ", 2023 ), thus making it possible to use retrieval- augmented strategies in combination with LLMs that are only available through APIs.\n",
      "While the usefulness of retrieval-augmented strategies is clear, their implementation requires a significant amount of tuning, as the overall per- formance will be affected by the retrieval model, the considered corpus, the LM, or the prompt for- mulation, among others. Automated evaluation of retrieval-augmented systems is thus paramount. In practice, RAG systems are often evaluated in terms of the language modelling task itself, i.e. by mea- suring perplexity on some reference corpus. How- ever, such evaluations are not always predictive of downstream performance ( Wang et al.\n",
      ", 2023c ).\n",
      "Moreover, this evaluation strategy relies on the LM probabilities, which are not accessible for some closed models (e.g. ChatGPT and GPT-4). Ques- tion answering is another common evaluation task, but usually only datasets with short extractive an- swers are considered, which may not be represen- tative of how the system will be used.\n",
      "To address these issues, in this paper we present RAGA S 1 , a framework for the automated assess- 1 RAGA S is available at https://github.com/ explodinggradients/ragas .\n",
      "# arXiv:2309.15217v1  [cs.CL]  26 Sep 2023ment of retrieval augmented generation systems.\n",
      "We focus on settings where reference answers may not be available, and where we want to estimate different proxies for correctness, in addition to the usefulness of the retrieved passages. The RAGA S framework provides an integration with both llama- index and Langchain , the most widely used frame- works for building RAG solutions, thus enabling developers to easily integrate RAGA S into their standard workflow.\n",
      "2 Related Work Estimating faithfulness using LLMs The prob- lem of detecting hallucinations in LLM generated responses has been extensively studied ( Ji et al.\n",
      ", 2023 ). Several authors have suggested the idea of predicting factuality using a few-shot prompt- ing strategy ( Zhang et al.\n",
      ", 2023 ). Recent analy- ses, however, suggest that existing models struggle with detecting hallucination when using standard prompting strategies ( Li et al.\n",
      ", 2023 ; Azaria and Mitchell , 2023 ). Other approaches rely on linking the generated responses to facts from an external knowledge base ( Min et al.\n",
      ", 2023 ), but this is not always possible.\n",
      "Yet another strategy is to inspect the probabili- ties assigned to individual tokens, where we would expect the model to be less confident in halluci- nated answers than in factual ones. For instance, BARTScore ( Yuan et al.\n",
      ", 2021 ) estimates factuality by looking at the conditional probability of the gen- erated text given the input.\n",
      "Kadavath et al.\n",
      "( 2022 ) use a variation of this idea. Starting from the ob- servation that LLMs provide well-calibrated proba- bilities when answering multiple-choice questions, they essentially convert the problem of validating model generated answers into a multiple-choice question which asks whether the answer is true or false. Rather than looking at the output probabil- ities, Azaria and Mitchell ( 2023 ) propose to train a supervised classifier on the weights from one of the hidden layers of the LLM, to predict whether a given statement is true or not. While the approach performs well, the need to access the hidden states of the model makes it unsuitable for systems that access LLMs through an API.\n",
      "For models that do not provide access to token probabilities, such as ChatGPT and GPT-4, differ- ent methods are needed. SelfCheckGPT ( Manakul et al.\n",
      ", 2023 ) addresses this problem by instead sam- pling multiple answers. Their core idea is that factual answers are more stable: when an answer is factual, we can expect that different samples will tend to be semantically similar, whereas this is less likely to be the case for hallucinated answers.\n",
      "Automated evaluation of text generation systems LLMs have also been leveraged to automatically evaluate other aspects of generated text fragments, beyond factuality. For instance, GPTScore ( Fu et al.\n",
      ", 2023 ) uses a prompt that specifies the consid- ered aspect (e.g. fluency) and then scores passages based on the average probability of the generated tokens, according to a given autoregressive LM.\n",
      "This idea of using prompts was previously also considered by Yuan et al.\n",
      "( 2021 ), although they used a smaller fine-tuned LM (i.e. BART) and did not observe a clear benefit from using prompts. An- other approach directly asks ChatGPT to evaluate a particular aspect of the given answer by provid- ing a score between 0 and 100, or by providing a rating on a 5-star scale ( Wang et al.\n",
      ", 2023a ). Re- markably, strong results can be obtained in this way, although it comes with the limitation of being sensitive to the design of the prompt. Rather than scoring individual answers, some authors have also focused on using an LLM to select the best answer among a number of candidates ( Wang et al.\n",
      ", 2023b ), typically to compare the performance of different LLMs. However, care is needed with this approach, as the order in which the answers is presented can influence the result ( Wang et al.\n",
      ", 2023b ).\n",
      "In terms of how ground truth answers or, more generally, generations, have been typically used in the literature, most approaches have relied on the availability of one or more reference answers.\n",
      "For instance, BERTScore ( Zhang et al.\n",
      ", 2020 ) and MoverScore ( Zhao et al.\n",
      ", 2019 ) use contex- tualised embeddings, produced by a pre-trained BERT model, to compare the similarity between the generated answer and the reference answers.\n",
      "BARTScore ( Yuan et al.\n",
      ", 2021 ) similarly uses refer- ence answers to compute aspects such as precision (estimated as the probability of generating the gen- erated answer given the reference) and recall (esti- mated as the probability of generating the reference given the generated answer).\n",
      "3 Evaluation Strategies We consider a standard RAG setting, where given a question q , the system first retrieves some context c ( q ) and then uses the retrieved context to generate an answer a s ( q ) . When building a RAG system,we usually do not have access to human-annotated datasets or reference answers. We therefore fo- cus on metrics that are fully self-contained and reference-free. We focus in particular three quality aspects, which we argue are of central importance.\n",
      "First, Faithfulness refers to the idea that the an- swer should be grounded in the given context. This is important to avoid hallucinations, and to ensure that the retrieved context can act as a justification for the generated answer. Indeed, RAG systems are often used in applications where the factual con- sistency of the generated text w.r.t. the grounded sources is highly important, e.g. in domains such as law, where information is constantly evolving. Sec- ond, Answer Relevance refers to the idea that the generated answer should address the actual ques- tion that was provided. Finally, Context Relevance refers to the idea that the retrieved context should be focused, containing as little irrelevant informa- tion as possible. This is important given the cost associated with feeding long context passages to LLMs. Moreover, when context passages are too long, LLMs are often less effective in exploiting that context, especially for information that is pro- vided in the middle of the context passage ( Liu et al.\n",
      ", 2023 ).\n",
      "We now explain how these three quality aspects can be measured in a fully automated way, by prompting an LLM. In our implementation and experiments, all prompts are evaluated using the gpt-3.5-turbo-16k model, which is available through the OpenAI API 2 .\n",
      "Faithfulness We say that the answer a s ( q ) is faithful to the context c ( q ) if the claims that are made in the answer can be inferred from the con- text. To estimate faithfulness, we first use an LLM to extract a set of statements, S ( a s ( q )) . The aim of this step is to decompose longer sentences into shorter and more focused assertions. We use the following prompt for this step 3 : Given a question and answer, create one or more statements from each sentence in the given answer.\n",
      "question: [question] answer: [answer] where [question] and [answer] refer to the given question and answer. For each statement s i 2 https://platform.openai.com 3 To help clarify the task, we include a demonstration as part of the prompt. This demonstration is not explicitly shown in the listing of the prompts throughout this paper.\n",
      "in S , the LLM determines if s i can be inferred from c ( q ) using a verification function v ( s i , c ( q )) . This verification step is carried out using the following prompt: Consider the given context and following statements, then determine whether they are supported by the information present in the context. Provide a brief explana- tion for each statement before arriving at the verdict (Yes/No). Provide a final verdict for each statement in order at the end in the given format. Do not deviate from the specified format.\n",
      "statement: [statement 1] ...\n",
      "statement: [statement n ] The final faithfulness score, F , is then computed as F = | V | | S | , where | V | is the number of statements that were supported according to the LLM and | S | is the total number of statements.\n",
      "Answer relevance We say that the answer a s ( q ) is relevant if it directly addresses the question in an appropriate way. In particular, our assessment of answer relevance does not take into account fac- tuality, but penalises cases where the answer is incomplete or where it contains redundant informa- tion. To estimate answer relevance, for the given answer a s ( q ) , we prompt the LLM to generate n potential questions q i based on a s ( q ) , as follows: Generate a question for the given answer.\n",
      "answer : [answer] We then obtain embeddings for all questions us- ing the text-embedding-ada-002 model, avail- able from the OpenAI API. For each q i , we cal- culate the similarity sim ( q, q i ) with the original question q , as the cosine between the correspond- ing embeddings. The answer relevance score, AR , for question q is then computed as: AR = 1 n n X i =1 sim ( q, q i ) (1) This metric evaluates how closely the generated answer aligns with the initial question or instruc- tion.\n",
      "Context relevance The context c ( q ) is consid- ered relevant to the extent that it exclusively con- tains information that is needed to answer the ques- tion. In particular, this metric aims to penalise theinclusion of redundant information. To estimate context relevance, given a question q and its con- text c ( q ) , the LLM extracts a subset of sentences, S ext , from c ( q ) that are crucial to answer q , using the following prompt: Please extract relevant sentences from the provided context that can potentially help answer the following question. If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return the phrase \"Insufficient Information\". While extract- ing candidate sentences you’re not al- lowed to make any changes to sentences from given context.\n",
      "The context relevance score is then computed as: CR = number of extracted sentences total number of sentences in c ( q ) (2) 4 The WikiEval Dataset To evaluate the proposed framework, we ideally need examples of question-context-answer triples which are annotated with human judgments. We can then verify to what extent our metrics agree with human assessments of faithfulness, answer relevance and context relevance. Since we are not aware of any publicly available datasets that could be used for this purpose, we created a new dataset, which we refer to as WikiEval 4 . To construct the dataset, we first selected 50 Wikipedia pages cov- ering events that have happened since the start of 2022 5 . In selecting these pages, we prioritised those with recent edits. For each of the 50 pages, we then asked ChatGPT to suggest a question that can be answered based on the introductory section of the page, using the following prompt: Your task is to formulate a question from given context satisfying the rules given below: 1. The question should be fully answered from the given context.\n",
      "2. The question should be framed from a part that contains non-trivial informa- tion.\n",
      "3. The answer should not contain any 4 https://huggingface.co/datasets/ explodinggradients/WikiEval 5 That is, beyond the reported training cutoff of the model we used in our experiments.\n",
      "links.\n",
      "4. The question should be of moderate difficulty.\n",
      "5. The question must be reasonable and must be understood and responded to by humans.\n",
      "6. Do not use phrases that ’provided con- text’, etc in the question context: We also used ChatGPT to answer the generated question, when given the corresponding introduc- tory section as context, using the following prompt: Answer the question using the informa- tion from the given context.\n",
      "question: [question] context: [context] All questions were annotated along the three con- sidered quality dimensions by two annotators. Both annotators were fluent in English and were given clear instructions about the meaning of the three considered quality dimensions. For faithfulness and context relevance, the two annotators agreed in around 95% of cases. For answer relevance, they agreed in around 90% of the cases. Disagreements were resolved after a discussion between the anno- tators.\n",
      "Faithfulness To obtain human judgements about faithfulness, we first used ChatGPT to answer the question without access to any additional context.\n",
      "We then asked the annotators to judge which of the two answers was the most faithful (i.e. the standard one or the one generated without context), given the question and corresponding Wikipedia page.\n",
      "Answer relevance We first used ChatGPT to obtain candidate answers with lower answer rel- evance, using the following prompt: Answer the given question in an incom- plete manner.\n",
      "question: [question] We then asked human annotators to compare this answer, and indicate which of the two answers had the highest answer relevance.\n",
      "Context relevance To measure this aspect, we first added additional sentences to the context by scraping back-links to the corresponding Wikipedia page. In this way, we were able to add information to the context that was related but less relevant forFaith.\n",
      "Ans. Rel.\n",
      "Cont. Rel.\n",
      "RAGAs 0.95 0.78 0.70 GPT Score 0.72 0.52 0.63 GPT Ranking 0.54 0.40 0.52 Table 1: Agreement with human annotators in pairwise comparisons of faithfulness, answer relevance and con- text relevance, using the WikEval dataset (accuracy).\n",
      "answering the question. For the few pages with- out any back-links, we instead used ChatGPT to complete the given context.\n",
      "5 Experiments Table 1 analyses the agreement between the met- rics proposed in Section 3 and the human assess- ments from the proposed WikiEval dataset. Each WikiEval instance requires the model to compare two answers or two context fragments. We count how often the answer/context preferred by the model (i.e. with highest estimated faithfulness, an- swer relevance, or context relevance) coincides with the answer/context preferred by the human annotators. We report the results in terms of ac- curacy (i.e. the fraction of instances on which the model agrees with the annotators).\n",
      "To put the results in context, we compare our proposed metrics (shown as RAGAs in Table 1 ) with two baseline methods. For the first method, shown as GPT Score , we ask ChatGPT to assign a score between 0 and 10 for the three quality dimensions.\n",
      "To this end, we use a prompt that describes the meaning of the quality metric and then asks to score the given answer/context in line with that definition. For instance, for evaluating faithfulness, we used the following prompt: Faithfulness measures the information consistency of the answer against the given context. Any claims that are made in the answer that cannot be deduced from context should be penalized.\n",
      "Given an answer and context, assign a score for faithfulness in the range 0-10.\n",
      "context : [context] answer : [answer] Ties, where the same score is assigned by the LLM to both answer candidates, were broken randomly.\n",
      "The second baseline, shown as GPT Ranking , in- stead asks ChatGPT to select the preferred answer/- context. In this case, the prompt again includes a definition of the considered quality metric. For instance, for evaluating answer relevance, we used the following prompt: Answer Relevancy measures the degree to which a response directly addresses and is appropriate for a given question.\n",
      "It penalizes the present of redundant in- formation or incomplete answers given a question. Given an question and answer, rank each answer based on Answer Rele- vancy.\n",
      "question : [question] answer 1 : [answer 1] answer 2 : [answer 2] The results in Table 1 show that our proposed metrics are much closer aligned with the human judgements than the predictions from the two base- lines. For faithfulness, the RAGAs prediction are in general highly accurate. For answer relevance, the agreement is lower, but this is largely due to the fact that the differences between the two candidate answers are often very subtle. We found context relevance to be the hardest quality dimension to evaluate. In particular, we observed that ChatGPT often struggles with the task of selecting the sen- tences from the context that are crucial, especially for longer contexts.\n",
      "6 Conclusions We have highlighted the need for automated reference-free evaluation of RAG systems. In par- ticular, we have argued the need for an evaluation framework that can assess faithfulness (i.e. is the answer grounded in the retrieved context), answer relevance (i.e. does the answer address the ques- tion) and context relevance (i.e. is the retrieved context sufficiently focused). To support the devel- opment of such a framework, we have introduced WikiEval , a dataset which human judgements of these three different aspects. Finally, we have also described RAGAs, our implementation of the three considered quality aspects. This framework is easy to use and can provide deverlopers of RAG sys- tems with valuable insights, even in the absence of any ground truth. Our evaluation on WikiEval has shown that the predictions from RAGAs are closely aligned with human predictions, especially for faithfulness and answer relevance.References Amos Azaria and Tom M. Mitchell. 2023.\n",
      "The inter- nal state of an LLM knows when its lying .\n",
      "CoRR , abs/2304.13734.\n",
      "Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Si- monyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.\n",
      "2022.\n",
      "Improving language models by retrieving from trillions of tokens . In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Bal- timore, Maryland, USA , volume 162 of Proceedings of Machine Learning Research , pages 2206–2240.\n",
      "PMLR.\n",
      "Sébastien Bubeck, Varun Chandrasekaran, Ronen El- dan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund- berg, et al. 2023. Sparks of artificial general intelli- gence: Early experiments with gpt-4.\n",
      "arXiv preprint arXiv:2303.12712 .\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.\n",
      "BERT: Pre-training of deep bidirectional transformers for language under- standing . In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers) , pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.\n",
      "Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023.\n",
      "Gptscore: Evaluate as you desire .\n",
      "CoRR , abs/2302.04166.\n",
      "Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu- pat, and Mingwei Chang. 2020. Retrieval augmented language model pre-training. In International confer- ence on machine learning , pages 3929–3938. PMLR.\n",
      "Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of halluci- nation in natural language generation.\n",
      "ACM Comput- ing Surveys , 55(12):1–38.\n",
      "Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jack- son Kernion, Shauna Kravec, Liane Lovitt, Ka- mal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. 2022.\n",
      "Language models (mostly) know what they know .\n",
      "CoRR , abs/2207.05221.\n",
      "Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2022.\n",
      "Large language models struggle to learn long-tail knowledge .\n",
      "CoRR , abs/2211.08411.\n",
      "Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020.\n",
      "Generalization through memorization: Nearest neighbor language models . In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net.\n",
      "Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. 2022.\n",
      "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive NLP .\n",
      "CoRR , abs/2212.14024.\n",
      "Kenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n",
      "2019. Latent retrieval for weakly supervised open do- main question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 6086–6096.\n",
      "Patrick S. H. Lewis, Ethan Perez, Aleksandra Pik- tus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020.\n",
      "Retrieval-augmented generation for knowledge-intensive NLP tasks . In Advances in Neu- ral Information Processing Systems 33: Annual Con- ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual .\n",
      "Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023.\n",
      "Halueval: A large- scale hallucination evaluation benchmark for large language models .\n",
      "CoRR , abs/2305.11747.\n",
      "Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran- jape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023.\n",
      "Lost in the middle: How language models use long contexts .\n",
      "Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023.\n",
      "When not to trust language models: Investigating effectiveness of parametric and non-parametric mem- ories . In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers) , pages 9802–9822, Toronto, Canada. Association for Computational Linguistics.\n",
      "Potsawee Manakul, Adian Liusie, and Mark J. F. Gales.\n",
      "2023.\n",
      "Selfcheckgpt: Zero-resource black-box hal- lucination detection for generative large language models .\n",
      "CoRR , abs/2303.08896.\n",
      "Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.\n",
      "Factscore: Fine-grained atomic evaluation of fac- tual precision in long form text generation .\n",
      "CoRR , abs/2305.14251.Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023.\n",
      "In-context retrieval-augmented lan- guage models .\n",
      "CoRR , abs/2302.00083.\n",
      "Adam Roberts, Colin Raffel, and Noam Shazeer. 2020.\n",
      "How much knowledge can you pack into the param- eters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 5418–5426, Online. Association for Computational Linguistics.\n",
      "Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023.\n",
      "REPLUG: retrieval-augmented black-box language models .\n",
      "CoRR , abs/2301.12652.\n",
      "Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxi- ang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023a.\n",
      "Is chatgpt a good NLG evaluator? A preliminary study .\n",
      "CoRR , abs/2303.04048.\n",
      "Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui.\n",
      "2023b.\n",
      "Large language models are not fair evaluators .\n",
      "CoRR , abs/2305.17926.\n",
      "Shufan Wang, Yixiao Song, Andrew Drozdov, Aparna Garimella, Varun Manjunatha, and Mohit Iyyer.\n",
      "2023c.\n",
      "KNN-LM does not improve open-ended text generation .\n",
      "CoRR , abs/2305.14625.\n",
      "Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\n",
      "Bartscore: Evaluating generated text as text genera- tion . In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Informa- tion Processing Systems 2021, NeurIPS 2021, De- cember 6-14, 2021, virtual , pages 27263–27277.\n",
      "Tianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei Fang, Luc Gaitskell, Thomas Hartvigsen, Xixin Wu, Danny Fox, Helen Meng, and James R. Glass. 2023.\n",
      "Interpretable unified language checking .\n",
      "CoRR , abs/2304.03728.\n",
      "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\n",
      "Weinberger, and Yoav Artzi. 2020.\n",
      "Bertscore: Evalu- ating text generation with BERT . In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenRe- view.net.\n",
      "Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris- tian M. Meyer, and Steffen Eger. 2019.\n",
      "MoverScore: Text generation evaluating with contextualized em- beddings and earth mover distance . In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna- tional Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP) , pages 563–578, Hong Kong, China. Association for Computational Lin- guistics.\n",
      "A Examples from WikiEval Tables 2 , 3 and 4 show examples from the WikiEval dataset, focusing in particular on answers with high and low faithfulness (Table 2 ), high and low answer relevance (Table 3 ), and high and low context rele- vance (Table 4 ).Question Context Answer Who directed the film Op- penheimer and who stars as J. Robert Oppenheimer in the film? Oppenheimer is a 2023 biographical thriller film written and directed by Christopher Nolan. Based on the 2005 biography American Prometheus by Kai Bird and Mar- tin J. Sherwin, the film chronicles the life of J. Robert Oppenheimer, a theoretical physicist who was pivotal in developing the first nuclear weapons as part of the Man- hattan Project, and thereby ushering in the Atomic Age.\n",
      "Cillian Murphy stars as Oppenheimer, with Emily Blunt as Oppenheimer’s wife Katherine \"Kitty\" Oppenheimer.\n",
      "High Faithfulness : Christopher Nolan directed the film Oppen- heimer. Cillian Murphy stars as J.\n",
      "Robert Oppenheimer in the film.\n",
      "Low Faithfulness : James Cameron directed the film Op- penheimer. Tom Cruise stars as J.\n",
      "Robert Oppenheimer in the film.\n",
      "Table 2: Example from WikiEval, showing answers with high and low faithfulness.\n",
      "Question Answer When is the scheduled launch date and time for the PSLV-C56 mission, and where will it be launched from? High answer relevance : The PSLV-C56 mission is scheduled to be launched on Sunday, 30 July 2023 at 06:30 IST / 01:00 UTC. It will be launched from the Satish Dhawan Space Centre, Sriharikota, Andhra Pradesh, India.\n",
      "Low answer relevance : The scheduled launch date and time for the PSLV-C56 mission have not been provided.The PSLV-C56 mission is an important space mission for India. It aims to launch a satellite into orbit to study weather patterns.\n",
      "Table 3: Example from WikiEval, showing answers with high and low answer relevance.\n",
      "Question Context When was the Chimnabai Clock Tower completed, and who was it named af- ter? High context relevance : The Chimnabai Clock Tower, also known as the Raopura Tower, is a clock tower situated in the Raopura area of Vadodara, Gujarat, India. It was completed in 1896 and named in memory of Chimnabai I (1864–1885), a queen and the first wife of Sayajirao Gaekwad III of Baroda State.\n",
      "Low context relevance : The Chimnabai Clock Tower, also known as the Raopura Tower, is a clock tower situated in the Raopura area of Vadodara, Gujarat, India. It was completed in 1896 and named in memory of Chimnabai I (1864–1885), a queen and the first wife of Sayajirao Gaekwad III of Baroda State. It was built in Indo-Saracenic architecture style.\n",
      "History. Chimnabai Clock Tower was built in 1896. The tower was named after Chimnabai I (1864–1885), a queen and the first wife of Sayajirao Gaekwad III of Baroda State. It was inaugurated by Mir Kamaluddin Hussainkhan, the last Nawab of Baroda. During the rule of Gaekwad, it was a stoppage for horse drawn trams. The clock tower was erected at the cost of 25,000 (equivalent to 9.2 million or USD 120,000 in 2023).\n",
      "Table 4: Example from WikiEval, showing answers with high and low context relevance.\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "\n",
    "def clean_extracted_text(text):\n",
    "    import re  # Import regular expressions library\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    last_line_was_heading = False  # To track consecutive heading lines\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # Handle consecutive heading lines\n",
    "        current_line_is_heading = line.startswith(\"#\")\n",
    "        if current_line_is_heading and last_line_was_heading:\n",
    "           line = line.replace('#', '')  # Remove all '#' symbols from the line\n",
    "        last_line_was_heading = current_line_is_heading\n",
    "\n",
    "        # Use a regular expression to match lines starting with '#' followed by one or several spaces and 'CHAPITRE'\n",
    "        if re.match(r'^#\\s+CHAPITRE', line):\n",
    "            line = \"\\n\\n\" + line  # Prepend two line breaks\n",
    "\n",
    "        # Decide on line breaks based on punctuation and specific cases\n",
    "        if line.endswith('.'):\n",
    "            # End of sentence or apostrophe, allow for normal line break\n",
    "            cleaned_lines.append(line + \"\\n\")\n",
    "        else:\n",
    "            # No line break for continuing sentences or lines ending with \"l'\"\n",
    "            cleaned_lines.append(line + \" \")\n",
    "\n",
    "    return ''.join(cleaned_lines).strip()\n",
    "\n",
    "\n",
    "def extract_and_clean_text(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "\n",
    "    for page in doc:\n",
    "        text = \"\"\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        for block in blocks:\n",
    "            if \"lines\" in block:\n",
    "                for line in block[\"lines\"]:\n",
    "                    for span in line[\"spans\"]:\n",
    "                        # Check for bold and italic\n",
    "                        is_bold = span['flags'] & 16  # Bold flag\n",
    "                        is_italic = span['flags'] & 2  # Italic flag\n",
    "                        # if span['size'] == 11.0 and is_bold and is_italic:\n",
    "                        #     text += \"## \" + span['text'] + \"\\n\"  # Consider as subtitle\n",
    "                        if span['size'] >= 14.0:  # Title\n",
    "                            text += \"# \" + span['text'] + \"\\n\"\n",
    "                        # elif span['size'] >= 11.0:  # Subtitle\n",
    "                        #     text += \"## \" + span['text'] + \"\\n\"\n",
    "                        else:\n",
    "                            text += span['text'] + \"\\n\"\n",
    "        # Clean the extracted text before adding it to the full_text\n",
    "        cleaned_text = clean_extracted_text(text)\n",
    "        full_text += cleaned_text\n",
    "\n",
    "    return full_text\n",
    "\n",
    "\n",
    "# Specify the path to your PDF here\n",
    "pdf_path = \"hackaton-mistral-studai/data/RAGAS_09_2023.pdf\"\n",
    "text = extract_and_clean_text(pdf_path)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({unstructured.documents.elements.NarrativeText: 59,\n",
       "         unstructured.documents.elements.ListItem: 28,\n",
       "         unstructured.documents.elements.Title: 12,\n",
       "         unstructured.documents.elements.Text: 5,\n",
       "         unstructured.documents.elements.Table: 4,\n",
       "         unstructured.documents.elements.Formula: 2,\n",
       "         unstructured.documents.elements.Header: 1})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "from collections import Counter\n",
    "\n",
    "# unstructured = os.path.join(\"hackaton-mistral-studai/data/\", \"RAGAS_09_2023.pdf\")\n",
    "elements = partition_pdf(\"hackaton-mistral-studai/data/RAGAS_09_2023.pdf\", strategy=\"hi_res\")\n",
    "display(Counter(type(element) for element in elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RAGAS: Automated Evaluation of Retrieval Augmented Generation',\n",
       " 'Abstract',\n",
       " 'Introduction',\n",
       " '2 Related Work',\n",
       " '3 Evaluation Strategies',\n",
       " '4 The WikiEval Dataset',\n",
       " '5 Experiments',\n",
       " '6 Conclusions',\n",
       " 'References',\n",
       " 'A Examples from WikiEval',\n",
       " 'Question',\n",
       " 'Context']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unstructured\n",
    "# for i in [(type(element), element.text) for element in elements[:] if type(element)==unstructured.documents.elements.Title]:\n",
    "#     print(i)\n",
    "\n",
    "# Create tuples (type(element), element.text) ensuring the elements are Title objects\n",
    "elements_tuples = [(type(element), element.text) for element in elements if isinstance(element, unstructured.documents.elements.Title)]\n",
    "\n",
    "# Extract and print the second column of each tuple\n",
    "chapters = [element[1] for element in elements_tuples]\n",
    "chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chapter: RAGAS: Automated Evaluation of Retrieval Augmented Generation\\nContent: Shahul Es † , Jithin James † , Luis Espinosa-Anke ∗♢ , Steven Schockaert ∗ † Exploding Gradients ∗ CardiffNLP, Cardiff University, United Kingdom ♢ AMPLYFI, United Kingdom shahules786@gmail.com,jamesjithin97@gmail.com {espinosa-ankel,schockaerts1}@cardiff.ac.uk',\n",
       " 'Chapter: Abstract\\nContent: We introduce RAGA S ( R etrieval A ugmented G eneration As sessment), a framework for reference-free evaluation of Retrieval Aug- mented Generation (RAG) pipelines.\\nRAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natu- ral language layer between a user and textual databases, reducing the risk of hallucinations.\\nEvaluating RAG architectures is, however, chal- lenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the gener- ation itself. With RAGA S , we put forward a suite of metrics which can be used to evaluate these different dimensions without having to rely on ground truth human annotations . We posit that such a framework can crucially con- tribute to faster evaluation cycles of RAG archi- tectures, which is especially important given the fast adoption of LLMs.\\n1',\n",
       " 'Chapter: Introduction\\nContent: Language Models (LMs) capture a vast amount of knowledge about the world, which allows them to answer questions without accessing any exter- nal sources. This idea of LMs as repositories of knowledge emerged shortly after the introduction of BERT ( Devlin et al.\\n, 2019 ) and became more firmly established with the introduction of ever larger LMs ( Roberts et al.\\n, 2020 ). While the most recent Large Language Models (LLMs) capture enough knowledge to rival human performance across a wide variety of question answering bench- marks ( Bubeck et al.\\n, 2023 ), the idea of using LLMs as knowledge bases still has two fundamen- tal limitations. First, LLMs are not able to answer questions about events that have happened after they were trained. Second, even the largest models struggle to memorise knowledge that is only rarely mentioned in the training corpus ( Kandpal et al.\\n, 2022 ; Mallen et al.\\n, 2023 ). The standard solution to these issues is to rely on Retrieval Augmented Generation (RAG) ( Lee et al.\\n, 2019 ; Lewis et al.\\n, 2020 ; Guu et al.\\n, 2020 ). Answering a question then essentially involves retrieving relevant pas- sages from a corpus and feeding these passages, along with the original question, to the LM. While initial approaches relied on specialised LMs for retrieval-augmented language modelling ( Khandel- wal et al.\\n, 2020 ; Borgeaud et al.\\n, 2022 ), recent work has suggested that simply adding retrieved docu- ments to the input of a standard LM can also work well ( Khattab et al.\\n, 2022 ; Ram et al.\\n, 2023 ; Shi et al.\\n, 2023 ), thus making it possible to use retrieval- augmented strategies in combination with LLMs that are only available through APIs.\\nWhile the usefulness of retrieval-augmented strategies is clear, their implementation requires a significant amount of tuning, as the overall per- formance will be affected by the retrieval model, the considered corpus, the LM, or the prompt for- mulation, among others. Automated evaluation of retrieval-augmented systems is thus paramount. In practice, RAG systems are often evaluated in terms of the language modelling task itself, i.e. by mea- suring perplexity on some reference corpus. How- ever, such evaluations are not always predictive of downstream performance ( Wang et al.\\n, 2023c ).\\nMoreover, this evaluation strategy relies on the LM probabilities, which are not accessible for some closed models (e.g. ChatGPT and GPT-4). Ques- tion answering is another common evaluation task, but usually only datasets with short extractive an- swers are considered, which may not be represen- tative of how the system will be used.\\nTo address these issues, in this paper we present RAGA S 1 , a framework for the automated assess- 1 RAGA S is available at https://github.com/ explodinggradients/ragas .\\n# arXiv:2309.15217v1  [cs.CL]  26 Sep 2023ment of retrieval augmented generation systems.\\nWe focus on settings where reference answers may not be available, and where we want to estimate different proxies for correctness, in addition to the usefulness of the retrieved passages. The RAGA S framework provides an integration with both llama- index and Langchain , the most widely used frame- works for building RAG solutions, thus enabling developers to easily integrate RAGA S into their standard workflow.',\n",
       " 'Chapter: 2 Related Work\\nContent: Estimating faithfulness using LLMs The prob- lem of detecting hallucinations in LLM generated responses has been extensively studied ( Ji et al.\\n, 2023 ). Several authors have suggested the idea of predicting factuality using a few-shot prompt- ing strategy ( Zhang et al.\\n, 2023 ). Recent analy- ses, however, suggest that existing models struggle with detecting hallucination when using standard prompting strategies ( Li et al.\\n, 2023 ; Azaria and Mitchell , 2023 ). Other approaches rely on linking the generated responses to facts from an external knowledge base ( Min et al.\\n, 2023 ), but this is not always possible.\\nYet another strategy is to inspect the probabili- ties assigned to individual tokens, where we would expect the model to be less confident in halluci- nated answers than in factual ones. For instance, BARTScore ( Yuan et al.\\n, 2021 ) estimates factuality by looking at the conditional probability of the gen- erated text given the input.\\nKadavath et al.\\n( 2022 ) use a variation of this idea. Starting from the ob- servation that LLMs provide well-calibrated proba- bilities when answering multiple-choice questions, they essentially convert the problem of validating model generated answers into a multiple-choice question which asks whether the answer is true or false. Rather than looking at the output probabil- ities, Azaria and Mitchell ( 2023 ) propose to train a supervised classifier on the weights from one of the hidden layers of the LLM, to predict whether a given statement is true or not. While the approach performs well, the need to access the hidden states of the model makes it unsuitable for systems that access LLMs through an API.\\nFor models that do not provide access to token probabilities, such as ChatGPT and GPT-4, differ- ent methods are needed. SelfCheckGPT ( Manakul et al.\\n, 2023 ) addresses this problem by instead sam- pling multiple answers. Their core idea is that factual answers are more stable: when an answer is factual, we can expect that different samples will tend to be semantically similar, whereas this is less likely to be the case for hallucinated answers.\\nAutomated evaluation of text generation systems LLMs have also been leveraged to automatically evaluate other aspects of generated text fragments, beyond factuality. For instance, GPTScore ( Fu et al.\\n, 2023 ) uses a prompt that specifies the consid- ered aspect (e.g. fluency) and then scores passages based on the average probability of the generated tokens, according to a given autoregressive LM.\\nThis idea of using prompts was previously also considered by Yuan et al.\\n( 2021 ), although they used a smaller fine-tuned LM (i.e. BART) and did not observe a clear benefit from using prompts. An- other approach directly asks ChatGPT to evaluate a particular aspect of the given answer by provid- ing a score between 0 and 100, or by providing a rating on a 5-star scale ( Wang et al.\\n, 2023a ). Re- markably, strong results can be obtained in this way, although it comes with the limitation of being sensitive to the design of the prompt. Rather than scoring individual answers, some authors have also focused on using an LLM to select the best answer among a number of candidates ( Wang et al.\\n, 2023b ), typically to compare the performance of different LLMs. However, care is needed with this approach, as the order in which the answers is presented can influence the result ( Wang et al.\\n, 2023b ).\\nIn terms of how ground truth answers or, more generally, generations, have been typically used in the literature, most approaches have relied on the availability of one or more reference answers.\\nFor instance, BERTScore ( Zhang et al.\\n, 2020 ) and MoverScore ( Zhao et al.\\n, 2019 ) use contex- tualised embeddings, produced by a pre-trained BERT model, to compare the similarity between the generated answer and the reference answers.\\nBARTScore ( Yuan et al.\\n, 2021 ) similarly uses refer- ence answers to compute aspects such as precision (estimated as the probability of generating the gen- erated answer given the reference) and recall (esti- mated as the probability of generating the reference given the generated answer).',\n",
       " 'Chapter: 3 Evaluation Strategies\\nContent: We consider a standard RAG setting, where given a question q , the system first retrieves some context c ( q ) and then uses the retrieved context to generate an answer a s ( q ) . When building a RAG system,we usually do not have access to human-annotated datasets or reference answers. We therefore fo- cus on metrics that are fully self-contained and reference-free. We focus in particular three quality aspects, which we argue are of central importance.\\nFirst, Faithfulness refers to the idea that the an- swer should be grounded in the given context. This is important to avoid hallucinations, and to ensure that the retrieved context can act as a justification for the generated answer. Indeed, RAG systems are often used in applications where the factual con- sistency of the generated text w.r.t. the grounded sources is highly important, e.g. in domains such as law, where information is constantly evolving. Sec- ond, Answer Relevance refers to the idea that the generated answer should address the actual ques- tion that was provided. Finally,',\n",
       " 'Chapter: 4 The WikiEval Dataset\\nContent: Relevance refers to the idea that the retrieved context should be focused, containing as little irrelevant informa- tion as possible. This is important given the cost associated with feeding long context passages to LLMs. Moreover, when context passages are too long, LLMs are often less effective in exploiting that context, especially for information that is pro- vided in the middle of the context passage ( Liu et al.\\n, 2023 ).\\nWe now explain how these three quality aspects can be measured in a fully automated way, by prompting an LLM. In our implementation and experiments, all prompts are evaluated using the gpt-3.5-turbo-16k model, which is available through the OpenAI API 2 .\\nFaithfulness We say that the answer a s ( q ) is faithful to the context c ( q ) if the claims that are made in the answer can be inferred from the con- text. To estimate faithfulness, we first use an LLM to extract a set of statements, S ( a s ( q )) . The aim of this step is to decompose longer sentences into shorter and more focused assertions. We use the following prompt for this step 3 : Given a question and answer, create one or more statements from each sentence in the given answer.\\nquestion: [question] answer: [answer] where [question] and [answer] refer to the given question and answer. For each statement s i 2 https://platform.openai.com 3 To help clarify the task, we include a demonstration as part of the prompt. This demonstration is not explicitly shown in the listing of the prompts throughout this paper.\\nin S , the LLM determines if s i can be inferred from c ( q ) using a verification function v ( s i , c ( q )) . This verification step is carried out using the following prompt: Consider the given context and following statements, then determine whether they are supported by the information present in the context. Provide a brief explana- tion for each statement before arriving at the verdict (Yes/No). Provide a final verdict for each statement in order at the end in the given format. Do not deviate from the specified format.\\nstatement: [statement 1] ...\\nstatement: [statement n ] The final faithfulness score, F , is then computed as F = | V | | S | , where | V | is the number of statements that were supported according to the LLM and | S | is the total number of statements.\\nAnswer relevance We say that the answer a s ( q ) is relevant if it directly addresses the question in an appropriate way. In particular, our assessment of answer relevance does not take into account fac- tuality, but penalises cases where the answer is incomplete or where it contains redundant informa- tion. To estimate answer relevance, for the given answer a s ( q ) , we prompt the LLM to generate n potential questions q i based on a s ( q ) , as follows: Generate a question for the given answer.\\nanswer : [answer] We then obtain embeddings for all questions us- ing the text-embedding-ada-002 model, avail- able from the OpenAI API. For each q i , we cal- culate the similarity sim ( q, q i ) with the original question q , as the cosine between the correspond- ing embeddings. The answer relevance score, AR , for question q is then computed as: AR = 1 n n X i =1 sim ( q, q i ) (1) This metric evaluates how closely the generated answer aligns with the initial question or instruc- tion.',\n",
       " 'Chapter: 5 Experiments\\nContent: relevance The context c ( q ) is consid- ered relevant to the extent that it exclusively con- tains information that is needed to answer the ques- tion. In particular, this metric aims to penalise theinclusion of redundant information. To estimate context relevance, given a question q and its con- text c ( q ) , the LLM extracts a subset of sentences, S ext , from c ( q ) that are crucial to answer q , using the following prompt: Please extract relevant sentences from the provided context that can potentially help answer the following question. If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return the phrase \"Insufficient Information\". While extract- ing candidate sentences you’re not al- lowed to make any changes to sentences from given context.\\nThe context relevance score is then computed as: CR = number of extracted sentences total number of sentences in c ( q ) (2)',\n",
       " 'Chapter: 6 Conclusions\\nContent: To evaluate the proposed framework, we ideally need examples of question-context-answer triples which are annotated with human judgments. We can then verify to what extent our metrics agree with human assessments of faithfulness, answer relevance and context relevance. Since we are not aware of any publicly available datasets that could be used for this purpose, we created a new dataset, which we refer to as WikiEval 4 . To construct the dataset, we first selected 50 Wikipedia pages cov- ering events that have happened since the start of 2022 5 . In selecting these pages, we prioritised those with recent edits. For each of the 50 pages, we then asked ChatGPT to suggest a question that can be answered based on the introductory section of the page, using the following prompt: Your task is to formulate a question from given context satisfying the rules given below: 1. The question should be fully answered from the given context.\\n2. The question should be framed from a part that contains non-trivial informa- tion.\\n3. The answer should not contain any 4 https://huggingface.co/datasets/ explodinggradients/WikiEval 5 That is, beyond the reported training cutoff of the model we used in our experiments.\\nlinks.\\n4. The question should be of moderate difficulty.\\n5. The question must be reasonable and must be understood and responded to by humans.\\n6. Do not use phrases that ’provided con- text’, etc in the question context: We also used ChatGPT to answer the generated question, when given the corresponding introduc- tory section as context, using the following prompt: Answer the question using the informa- tion from the given context.\\nquestion: [question] context: [context] All questions were annotated along the three con- sidered quality dimensions by two annotators. Both annotators were fluent in English and were given clear instructions about the meaning of the three considered quality dimensions. For faithfulness and context relevance, the two annotators agreed in around 95% of cases. For answer relevance, they agreed in around 90% of the cases. Disagreements were resolved after a discussion between the anno- tators.\\nFaithfulness To obtain human judgements about faithfulness, we first used ChatGPT to answer the question without access to any additional context.\\nWe then asked the annotators to judge which of the two answers was the most faithful (i.e. the standard one or the one generated without context), given the question and corresponding Wikipedia page.\\nAnswer relevance We first used ChatGPT to obtain candidate answers with lower answer rel- evance, using the following prompt: Answer the given question in an incom- plete manner.\\nquestion: [question] We then asked human annotators to compare this answer, and indicate which of the two answers had the highest answer relevance.',\n",
       " 'Chapter: References\\nContent: relevance To measure this aspect, we first added additional sentences to the context by scraping back-links to the corresponding Wikipedia page. In this way, we were able to add information to the context that was related but less relevant forFaith.\\nAns. Rel.\\nCont. Rel.\\nRAGAs 0.95 0.78 0.70 GPT Score 0.72 0.52 0.63 GPT Ranking 0.54 0.40 0.52 Table 1: Agreement with human annotators in pairwise comparisons of faithfulness, answer relevance and con- text relevance, using the WikEval dataset (accuracy).\\nanswering the question. For the few pages with- out any back-links, we instead used ChatGPT to complete the given context.',\n",
       " 'Chapter: A Examples from WikiEval\\nContent: Table 1 analyses the agreement between the met- rics proposed in Section 3 and the human assess- ments from the proposed WikiEval dataset. Each WikiEval instance requires the model to compare two answers or two context fragments. We count how often the answer/context preferred by the model (i.e. with highest estimated faithfulness, an- swer relevance, or context relevance) coincides with the answer/context preferred by the human annotators. We report the results in terms of ac- curacy (i.e. the fraction of instances on which the model agrees with the annotators).\\nTo put the results in context, we compare our proposed metrics (shown as RAGAs in Table 1 ) with two baseline methods. For the first method, shown as GPT Score , we ask ChatGPT to assign a score between 0 and 10 for the three quality dimensions.\\nTo this end, we use a prompt that describes the meaning of the quality metric and then asks to score the given answer/context in line with that definition. For instance, for evaluating faithfulness, we used the following prompt: Faithfulness measures the information consistency of the answer against the given context. Any claims that are made in the answer that cannot be deduced from context should be penalized.\\nGiven an answer and context, assign a score for faithfulness in the range 0-10.\\ncontext : [context] answer : [answer] Ties, where the same score is assigned by the LLM to both answer candidates, were broken randomly.\\nThe second baseline, shown as GPT Ranking , in- stead asks ChatGPT to select the preferred answer/- context. In this case, the prompt again includes a definition of the considered quality metric. For instance, for evaluating answer relevance, we used the following prompt: Answer Relevancy measures the degree to which a response directly addresses and is appropriate for a given question.\\nIt penalizes the present of redundant in- formation or incomplete answers given a question. Given an question and answer, rank each answer based on Answer Rele- vancy.\\nquestion : [question] answer 1 : [answer 1] answer 2 : [answer 2] The results in Table 1 show that our proposed metrics are much closer aligned with the human judgements than the predictions from the two base- lines. For faithfulness, the RAGAs prediction are in general highly accurate. For answer relevance, the agreement is lower, but this is largely due to the fact that the differences between the two candidate answers are often very subtle. We found context relevance to be the hardest quality dimension to evaluate. In particular, we observed that ChatGPT often struggles with the task of selecting the sen- tences from the context that are crucial, especially for longer contexts.',\n",
       " 'Chapter: Question\\nContent: We have highlighted the need for automated reference-free evaluation of RAG systems. In par- ticular, we have argued the need for an evaluation framework that can assess faithfulness (i.e. is the answer grounded in the retrieved context), answer relevance (i.e. does the answer address the ques- tion) and context relevance (i.e. is the retrieved context sufficiently focused). To support the devel- opment of such a framework, we have introduced WikiEval , a dataset which human judgements of these three different aspects. Finally, we have also described RAGAs, our implementation of the three considered quality aspects. This framework is easy to use and can provide deverlopers of RAG sys- tems with valuable insights, even in the absence of any ground truth. Our evaluation on WikiEval has shown that the predictions from RAGAs are closely aligned with human predictions, especially for faithfulness and answer relevance.',\n",
       " 'Chapter: Context\\nContent: Amos Azaria and Tom M. Mitchell. 2023.\\nThe inter- nal state of an LLM knows when its lying .\\nCoRR , abs/2304.13734.\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Si- monyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.\\n2022.\\nImproving language models by retrieving from trillions of tokens . In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Bal- timore, Maryland, USA , volume 162 of Proceedings of Machine Learning Research , pages 2206–2240.\\nPMLR.\\nSébastien Bubeck, Varun Chandrasekaran, Ronen El- dan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund- berg, et al. 2023. Sparks of artificial general intelli- gence: Early experiments with gpt-4.\\narXiv preprint arXiv:2303.12712 .\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.\\nBERT: Pre-training of deep bidirectional transformers for language under- standing . In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers) , pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.\\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023.\\nGptscore: Evaluate as you desire .\\nCoRR , abs/2302.04166.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu- pat, and Mingwei Chang. 2020. Retrieval augmented language model pre-training. In International confer- ence on machine learning , pages 3929–3938. PMLR.\\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of halluci- nation in natural language generation.\\nACM Comput- ing Surveys , 55(12):1–38.\\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jack- son Kernion, Shauna Kravec, Liane Lovitt, Ka- mal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. 2022.\\nLanguage models (mostly) know what they know .\\nCoRR , abs/2207.05221.\\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2022.\\nLarge language models struggle to learn long-tail knowledge .\\nCoRR , abs/2211.08411.\\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020.\\nGeneralization through memorization: Nearest neighbor language models . In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net.\\nOmar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. 2022.\\nDemonstrate-search-predict: Composing retrieval and language models for knowledge-intensive NLP .\\nCoRR , abs/2212.14024.\\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\\n2019. Latent retrieval for weakly supervised open do- main question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 6086–6096.\\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik- tus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020.\\nRetrieval-augmented generation for knowledge-intensive NLP tasks . In Advances in Neu- ral Information Processing Systems 33: Annual Con- ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual .\\nJunyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023.\\nHalueval: A large- scale hallucination evaluation benchmark for large language models .\\nCoRR , abs/2305.11747.\\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran- jape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023.\\nLost in the middle: How language models use long contexts .\\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023.\\nWhen not to trust language models: Investigating effectiveness of parametric and non-parametric mem- ories . In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers) , pages 9802–9822, Toronto, Canada. Association for Computational Linguistics.\\nPotsawee Manakul, Adian Liusie, and Mark J. F. Gales.\\n2023.\\nSelfcheckgpt: Zero-resource black-box hal- lucination detection for generative large language models .\\nCoRR , abs/2303.08896.\\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.\\nFactscore: Fine-grained atomic evaluation of fac- tual precision in long form text generation .\\nCoRR , abs/2305.14251.Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023.\\nIn-context retrieval-augmented lan- guage models .\\nCoRR , abs/2302.00083.\\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\\nHow much knowledge can you pack into the param- eters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 5418–5426, Online. Association for Computational Linguistics.\\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023.\\nREPLUG: retrieval-augmented black-box language models .\\nCoRR , abs/2301.12652.\\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxi- ang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023a.\\nIs chatgpt a good NLG evaluator? A preliminary study .\\nCoRR , abs/2303.04048.\\nPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui.\\n2023b.\\nLarge language models are not fair evaluators .\\nCoRR , abs/2305.17926.\\nShufan Wang, Yixiao Song, Andrew Drozdov, Aparna Garimella, Varun Manjunatha, and Mohit Iyyer.\\n2023c.\\nKNN-LM does not improve open-ended text generation .\\nCoRR , abs/2305.14625.\\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\\nBartscore: Evaluating generated text as text genera- tion . In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Informa- tion Processing Systems 2021, NeurIPS 2021, De- cember 6-14, 2021, virtual , pages 27263–27277.\\nTianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei Fang, Luc Gaitskell, Thomas Hartvigsen, Xixin Wu, Danny Fox, Helen Meng, and James R. Glass. 2023.\\nInterpretable unified language checking .\\nCoRR , abs/2304.03728.\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\\nWeinberger, and Yoav Artzi. 2020.\\nBertscore: Evalu- ating text generation with BERT . In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenRe- view.net.\\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris- tian M. Meyer, and Steffen Eger. 2019.\\nMoverScore: Text generation evaluating with contextualized em- beddings and earth mover distance . In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna- tional Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP) , pages 563–578, Hong Kong, China. Association for Computational Lin- guistics.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a regular expression pattern to match the section titles\n",
    "pattern = '|'.join([re.escape(section) for section in chapters])\n",
    "split_text = re.split(pattern, text)\n",
    "\n",
    "# The split_text list will contain empty strings at the start and end, and the text of sections in between\n",
    "# Remove empty strings from the list\n",
    "split_text = [section for section in split_text if section.strip()]\n",
    "\n",
    "# Create a dictionary with section titles as keys and section texts as values\n",
    "section_texts = {}\n",
    "for i, section in enumerate(chapters):\n",
    "    if i < len(split_text):\n",
    "        section_texts[section] = split_text[i+1].strip()\n",
    "\n",
    "structured_text = []\n",
    "# Print the sections and their corresponding texts\n",
    "for section, content in section_texts.items():\n",
    "    structured_text.append(f\"Chapter: {section}\\nContent: {content}\")\n",
    "    # print(f\"Section: {section}\\nContent: {content}\\n{'-'*40}\")\n",
    "\n",
    "structured_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to transform the array\n",
    "def transform_array(array):\n",
    "    transformed_array = []\n",
    "    \n",
    "    for entry in array:\n",
    "        # Extract chapter title\n",
    "        chapter_title_match = re.match(r'Chapter: ([^\\n]+)', entry)\n",
    "        if not chapter_title_match:\n",
    "            continue\n",
    "        chapter_title = chapter_title_match.group(1)\n",
    "        \n",
    "        # Extract content\n",
    "        content_match = re.search(r'Content: (.+)', entry, re.DOTALL)\n",
    "        if not content_match:\n",
    "            continue\n",
    "        content = content_match.group(1)\n",
    "        \n",
    "        # Split content into sentences\n",
    "        sentences = re.split(r'(?<=\\.) ', content)\n",
    "        \n",
    "        # Prepend chapter title to each sentence and add to transformed array\n",
    "        for sentence in sentences:\n",
    "            transformed_array.append(f'Chapter: {chapter_title}\\n{sentence.strip()}')\n",
    "    \n",
    "    return transformed_array\n",
    "\n",
    "# Transform the array\n",
    "structured_text = transform_array(structured_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Chapter: RAGAS: Automated Evaluation of Retrieval Augmented Generation\\nShahul Es † , Jithin James † , Luis Espinosa-Anke ∗♢ , Steven Schockaert ∗ † Exploding Gradients ∗ CardiffNLP, Cardiff University, United Kingdom ♢ AMPLYFI, United Kingdom shahules786@gmail.com,jamesjithin97@gmail.com {espinosa-ankel,schockaerts1}@cardiff.ac.uk '),\n",
       " Document(page_content='Chapter: Abstract\\nWe introduce RAGA S ( R etrieval A ugmented G eneration As sessment), a framework for reference-free evaluation of Retrieval Aug- mented Generation (RAG) pipelines.\\nRAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natu- ral language layer between a user and textual databases, reducing the risk of hallucinations.\\nEvaluating RAG architectures is, however, chal- lenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the gener- ation itself. '),\n",
       " Document(page_content='Chapter: Abstract\\nWith RAGA S , we put forward a suite of metrics which can be used to evaluate these different dimensions without having to rely on ground truth human annotations . '),\n",
       " Document(page_content='Chapter: Abstract\\nWe posit that such a framework can crucially con- tribute to faster evaluation cycles of RAG archi- tectures, which is especially important given the fast adoption of LLMs.\\n1 '),\n",
       " Document(page_content='Chapter: Introduction\\nLanguage Models (LMs) capture a vast amount of knowledge about the world, which allows them to answer questions without accessing any exter- nal sources. '),\n",
       " Document(page_content='Chapter: Introduction\\nThis idea of LMs as repositories of knowledge emerged shortly after the introduction of BERT ( Devlin et al.\\n, 2019 ) and became more firmly established with the introduction of ever larger LMs ( Roberts et al.\\n, 2020 ). '),\n",
       " Document(page_content='Chapter: Introduction\\nWhile the most recent Large Language Models (LLMs) capture enough knowledge to rival human performance across a wide variety of question answering bench- marks ( Bubeck et al.\\n, 2023 ), the idea of using LLMs as knowledge bases still has two fundamen- tal limitations. '),\n",
       " Document(page_content='Chapter: Introduction\\nFirst, LLMs are not able to answer questions about events that have happened after they were trained. '),\n",
       " Document(page_content='Chapter: Introduction\\nSecond, even the largest models struggle to memorise knowledge that is only rarely mentioned in the training corpus ( Kandpal et al.\\n, 2022 ; Mallen et al.\\n, 2023 ). '),\n",
       " Document(page_content='Chapter: Introduction\\nThe standard solution to these issues is to rely on Retrieval Augmented Generation (RAG) ( Lee et al.\\n, 2019 ; Lewis et al.\\n, 2020 ; Guu et al.\\n, 2020 ). '),\n",
       " Document(page_content='Chapter: Introduction\\nAnswering a question then essentially involves retrieving relevant pas- sages from a corpus and feeding these passages, along with the original question, to the LM. '),\n",
       " Document(page_content='Chapter: Introduction\\nWhile initial approaches relied on specialised LMs for retrieval-augmented language modelling ( Khandel- wal et al.\\n, 2020 ; Borgeaud et al.\\n, 2022 ), recent work has suggested that simply adding retrieved docu- ments to the input of a standard LM can also work well ( Khattab et al.\\n, 2022 ; Ram et al.\\n, 2023 ; Shi et al.\\n, 2023 ), thus making it possible to use retrieval- augmented strategies in combination with LLMs that are only available through APIs.\\nWhile the usefulness of retrieval-augmented strategies is clear, their implementation requires a significant amount of tuning, as the overall per- formance will be affected by the retrieval model, the considered corpus, the LM, or the prompt for- mulation, among others. '),\n",
       " Document(page_content='Chapter: Introduction\\nAutomated evaluation of retrieval-augmented systems is thus paramount.'),\n",
       " Document(page_content='Chapter: Introduction\\nIn practice, RAG systems are often evaluated in terms of the language modelling task itself, i.e. '),\n",
       " Document(page_content='Chapter: Introduction\\nby mea- suring perplexity on some reference corpus.'),\n",
       " Document(page_content='Chapter: Introduction\\nHow- ever, such evaluations are not always predictive of downstream performance ( Wang et al.\\n, 2023c ).\\nMoreover, this evaluation strategy relies on the LM probabilities, which are not accessible for some closed models (e.g. '),\n",
       " Document(page_content='Chapter: Introduction\\nChatGPT and GPT-4).'),\n",
       " Document(page_content='Chapter: Introduction\\nQues- tion answering is another common evaluation task, but usually only datasets with short extractive an- swers are considered, which may not be represen- tative of how the system will be used.\\nTo address these issues, in this paper we present RAGA S 1 , a framework for the automated assess- 1 RAGA S is available at https://github.com/ explodinggradients/ragas .\\n# arXiv:2309.15217v1  [cs.CL]  26 Sep 2023ment of retrieval augmented generation systems.\\nWe focus on settings where reference answers may not be available, and where we want to estimate different proxies for correctness, in addition to the usefulness of the retrieved passages. '),\n",
       " Document(page_content='Chapter: Introduction\\nThe RAGA S framework provides an integration with both llama- index and Langchain , the most widely used frame- works for building RAG solutions, thus enabling developers to easily integrate RAGA S into their standard workflow. '),\n",
       " Document(page_content='Chapter: 2 Related Work\\nEstimating faithfulness using LLMs The prob- lem of detecting hallucinations in LLM generated responses has been extensively studied ( Ji et al.\\n, 2023 ). '),\n",
       " Document(page_content='Chapter: 2 Related Work\\nSeveral authors have suggested the idea of predicting factuality using a few-shot prompt- ing strategy ( Zhang et al.\\n, 2023 ). '),\n",
       " Document(page_content='Chapter: 2 Related Work\\nRecent analy- ses, however, suggest that existing models struggle with detecting hallucination when using standard prompting strategies ( Li et al.\\n, 2023 ; Azaria and Mitchell , 2023 ). '),\n",
       " Document(page_content='Chapter: 2 Related Work\\nOther approaches rely on linking the generated responses to facts from an external knowledge base ( Min et al.\\n, 2023 ), but this is not always possible.\\nYet another strategy is to inspect the probabili- ties assigned to individual tokens, where we would expect the model to be less confident in halluci- nated answers than in factual ones. '),\n",
       " Document(page_content='Chapter: 2 Related Work\\nFor instance, BARTScore ( Yuan et al.\\n, 2021 ) estimates factuality by looking at the conditional probability of the gen- erated text given the input.\\nKadavath et al.\\n( 2022 ) use a variation of this idea. '),\n",
       " Document(page_content='Chapter: 2 Related Work\\nStarting from the ob- servation that LLMs provide well-calibrated proba- bilities when answering multiple-choice questions, they essentially convert the problem of validating model generated answers into a multiple-choice question which asks whether the answer is true or false. '),\n",
       " Document(page_content='Chapter: 2 Related Work\\nRather than looking at the output probabil- ities, Azaria and Mitchell ( 2023 ) propose to train a supervised classifier on the weights from one of the hidden layers of the LLM, to predict whether a given statement is true or not. '),\n",
       " Document(page_content='Chapter: 2 Related Work\\nWhile the approach performs well, the need to access the hidden states of the model makes it unsuitable for systems that access LLMs through an API.\\nFor models that do not provide access to token probabilities, such as ChatGPT and GPT-4, differ- ent methods are needed. '),\n",
       " Document(page_content='Chapter: 2 Related Work\\nSelfCheckGPT ( Manakul et al.\\n, 2023 ) addresses this problem by instead sam- pling multiple answers. '),\n",
       " Document(page_content='Chapter: 2 Related Work\\nTheir core idea is that factual answers are more stable: when an answer is factual, we can expect that different samples will tend to be semantically similar, whereas this is less likely to be the case for hallucinated answers.\\nAutomated evaluation of text generation systems LLMs have also been leveraged to automatically evaluate other aspects of generated text fragments, beyond factuality. '),\n",
       " Document(page_content='Chapter: 2 Related Work\\nFor instance, GPTScore ( Fu et al.\\n, 2023 ) uses a prompt that specifies the consid- ered aspect (e.g. '),\n",
       " Document(page_content='Chapter: 2 Related Work\\nfluency) and then scores passages based on the average probability of the generated tokens, according to a given autoregressive LM.\\nThis idea of using prompts was previously also considered by Yuan et al.\\n( 2021 ), although they used a smaller fine-tuned LM (i.e. '),\n",
       " Document(page_content='Chapter: 2 Related Work\\nBART) and did not observe a clear benefit from using prompts.'),\n",
       " Document(page_content='Chapter: 2 Related Work\\nAn- other approach directly asks ChatGPT to evaluate a particular aspect of the given answer by provid- ing a score between 0 and 100, or by providing a rating on a 5-star scale ( Wang et al.\\n, 2023a ). '),\n",
       " Document(page_content='Chapter: 2 Related Work\\nRe- markably, strong results can be obtained in this way, although it comes with the limitation of being sensitive to the design of the prompt. '),\n",
       " Document(page_content='Chapter: 2 Related Work\\nRather than scoring individual answers, some authors have also focused on using an LLM to select the best answer among a number of candidates ( Wang et al.\\n, 2023b ), typically to compare the performance of different LLMs. '),\n",
       " Document(page_content='Chapter: 2 Related Work\\nHowever, care is needed with this approach, as the order in which the answers is presented can influence the result ( Wang et al.\\n, 2023b ).\\nIn terms of how ground truth answers or, more generally, generations, have been typically used in the literature, most approaches have relied on the availability of one or more reference answers.\\nFor instance, BERTScore ( Zhang et al.\\n, 2020 ) and MoverScore ( Zhao et al.\\n, 2019 ) use contex- tualised embeddings, produced by a pre-trained BERT model, to compare the similarity between the generated answer and the reference answers.\\nBARTScore ( Yuan et al.\\n, 2021 ) similarly uses refer- ence answers to compute aspects such as precision (estimated as the probability of generating the gen- erated answer given the reference) and recall (esti- mated as the probability of generating the reference given the generated answer). '),\n",
       " Document(page_content='Chapter: 3 Evaluation Strategies\\nWe consider a standard RAG setting, where given a question q , the system first retrieves some context c ( q ) and then uses the retrieved context to generate an answer a s ( q ) . '),\n",
       " Document(page_content='Chapter: 3 Evaluation Strategies\\nWhen building a RAG system,we usually do not have access to human-annotated datasets or reference answers. '),\n",
       " Document(page_content='Chapter: 3 Evaluation Strategies\\nWe therefore fo- cus on metrics that are fully self-contained and reference-free. '),\n",
       " Document(page_content='Chapter: 3 Evaluation Strategies\\nWe focus in particular three quality aspects, which we argue are of central importance.\\nFirst, Faithfulness refers to the idea that the an- swer should be grounded in the given context. '),\n",
       " Document(page_content='Chapter: 3 Evaluation Strategies\\nThis is important to avoid hallucinations, and to ensure that the retrieved context can act as a justification for the generated answer. '),\n",
       " Document(page_content='Chapter: 3 Evaluation Strategies\\nIndeed, RAG systems are often used in applications where the factual con- sistency of the generated text w.r.t. '),\n",
       " Document(page_content='Chapter: 3 Evaluation Strategies\\nthe grounded sources is highly important, e.g.'),\n",
       " Document(page_content='Chapter: 3 Evaluation Strategies\\nin domains such as law, where information is constantly evolving.'),\n",
       " Document(page_content='Chapter: 3 Evaluation Strategies\\nSec- ond, Answer Relevance refers to the idea that the generated answer should address the actual ques- tion that was provided. '),\n",
       " Document(page_content='Chapter: 3 Evaluation Strategies\\nFinally,'),\n",
       " Document(page_content='Chapter: 4 The WikiEval Dataset\\nRelevance refers to the idea that the retrieved context should be focused, containing as little irrelevant informa- tion as possible. '),\n",
       " Document(page_content='Chapter: 4 The WikiEval Dataset\\nThis is important given the cost associated with feeding long context passages to LLMs. '),\n",
       " Document(page_content='Chapter: 4 The WikiEval Dataset\\nMoreover, when context passages are too long, LLMs are often less effective in exploiting that context, especially for information that is pro- vided in the middle of the context passage ( Liu et al.\\n, 2023 ).\\nWe now explain how these three quality aspects can be measured in a fully automated way, by prompting an LLM. '),\n",
       " Document(page_content='Chapter: 4 The WikiEval Dataset\\nIn our implementation and experiments, all prompts are evaluated using the gpt-3.5-turbo-16k model, which is available through the OpenAI API 2 .\\nFaithfulness We say that the answer a s ( q ) is faithful to the context c ( q ) if the claims that are made in the answer can be inferred from the con- text. '),\n",
       " Document(page_content='Chapter: 4 The WikiEval Dataset\\nTo estimate faithfulness, we first use an LLM to extract a set of statements, S ( a s ( q )) . '),\n",
       " Document(page_content='Chapter: 4 The WikiEval Dataset\\nThe aim of this step is to decompose longer sentences into shorter and more focused assertions. '),\n",
       " Document(page_content='Chapter: 4 The WikiEval Dataset\\nWe use the following prompt for this step 3 : Given a question and answer, create one or more statements from each sentence in the given answer.\\nquestion: [question] answer: [answer] where [question] and [answer] refer to the given question and answer. '),\n",
       " Document(page_content='Chapter: 4 The WikiEval Dataset\\nFor each statement s i 2 https://platform.openai.com 3 To help clarify the task, we include a demonstration as part of the prompt. '),\n",
       " Document(page_content='Chapter: 4 The WikiEval Dataset\\nThis demonstration is not explicitly shown in the listing of the prompts throughout this paper.\\nin S , the LLM determines if s i can be inferred from c ( q ) using a verification function v ( s i , c ( q )) . '),\n",
       " Document(page_content='Chapter: 4 The WikiEval Dataset\\nThis verification step is carried out using the following prompt: Consider the given context and following statements, then determine whether they are supported by the information present in the context. '),\n",
       " Document(page_content='Chapter: 4 The WikiEval Dataset\\nProvide a brief explana- tion for each statement before arriving at the verdict (Yes/No). '),\n",
       " Document(page_content='Chapter: 4 The WikiEval Dataset\\nProvide a final verdict for each statement in order at the end in the given format. '),\n",
       " Document(page_content='Chapter: 4 The WikiEval Dataset\\nDo not deviate from the specified format.\\nstatement: [statement 1] ...\\nstatement: [statement n ] The final faithfulness score, F , is then computed as F = | V | | S | , where | V | is the number of statements that were supported according to the LLM and | S | is the total number of statements.\\nAnswer relevance We say that the answer a s ( q ) is relevant if it directly addresses the question in an appropriate way. '),\n",
       " Document(page_content='Chapter: 4 The WikiEval Dataset\\nIn particular, our assessment of answer relevance does not take into account fac- tuality, but penalises cases where the answer is incomplete or where it contains redundant informa- tion. '),\n",
       " Document(page_content='Chapter: 4 The WikiEval Dataset\\nTo estimate answer relevance, for the given answer a s ( q ) , we prompt the LLM to generate n potential questions q i based on a s ( q ) , as follows: Generate a question for the given answer.\\nanswer : [answer] We then obtain embeddings for all questions us- ing the text-embedding-ada-002 model, avail- able from the OpenAI API. '),\n",
       " Document(page_content='Chapter: 4 The WikiEval Dataset\\nFor each q i , we cal- culate the similarity sim ( q, q i ) with the original question q , as the cosine between the correspond- ing embeddings. '),\n",
       " Document(page_content='Chapter: 4 The WikiEval Dataset\\nThe answer relevance score, AR , for question q is then computed as: AR = 1 n n X i =1 sim ( q, q i ) (1) This metric evaluates how closely the generated answer aligns with the initial question or instruc- tion. '),\n",
       " Document(page_content='Chapter: 5 Experiments\\nrelevance The context c ( q ) is consid- ered relevant to the extent that it exclusively con- tains information that is needed to answer the ques- tion. '),\n",
       " Document(page_content='Chapter: 5 Experiments\\nIn particular, this metric aims to penalise theinclusion of redundant information. '),\n",
       " Document(page_content='Chapter: 5 Experiments\\nTo estimate context relevance, given a question q and its con- text c ( q ) , the LLM extracts a subset of sentences, S ext , from c ( q ) that are crucial to answer q , using the following prompt: Please extract relevant sentences from the provided context that can potentially help answer the following question. '),\n",
       " Document(page_content='Chapter: 5 Experiments\\nIf no relevant sentences are found, or if you believe the question cannot be answered from the given context, return the phrase \"Insufficient Information\". '),\n",
       " Document(page_content='Chapter: 5 Experiments\\nWhile extract- ing candidate sentences you’re not al- lowed to make any changes to sentences from given context.\\nThe context relevance score is then computed as: CR = number of extracted sentences total number of sentences in c ( q ) (2) '),\n",
       " Document(page_content='Chapter: 6 Conclusions\\nTo evaluate the proposed framework, we ideally need examples of question-context-answer triples which are annotated with human judgments. '),\n",
       " Document(page_content='Chapter: 6 Conclusions\\nWe can then verify to what extent our metrics agree with human assessments of faithfulness, answer relevance and context relevance. '),\n",
       " Document(page_content='Chapter: 6 Conclusions\\nSince we are not aware of any publicly available datasets that could be used for this purpose, we created a new dataset, which we refer to as WikiEval 4 . '),\n",
       " Document(page_content='Chapter: 6 Conclusions\\nTo construct the dataset, we first selected 50 Wikipedia pages cov- ering events that have happened since the start of 2022 5 . '),\n",
       " Document(page_content='Chapter: 6 Conclusions\\nIn selecting these pages, we prioritised those with recent edits.'),\n",
       " Document(page_content='Chapter: 6 Conclusions\\nFor each of the 50 pages, we then asked ChatGPT to suggest a question that can be answered based on the introductory section of the page, using the following prompt: Your task is to formulate a question from given context satisfying the rules given below: 1. '),\n",
       " Document(page_content='Chapter: 6 Conclusions\\nThe question should be fully answered from the given context.\\n2.'),\n",
       " Document(page_content='Chapter: 6 Conclusions\\nThe question should be framed from a part that contains non-trivial informa- tion.\\n3. '),\n",
       " Document(page_content='Chapter: 6 Conclusions\\nThe answer should not contain any 4 https://huggingface.co/datasets/ explodinggradients/WikiEval 5 That is, beyond the reported training cutoff of the model we used in our experiments.\\nlinks.\\n4. '),\n",
       " Document(page_content='Chapter: 6 Conclusions\\nThe question should be of moderate difficulty.\\n5.'),\n",
       " Document(page_content='Chapter: 6 Conclusions\\nThe question must be reasonable and must be understood and responded to by humans.\\n6. '),\n",
       " Document(page_content='Chapter: 6 Conclusions\\nDo not use phrases that ’provided con- text’, etc in the question context: We also used ChatGPT to answer the generated question, when given the corresponding introduc- tory section as context, using the following prompt: Answer the question using the informa- tion from the given context.\\nquestion: [question] context: [context] All questions were annotated along the three con- sidered quality dimensions by two annotators. '),\n",
       " Document(page_content='Chapter: 6 Conclusions\\nBoth annotators were fluent in English and were given clear instructions about the meaning of the three considered quality dimensions. '),\n",
       " Document(page_content='Chapter: 6 Conclusions\\nFor faithfulness and context relevance, the two annotators agreed in around 95% of cases. '),\n",
       " Document(page_content='Chapter: 6 Conclusions\\nFor answer relevance, they agreed in around 90% of the cases.'),\n",
       " Document(page_content='Chapter: 6 Conclusions\\nDisagreements were resolved after a discussion between the anno- tators.\\nFaithfulness To obtain human judgements about faithfulness, we first used ChatGPT to answer the question without access to any additional context.\\nWe then asked the annotators to judge which of the two answers was the most faithful (i.e. '),\n",
       " Document(page_content='Chapter: 6 Conclusions\\nthe standard one or the one generated without context), given the question and corresponding Wikipedia page.\\nAnswer relevance We first used ChatGPT to obtain candidate answers with lower answer rel- evance, using the following prompt: Answer the given question in an incom- plete manner.\\nquestion: [question] We then asked human annotators to compare this answer, and indicate which of the two answers had the highest answer relevance. '),\n",
       " Document(page_content='Chapter: References\\nrelevance To measure this aspect, we first added additional sentences to the context by scraping back-links to the corresponding Wikipedia page. '),\n",
       " Document(page_content='Chapter: References\\nIn this way, we were able to add information to the context that was related but less relevant forFaith.\\nAns. '),\n",
       " Document(page_content='Chapter: References\\nRel.\\nCont.'),\n",
       " Document(page_content='Chapter: References\\nRel.\\nRAGAs 0.95 0.78 0.70 GPT Score 0.72 0.52 0.63 GPT Ranking 0.54 0.40 0.52 Table 1: Agreement with human annotators in pairwise comparisons of faithfulness, answer relevance and con- text relevance, using the WikEval dataset (accuracy).\\nanswering the question. '),\n",
       " Document(page_content='Chapter: References\\nFor the few pages with- out any back-links, we instead used ChatGPT to complete the given context. '),\n",
       " Document(page_content='Chapter: A Examples from WikiEval\\nTable 1 analyses the agreement between the met- rics proposed in Section 3 and the human assess- ments from the proposed WikiEval dataset. '),\n",
       " Document(page_content='Chapter: A Examples from WikiEval\\nEach WikiEval instance requires the model to compare two answers or two context fragments. '),\n",
       " Document(page_content='Chapter: A Examples from WikiEval\\nWe count how often the answer/context preferred by the model (i.e. '),\n",
       " Document(page_content='Chapter: A Examples from WikiEval\\nwith highest estimated faithfulness, an- swer relevance, or context relevance) coincides with the answer/context preferred by the human annotators. '),\n",
       " Document(page_content='Chapter: A Examples from WikiEval\\nWe report the results in terms of ac- curacy (i.e.'),\n",
       " Document(page_content='Chapter: A Examples from WikiEval\\nthe fraction of instances on which the model agrees with the annotators).\\nTo put the results in context, we compare our proposed metrics (shown as RAGAs in Table 1 ) with two baseline methods. '),\n",
       " Document(page_content='Chapter: A Examples from WikiEval\\nFor the first method, shown as GPT Score , we ask ChatGPT to assign a score between 0 and 10 for the three quality dimensions.\\nTo this end, we use a prompt that describes the meaning of the quality metric and then asks to score the given answer/context in line with that definition. '),\n",
       " Document(page_content='Chapter: A Examples from WikiEval\\nFor instance, for evaluating faithfulness, we used the following prompt: Faithfulness measures the information consistency of the answer against the given context. '),\n",
       " Document(page_content='Chapter: A Examples from WikiEval\\nAny claims that are made in the answer that cannot be deduced from context should be penalized.\\nGiven an answer and context, assign a score for faithfulness in the range 0-10.\\ncontext : [context] answer : [answer] Ties, where the same score is assigned by the LLM to both answer candidates, were broken randomly.\\nThe second baseline, shown as GPT Ranking , in- stead asks ChatGPT to select the preferred answer/- context. '),\n",
       " Document(page_content='Chapter: A Examples from WikiEval\\nIn this case, the prompt again includes a definition of the considered quality metric. '),\n",
       " Document(page_content='Chapter: A Examples from WikiEval\\nFor instance, for evaluating answer relevance, we used the following prompt: Answer Relevancy measures the degree to which a response directly addresses and is appropriate for a given question.\\nIt penalizes the present of redundant in- formation or incomplete answers given a question. '),\n",
       " Document(page_content='Chapter: A Examples from WikiEval\\nGiven an question and answer, rank each answer based on Answer Rele- vancy.\\nquestion : [question] answer 1 : [answer 1] answer 2 : [answer 2] The results in Table 1 show that our proposed metrics are much closer aligned with the human judgements than the predictions from the two base- lines. '),\n",
       " Document(page_content='Chapter: A Examples from WikiEval\\nFor faithfulness, the RAGAs prediction are in general highly accurate. '),\n",
       " Document(page_content='Chapter: A Examples from WikiEval\\nFor answer relevance, the agreement is lower, but this is largely due to the fact that the differences between the two candidate answers are often very subtle. '),\n",
       " Document(page_content='Chapter: A Examples from WikiEval\\nWe found context relevance to be the hardest quality dimension to evaluate. '),\n",
       " Document(page_content='Chapter: A Examples from WikiEval\\nIn particular, we observed that ChatGPT often struggles with the task of selecting the sen- tences from the context that are crucial, especially for longer contexts. '),\n",
       " Document(page_content='Chapter: Question\\nWe have highlighted the need for automated reference-free evaluation of RAG systems. '),\n",
       " Document(page_content='Chapter: Question\\nIn par- ticular, we have argued the need for an evaluation framework that can assess faithfulness (i.e. '),\n",
       " Document(page_content='Chapter: Question\\nis the answer grounded in the retrieved context), answer relevance (i.e.'),\n",
       " Document(page_content='Chapter: Question\\ndoes the answer address the ques- tion) and context relevance (i.e.'),\n",
       " Document(page_content='Chapter: Question\\nis the retrieved context sufficiently focused).'),\n",
       " Document(page_content='Chapter: Question\\nTo support the devel- opment of such a framework, we have introduced WikiEval , a dataset which human judgements of these three different aspects. '),\n",
       " Document(page_content='Chapter: Question\\nFinally, we have also described RAGAs, our implementation of the three considered quality aspects. '),\n",
       " Document(page_content='Chapter: Question\\nThis framework is easy to use and can provide deverlopers of RAG sys- tems with valuable insights, even in the absence of any ground truth. '),\n",
       " Document(page_content='Chapter: Question\\nOur evaluation on WikiEval has shown that the predictions from RAGAs are closely aligned with human predictions, especially for faithfulness and answer relevance. '),\n",
       " Document(page_content='Chapter: Context\\nAmos Azaria and Tom M. Chapter: Context\\nMitchell.'),\n",
       " Document(page_content='Chapter: Context\\n2023.\\nThe inter- nal state of an LLM knows when its lying .\\nCoRR , abs/2304.13734.\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Si- monyan, Jack W. '),\n",
       " Document(page_content='Chapter: Context\\nRae, Erich Elsen, and Laurent Sifre.\\n2022.\\nImproving language models by retrieving from trillions of tokens . '),\n",
       " Document(page_content='Chapter: Context\\nIn International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Bal- timore, Maryland, USA , volume 162 of Proceedings of Machine Learning Research , pages 2206–2240.\\nPMLR.\\nSébastien Bubeck, Varun Chandrasekaran, Ronen El- dan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund- berg, et al. '),\n",
       " Document(page_content='Chapter: Context\\n2023.'),\n",
       " Document(page_content='Chapter: Context\\nSparks of artificial general intelli- gence: Early experiments with gpt-4.\\narXiv preprint arXiv:2303.12712 .\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. '),\n",
       " Document(page_content='Chapter: Context\\n2019.\\nBERT: Pre-training of deep bidirectional transformers for language under- standing . '),\n",
       " Document(page_content='Chapter: Context\\nIn Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers) , pages 4171–4186, Minneapolis, Minnesota. '),\n",
       " Document(page_content='Chapter: Context\\nAssociation for Computational Linguistics.\\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. '),\n",
       " Document(page_content='Chapter: Context\\n2023.\\nGptscore: Evaluate as you desire .\\nCoRR , abs/2302.04166.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu- pat, and Mingwei Chang. '),\n",
       " Document(page_content='Chapter: Context\\n2020. Chapter: Context\\nRetrieval augmented language model pre-training.'),\n",
       " Document(page_content='Chapter: Context\\nIn International confer- ence on machine learning , pages 3929–3938.'),\n",
       " Document(page_content='Chapter: Context\\nPMLR.\\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. '),\n",
       " Document(page_content='Chapter: Context\\n2023.'),\n",
       " Document(page_content='Chapter: Context\\nSurvey of halluci- nation in natural language generation.\\nACM Comput- ing Surveys , 55(12):1–38.\\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jack- son Kernion, Shauna Kravec, Liane Lovitt, Ka- mal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. '),\n",
       " Document(page_content='Chapter: Context\\n2022.\\nLanguage models (mostly) know what they know .\\nCoRR , abs/2207.05221.\\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. '),\n",
       " Document(page_content='Chapter: Context\\n2022.\\nLarge language models struggle to learn long-tail knowledge .\\nCoRR , abs/2211.08411.\\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. '),\n",
       " Document(page_content='Chapter: Context\\n2020.\\nGeneralization through memorization: Nearest neighbor language models .'),\n",
       " Document(page_content='Chapter: Context\\nIn 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . '),\n",
       " Document(page_content='Chapter: Context\\nOpenReview.net.\\nOmar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. '),\n",
       " Document(page_content='Chapter: Context\\n2022.\\nDemonstrate-search-predict: Composing retrieval and language models for knowledge-intensive NLP .\\nCoRR , abs/2212.14024.\\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\\n2019. '),\n",
       " Document(page_content='Chapter: Context\\nLatent retrieval for weakly supervised open do- main question answering.'),\n",
       " Document(page_content='Chapter: Context\\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 6086–6096.\\nPatrick S. '),\n",
       " Document(page_content='Chapter: Context\\nH.'),\n",
       " Document(page_content='Chapter: Context\\nLewis, Ethan Perez, Aleksandra Pik- tus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. '),\n",
       " Document(page_content='Chapter: Context\\n2020.\\nRetrieval-augmented generation for knowledge-intensive NLP tasks .'),\n",
       " Document(page_content='Chapter: Context\\nIn Advances in Neu- ral Information Processing Systems 33: Annual Con- ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual .\\nJunyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. '),\n",
       " Document(page_content='Chapter: Context\\n2023.\\nHalueval: A large- scale hallucination evaluation benchmark for large language models .\\nCoRR , abs/2305.11747.\\nNelson F. '),\n",
       " Document(page_content='Chapter: Context\\nLiu, Kevin Lin, John Hewitt, Ashwin Paran- jape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. '),\n",
       " Document(page_content='Chapter: Context\\n2023.\\nLost in the middle: How language models use long contexts .\\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. '),\n",
       " Document(page_content='Chapter: Context\\n2023.\\nWhen not to trust language models: Investigating effectiveness of parametric and non-parametric mem- ories . '),\n",
       " Document(page_content='Chapter: Context\\nIn Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers) , pages 9802–9822, Toronto, Canada. '),\n",
       " Document(page_content='Chapter: Context\\nAssociation for Computational Linguistics.\\nPotsawee Manakul, Adian Liusie, and Mark J. '),\n",
       " Document(page_content='Chapter: Context\\nF.'),\n",
       " Document(page_content='Chapter: Context\\nGales.\\n2023.\\nSelfcheckgpt: Zero-resource black-box hal- lucination detection for generative large language models .\\nCoRR , abs/2303.08896.\\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. '),\n",
       " Document(page_content='Chapter: Context\\n2023.\\nFactscore: Fine-grained atomic evaluation of fac- tual precision in long form text generation .\\nCoRR , abs/2305.14251.Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. '),\n",
       " Document(page_content='Chapter: Context\\n2023.\\nIn-context retrieval-augmented lan- guage models .\\nCoRR , abs/2302.00083.\\nAdam Roberts, Colin Raffel, and Noam Shazeer. '),\n",
       " Document(page_content='Chapter: Context\\n2020.\\nHow much knowledge can you pack into the param- eters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 5418–5426, Online. '),\n",
       " Document(page_content='Chapter: Context\\nAssociation for Computational Linguistics.\\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. '),\n",
       " Document(page_content='Chapter: Context\\n2023.\\nREPLUG: retrieval-augmented black-box language models .\\nCoRR , abs/2301.12652.\\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxi- ang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. '),\n",
       " Document(page_content='Chapter: Context\\n2023a.\\nIs chatgpt a good NLG evaluator? A preliminary study .\\nCoRR , abs/2303.04048.\\nPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui.\\n2023b.\\nLarge language models are not fair evaluators .\\nCoRR , abs/2305.17926.\\nShufan Wang, Yixiao Song, Andrew Drozdov, Aparna Garimella, Varun Manjunatha, and Mohit Iyyer.\\n2023c.\\nKNN-LM does not improve open-ended text generation .\\nCoRR , abs/2305.14625.\\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. '),\n",
       " Document(page_content='Chapter: Context\\n2021.\\nBartscore: Evaluating generated text as text genera- tion .'),\n",
       " Document(page_content='Chapter: Context\\nIn Advances in Neural Information Processing Systems 34: Annual Conference on Neural Informa- tion Processing Systems 2021, NeurIPS 2021, De- cember 6-14, 2021, virtual , pages 27263–27277.\\nTianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei Fang, Luc Gaitskell, Thomas Hartvigsen, Xixin Wu, Danny Fox, Helen Meng, and James R. '),\n",
       " Document(page_content='Chapter: Context\\nGlass.'),\n",
       " Document(page_content='Chapter: Context\\n2023.\\nInterpretable unified language checking .\\nCoRR , abs/2304.03728.\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\\nWeinberger, and Yoav Artzi. '),\n",
       " Document(page_content='Chapter: Context\\n2020.\\nBertscore: Evalu- ating text generation with BERT .'),\n",
       " Document(page_content='Chapter: Context\\nIn 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . '),\n",
       " Document(page_content='Chapter: Context\\nOpenRe- view.net.\\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris- tian M.'),\n",
       " Document(page_content='Chapter: Context\\nMeyer, and Steffen Eger.'),\n",
       " Document(page_content='Chapter: Context\\n2019.\\nMoverScore: Text generation evaluating with contextualized em- beddings and earth mover distance . '),\n",
       " Document(page_content='Chapter: Context\\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna- tional Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP) , pages 563–578, Hong Kong, China. '),\n",
       " Document(page_content='Chapter: Context\\nAssociation for Computational Lin- guistics.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Convert list to string with a space separator\n",
    "structured_text = ' '.join(structured_text)\n",
    "\n",
    "# splitter = CharacterTextSplitter(separator=\"Chapter:\", chunk_size=30, chunk_overlap=10)\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20, separators=['Chapter:','\\n\\n'])\n",
    "chunks = splitter.split_text(structured_text)\n",
    "docs = splitter.create_documents(chunks)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai.client import MistralClient\n",
    "\n",
    "api_key = \"ImsUHfFLA6OjlX6mARbnM1YcDOy7ujsq\"\n",
    "client = MistralClient(api_key=\"ImsUHfFLA6OjlX6mARbnM1YcDOy7ujsq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manualy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embedding(txt):\n",
    "    client = MistralClient(api_key=api_key)\n",
    "    embeddings_batch_response = client.embeddings(model=\"mistral-embed\", input=txt)\n",
    "    return embeddings_batch_response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_text_embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m text_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([get_text_embedding(chunk) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks])\n\u001b[1;32m      4\u001b[0m text_embeddings\n",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m text_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([get_text_embedding(chunk) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks])\n\u001b[1;32m      4\u001b[0m text_embeddings\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_text_embedding' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])\n",
    "text_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain + Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "output_path = \"hackaton-mistral-studai/data/chromadb\"\n",
    "mistral_embeddings = MistralAIEmbeddings(api_key=api_key)\n",
    "mistral_embeddings.model = \"mistral-embed\"  \n",
    "chroma_db = Chroma.from_documents(docs, mistral_embeddings, persist_directory=output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Vector Database \"\"\"\n",
    "import faiss\n",
    "\n",
    "d = text_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(text_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manualy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00650787, -0.04208374,  0.06274414, ..., -0.02349854,\n",
       "         0.03463745,  0.02111816]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is RAGAs?\"\n",
    "question_embeddings = np.array([get_text_embedding(question)])\n",
    "question_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18 20]]\n"
     ]
    }
   ],
   "source": [
    "D, I = index.search(question_embeddings, k=2)\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nTo address these issues, in this paper we present RAGA S 1 , a framework for the automated assess- 1 RAGA S is available at https://github.com/ explodinggradients/ragas .', '\\nWe focus on settings where reference answers may not be available, and where we want to estimate different proxies for correctness, in addition to the usefulness of the retrieved passages. The RAGA S framework provides an integration with both llama- index and Langchain , the most widely used frame- works for building RAG solutions, thus enabling developers to easily integrate RAGA S into their standard workflow.']\n"
     ]
    }
   ],
   "source": [
    "retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
    "print(retrieved_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{retrieved_chunk}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "client = MistralClient(api_key=\"ImsUHfFLA6OjlX6mARbnM1YcDOy7ujsq\")\n",
    "\n",
    "def mistral(user_message, model=\"mistral-small-latest\", is_json=False):\n",
    "    messages = [ChatMessage(role=\"user\", content=user_message)]\n",
    "\n",
    "    if is_json:\n",
    "        chat_response = client.chat(\n",
    "            model=model, messages=messages, response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "    else:\n",
    "        chat_response = client.chat(model=model, messages=messages)\n",
    "\n",
    "    return chat_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGA S is a framework for automated assessment, specifically designed for settings where reference answers may not be available. It aims to estimate different proxies for correctness and the usefulness of retrieved passages. The RAGA S framework is available for integration with llama-index and Langchain, which are widely used frameworks for building RAG (Retrieval-Augmented Generation) solutions. You can find RAGA S at this GitHub link: https://github.com/explodinggradients/ragas.\n"
     ]
    }
   ],
   "source": [
    "response = mistral(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain + Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors stored: 167\n"
     ]
    }
   ],
   "source": [
    "# Load the vector database to confirm it contains the data\n",
    "path = \"hackaton-mistral-studai/data/chromadb\"\n",
    "db = Chroma(persist_directory=path)\n",
    "\n",
    "# Optionally, verify some details about the stored vectors\n",
    "print(\"Number of vectors stored:\", len(db))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maximejabarian/miniconda3/envs/studai_env/lib/python3.11/site-packages/langchain_mistralai/embeddings.py:105: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Chapter: Introduction\\nChatGPT and GPT-4).'),\n",
       " Document(page_content='Chapter: Introduction\\nby mea- suring perplexity on some reference corpus.'),\n",
       " Document(page_content='Chapter: Introduction\\nAutomated evaluation of retrieval-augmented systems is thus paramount.'),\n",
       " Document(page_content='Chapter: Introduction\\nIn practice, RAG systems are often evaluated in terms of the language modelling task itself, i.e. '),\n",
       " Document(page_content='Chapter: Introduction\\nWhile initial approaches relied on specialised LMs for retrieval-augmented language modelling ( Khandel- wal et al.\\n, 2020 ; Borgeaud et al.\\n, 2022 ), recent work has suggested that simply adding retrieved docu- ments to the input of a standard LM can also work well ( Khattab et al.\\n, 2022 ; Ram et al.\\n, 2023 ; Shi et al.\\n, 2023 ), thus making it possible to use retrieval- augmented strategies in combination with LLMs that are only available through APIs.\\nWhile the usefulness of retrieval-augmented strategies is clear, their implementation requires a significant amount of tuning, as the overall per- formance will be affected by the retrieval model, the considered corpus, the LM, or the prompt for- mulation, among others. '),\n",
       " Document(page_content='Chapter: Introduction\\nThis idea of LMs as repositories of knowledge emerged shortly after the introduction of BERT ( Devlin et al.\\n, 2019 ) and became more firmly established with the introduction of ever larger LMs ( Roberts et al.\\n, 2020 ). '),\n",
       " Document(page_content='Chapter: Context\\nH.'),\n",
       " Document(page_content='Chapter: Introduction\\nSecond, even the largest models struggle to memorise knowledge that is only rarely mentioned in the training corpus ( Kandpal et al.\\n, 2022 ; Mallen et al.\\n, 2023 ). '),\n",
       " Document(page_content='Chapter: Introduction\\nThe standard solution to these issues is to rely on Retrieval Augmented Generation (RAG) ( Lee et al.\\n, 2019 ; Lewis et al.\\n, 2020 ; Guu et al.\\n, 2020 ). '),\n",
       " Document(page_content='Chapter: Introduction\\nWhile the most recent Large Language Models (LLMs) capture enough knowledge to rival human performance across a wide variety of question answering bench- marks ( Bubeck et al.\\n, 2023 ), the idea of using LLMs as knowledge bases still has two fundamen- tal limitations. '),\n",
       " Document(page_content='Chapter: Introduction\\nHow- ever, such evaluations are not always predictive of downstream performance ( Wang et al.\\n, 2023c ).\\nMoreover, this evaluation strategy relies on the LM probabilities, which are not accessible for some closed models (e.g. '),\n",
       " Document(page_content='Chapter: Context\\nF.'),\n",
       " Document(page_content='Chapter: Introduction\\nLanguage Models (LMs) capture a vast amount of knowledge about the world, which allows them to answer questions without accessing any exter- nal sources. '),\n",
       " Document(page_content='Chapter: Introduction\\nQues- tion answering is another common evaluation task, but usually only datasets with short extractive an- swers are considered, which may not be represen- tative of how the system will be used.\\nTo address these issues, in this paper we present RAGA S 1 , a framework for the automated assess- 1 RAGA S is available at https://github.com/ explodinggradients/ragas .\\n# arXiv:2309.15217v1  [cs.CL]  26 Sep 2023ment of retrieval augmented generation systems.\\nWe focus on settings where reference answers may not be available, and where we want to estimate different proxies for correctness, in addition to the usefulness of the retrieved passages. '),\n",
       " Document(page_content='Chapter: Abstract\\nWe posit that such a framework can crucially con- tribute to faster evaluation cycles of RAG archi- tectures, which is especially important given the fast adoption of LLMs.\\n1 ')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from mistralai.client import MistralClient\n",
    "\n",
    "path = \"hackaton-mistral-studai/data/chromadb\"\n",
    "\n",
    "api_key = \"ImsUHfFLA6OjlX6mARbnM1YcDOy7ujsq\"\n",
    "client = MistralClient(api_key=\"ImsUHfFLA6OjlX6mARbnM1YcDOy7ujsq\")\n",
    "\n",
    "# Mistral Embedding model\n",
    "mistral_embeddings = MistralAIEmbeddings(api_key=api_key)\n",
    "mistral_embeddings.model = \"mistral-embed\"  \n",
    "\n",
    "# load vector_db\n",
    "db = Chroma(persist_directory=path, embedding_function=mistral_embeddings)\n",
    "\n",
    "# define retriever\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 15})\n",
    "retriever.invoke(\"Chapter: Introduction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGAS is a special tool that helps us check how good a computer program is at answering questions using information it finds. It's like asking your friend to find the answer to a question in a book, but instead of a friend, it's a computer program. The words to fill in are: computer program, answers, information, book.\n",
      "\n",
      "So, RAGAS helps us check if a computer program can find the right answers using information it finds, just like your friend finding answers in a book.\n"
     ]
    }
   ],
   "source": [
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "# Define LLM\n",
    "model = ChatMistralAI(mistral_api_key=api_key, model_name=\"open-mixtral-8x22b\")\n",
    "# Define prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "# Create a retrieval chain to answer questions\n",
    "document_chain = create_stuff_documents_chain(model, prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "response = retrieval_chain.invoke({\"input\": \"To check if I really understood what's RAGAS, give me short explanation whith missing words to complete with a list of words to fill with, as if I was a child of 10 years old.\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Chatbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "from mistralai.client import MistralClient\n",
    "import panel as pn\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mistral(contents, user, chat_interface):\n",
    "    messages = [ChatMessage(role=\"user\", content=contents)]\n",
    "    chat_response = client.chat(\n",
    "        model=\"mistral-large-latest\", \n",
    "        messages=messages)\n",
    "    return chat_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='5712a6f3-4d6b-4925-a2b4-cbc677b0304e'>\n",
       "  <div id=\"c3f776ed-d1bd-4869-b42d-730c8b16e30d\" data-root-id=\"5712a6f3-4d6b-4925-a2b4-cbc677b0304e\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"b8952d1d-4697-4a89-aa0f-c70ebf2c49f8\":{\"version\":\"3.4.1\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.layout.Card\",\"id\":\"5712a6f3-4d6b-4925-a2b4-cbc677b0304e\",\"attributes\":{\"name\":\"Card00144\",\"css_classes\":[\"chat-interface\"],\"styles\":{\"type\":\"map\",\"entries\":[[\"padding\",\"0px\"]]},\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"dd9b8770-149d-4ff3-b55e-dd87ee6b415a\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.4.3/dist/css/loading.css\"}},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"e9bea015-78d1-4fe0-84e1-64c40a0fe0b6\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.4.3/dist/css/listpanel.css\"}},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"96755a2b-0732-48a9-8fb9-3715b5ed2374\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.4.3/dist/bundled/theme/default.css\"}},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"28ba4ebe-bcdc-4446-9b0f-9504f70d0c3f\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.4.3/dist/bundled/theme/native.css\"}},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"08696dcc-657e-496f-a2d8-76f21b6fdfc4\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.4.3/dist/css/chat_interface.css\"}}],\"margin\":5,\"sizing_mode\":\"stretch_both\",\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"Row\",\"id\":\"f803a0fd-870a-4ee1-9c12-154d100a23c9\",\"attributes\":{\"name\":\"Row00143\",\"css_classes\":[\"card-header-row\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"dd9b8770-149d-4ff3-b55e-dd87ee6b415a\"},{\"id\":\"e9bea015-78d1-4fe0-84e1-64c40a0fe0b6\"},{\"id\":\"96755a2b-0732-48a9-8fb9-3715b5ed2374\"},{\"id\":\"28ba4ebe-bcdc-4446-9b0f-9504f70d0c3f\"}],\"margin\":0,\"sizing_mode\":\"stretch_width\",\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"panel.models.markup.HTML\",\"id\":\"68f9ca20-c3c1-43dd-984d-f765c6371cc9\",\"attributes\":{\"css_classes\":[\"chat-feed-title\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"dd9b8770-149d-4ff3-b55e-dd87ee6b415a\"},{\"id\":\"96755a2b-0732-48a9-8fb9-3715b5ed2374\"},{\"id\":\"28ba4ebe-bcdc-4446-9b0f-9504f70d0c3f\"}],\"margin\":[5,0],\"align\":\"start\",\"text\":\"&amp;#8203;\",\"disable_math\":true}}]}},{\"type\":\"object\",\"name\":\"panel.models.feed.Feed\",\"id\":\"4fac9a37-2d52-469d-8dae-8d6632c21c9c\",\"attributes\":{\"name\":\"Feed00141\",\"subscribed_events\":{\"type\":\"set\",\"entries\":[\"scroll_button_click\"]},\"css_classes\":[\"chat-feed-log\",\"scroll-vertical\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"dd9b8770-149d-4ff3-b55e-dd87ee6b415a\"},{\"id\":\"e9bea015-78d1-4fe0-84e1-64c40a0fe0b6\"},{\"id\":\"96755a2b-0732-48a9-8fb9-3715b5ed2374\"},{\"id\":\"28ba4ebe-bcdc-4446-9b0f-9504f70d0c3f\"},{\"id\":\"08696dcc-657e-496f-a2d8-76f21b6fdfc4\"}],\"margin\":0,\"sizing_mode\":\"stretch_width\",\"align\":\"start\",\"auto_scroll_limit\":200,\"scroll_button_threshold\":100,\"view_latest\":true}},{\"type\":\"object\",\"name\":\"Spacer\",\"id\":\"d1578b9d-c145-4558-8391-bf0b3c4a358c\",\"attributes\":{\"name\":\"VSpacer00142\",\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"dd9b8770-149d-4ff3-b55e-dd87ee6b415a\"},{\"id\":\"96755a2b-0732-48a9-8fb9-3715b5ed2374\"},{\"id\":\"28ba4ebe-bcdc-4446-9b0f-9504f70d0c3f\"}],\"margin\":0,\"sizing_mode\":\"stretch_height\",\"align\":\"start\"}},{\"type\":\"object\",\"name\":\"Row\",\"id\":\"d5dc746a-15aa-4b9f-b843-40a9c3b4a371\",\"attributes\":{\"name\":\"Row00147\",\"css_classes\":[\"chat-interface-input-container\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"dd9b8770-149d-4ff3-b55e-dd87ee6b415a\"},{\"id\":\"e9bea015-78d1-4fe0-84e1-64c40a0fe0b6\"},{\"id\":\"96755a2b-0732-48a9-8fb9-3715b5ed2374\"},{\"id\":\"28ba4ebe-bcdc-4446-9b0f-9504f70d0c3f\"},{\"id\":\"08696dcc-657e-496f-a2d8-76f21b6fdfc4\"}],\"margin\":0,\"sizing_mode\":\"stretch_width\",\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"Row\",\"id\":\"17a5d1a5-12f7-487a-a848-343d28a2c727\",\"attributes\":{\"name\":\"Row00164\",\"css_classes\":[\"chat-interface-input-row\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"dd9b8770-149d-4ff3-b55e-dd87ee6b415a\"},{\"id\":\"e9bea015-78d1-4fe0-84e1-64c40a0fe0b6\"},{\"id\":\"96755a2b-0732-48a9-8fb9-3715b5ed2374\"},{\"id\":\"28ba4ebe-bcdc-4446-9b0f-9504f70d0c3f\"},{\"id\":\"08696dcc-657e-496f-a2d8-76f21b6fdfc4\"}],\"margin\":0,\"sizing_mode\":\"stretch_width\",\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"panel.models.chatarea_input.ChatAreaInput\",\"id\":\"8632d8c2-816e-41eb-af3e-65c0ae252eee\",\"attributes\":{\"subscribed_events\":{\"type\":\"set\",\"entries\":[\"chat_message_event\"]},\"css_classes\":[\"chat-interface-input-widget\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"dd9b8770-149d-4ff3-b55e-dd87ee6b415a\"},{\"id\":\"96755a2b-0732-48a9-8fb9-3715b5ed2374\"},{\"id\":\"28ba4ebe-bcdc-4446-9b0f-9504f70d0c3f\"}],\"width\":300,\"margin\":[5,10],\"sizing_mode\":\"stretch_width\",\"align\":\"start\",\"resizable\":\"height\",\"placeholder\":\"Send a message\",\"max_length\":5000,\"rows\":1,\"auto_grow\":true,\"max_rows\":10}},{\"type\":\"object\",\"name\":\"panel.models.widgets.Button\",\"id\":\"5c0b644a-1f2e-4a74-9d88-e2a1fcdef473\",\"attributes\":{\"subscribed_events\":{\"type\":\"set\",\"entries\":[\"button_click\"]},\"css_classes\":[\"solid\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"dd9b8770-149d-4ff3-b55e-dd87ee6b415a\"},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"2410c7c6-1024-4fdd-ae60-2235f91e04a8\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.4.3/dist/css/button.css\"}},{\"id\":\"96755a2b-0732-48a9-8fb9-3715b5ed2374\"},{\"id\":\"28ba4ebe-bcdc-4446-9b0f-9504f70d0c3f\"}],\"max_width\":90,\"max_height\":50,\"margin\":[0,5,0,0],\"sizing_mode\":\"stretch_width\",\"align\":\"center\",\"label\":\"Send\",\"icon\":{\"type\":\"object\",\"name\":\"TablerIcon\",\"id\":\"60591615-a10e-45c0-bb15-278f6290faad\",\"attributes\":{\"icon_name\":\"send\"}}}},{\"type\":\"object\",\"name\":\"panel.models.widgets.Button\",\"id\":\"d58462f6-a493-44bd-be4c-9b0864a190be\",\"attributes\":{\"subscribed_events\":{\"type\":\"set\",\"entries\":[\"button_click\"]},\"visible\":false,\"css_classes\":[\"solid\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"dd9b8770-149d-4ff3-b55e-dd87ee6b415a\"},{\"id\":\"2410c7c6-1024-4fdd-ae60-2235f91e04a8\"},{\"id\":\"96755a2b-0732-48a9-8fb9-3715b5ed2374\"},{\"id\":\"28ba4ebe-bcdc-4446-9b0f-9504f70d0c3f\"}],\"max_width\":90,\"max_height\":50,\"margin\":[0,5,0,0],\"sizing_mode\":\"stretch_width\",\"align\":\"center\",\"label\":\"Stop\",\"icon\":{\"type\":\"object\",\"name\":\"TablerIcon\",\"id\":\"5cc3abc9-bb52-4da1-85e9-5ad5bba844ad\",\"attributes\":{\"icon_name\":\"player-stop\"}}}},{\"type\":\"object\",\"name\":\"panel.models.widgets.Button\",\"id\":\"87f97382-32de-43f7-9592-659b2a9957c8\",\"attributes\":{\"subscribed_events\":{\"type\":\"set\",\"entries\":[\"button_click\"]},\"css_classes\":[\"solid\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"dd9b8770-149d-4ff3-b55e-dd87ee6b415a\"},{\"id\":\"2410c7c6-1024-4fdd-ae60-2235f91e04a8\"},{\"id\":\"96755a2b-0732-48a9-8fb9-3715b5ed2374\"},{\"id\":\"28ba4ebe-bcdc-4446-9b0f-9504f70d0c3f\"}],\"max_width\":90,\"max_height\":50,\"margin\":[0,5,0,0],\"sizing_mode\":\"stretch_width\",\"align\":\"center\",\"label\":\"Rerun\",\"icon\":{\"type\":\"object\",\"name\":\"TablerIcon\",\"id\":\"67960f6c-15ca-4ed0-92ec-e992c7c8172a\",\"attributes\":{\"icon_name\":\"repeat\"}}}},{\"type\":\"object\",\"name\":\"panel.models.widgets.Button\",\"id\":\"80afe1d6-0758-483a-a6cd-6efafeb2b34a\",\"attributes\":{\"subscribed_events\":{\"type\":\"set\",\"entries\":[\"button_click\"]},\"css_classes\":[\"solid\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"dd9b8770-149d-4ff3-b55e-dd87ee6b415a\"},{\"id\":\"2410c7c6-1024-4fdd-ae60-2235f91e04a8\"},{\"id\":\"96755a2b-0732-48a9-8fb9-3715b5ed2374\"},{\"id\":\"28ba4ebe-bcdc-4446-9b0f-9504f70d0c3f\"}],\"max_width\":90,\"max_height\":50,\"margin\":[0,5,0,0],\"sizing_mode\":\"stretch_width\",\"align\":\"center\",\"label\":\"Undo\",\"icon\":{\"type\":\"object\",\"name\":\"TablerIcon\",\"id\":\"8a039656-f3bf-42fb-bd03-7bd1bb5e6ee5\",\"attributes\":{\"icon_name\":\"arrow-back\"}}}},{\"type\":\"object\",\"name\":\"panel.models.widgets.Button\",\"id\":\"e4125077-bb27-470b-8cdb-679474487c14\",\"attributes\":{\"subscribed_events\":{\"type\":\"set\",\"entries\":[\"button_click\"]},\"css_classes\":[\"solid\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"dd9b8770-149d-4ff3-b55e-dd87ee6b415a\"},{\"id\":\"2410c7c6-1024-4fdd-ae60-2235f91e04a8\"},{\"id\":\"96755a2b-0732-48a9-8fb9-3715b5ed2374\"},{\"id\":\"28ba4ebe-bcdc-4446-9b0f-9504f70d0c3f\"}],\"max_width\":90,\"max_height\":50,\"margin\":[0,5,0,0],\"sizing_mode\":\"stretch_width\",\"align\":\"center\",\"label\":\"Clear\",\"icon\":{\"type\":\"object\",\"name\":\"TablerIcon\",\"id\":\"44b19b75-23d0-419c-8d57-734a409fef5c\",\"attributes\":{\"icon_name\":\"trash\"}}}}]}}]}}],\"active_header_background\":\"\",\"button_css_classes\":[\"card-button\"],\"collapsed\":false,\"collapsible\":false,\"header_background\":\"\",\"header_color\":\"\",\"header_css_classes\":[\"chat-feed-header\"],\"hide_header\":true}},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"998c5cb4-fba9-4202-912b-ad33c910d387\",\"attributes\":{\"plot_id\":\"5712a6f3-4d6b-4925-a2b4-cbc677b0304e\",\"comm_id\":\"63ceeb6370064d7a87ac3240aef7eaf5\",\"client_comm_id\":\"ddbddff95857429e9d6c794a239bd420\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"copy_to_clipboard1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":null}]}]}};\n",
       "  var render_items = [{\"docid\":\"b8952d1d-4697-4a89-aa0f-c70ebf2c49f8\",\"roots\":{\"5712a6f3-4d6b-4925-a2b4-cbc677b0304e\":\"c3f776ed-d1bd-4869-b42d-730c8b16e30d\"},\"root_ids\":[\"5712a6f3-4d6b-4925-a2b4-cbc677b0304e\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  async function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    await Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && id_el.children[0].hasAttribute('data-root-id')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t  for (const child of root_el.children) {\n",
       "            // Ensure JupyterLab does not capture keyboard shortcuts\n",
       "            // see: https://jupyterlab.readthedocs.io/en/4.1.x/extension/notebook.html#keyboard-interaction-model\n",
       "\t    child.setAttribute('data-lm-suppress-shortcuts', 'true')\n",
       "\t  }\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined)\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ],
      "text/plain": [
       "ChatInterface(_button_data={'send': _ChatButtonData(i...}, _buttons={'send': Button(align='cen...}, _input_container=Row, _input_layout=Row, _placeholder=ChatMessage, _widgets={'ChatAreaInput': ChatArea...}, callback=<function run_mistral a..., callback_user='Mistral', show_button_name=True, sizing_mode='stretch_width', widgets=[ChatAreaInput(css_classes...])"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "5712a6f3-4d6b-4925-a2b4-cbc677b0304e"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_interface = pn.chat.ChatInterface(\n",
    "    callback=run_mistral, \n",
    "    callback_user=\"Mistral\"\n",
    ")\n",
    "\n",
    "chat_interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "client = MistralClient(api_key=\"ImsUHfFLA6OjlX6mARbnM1YcDOy7ujsq\")\n",
    "\n",
    "prompt = \"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{retrieved_chunk}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "def get_text_embedding(input):\n",
    "    embeddings_batch_response = client.embeddings(model=\"mistral-embed\", input=input)\n",
    "    return embeddings_batch_response.data[0].embedding\n",
    "\n",
    "def run_mistral(user_message, model=\"mistral-large-latest\"):\n",
    "    messages = [ChatMessage(role=\"user\", content=user_message)]\n",
    "    chat_response = client.chat(model=model, messages=messages)\n",
    "    return chat_response.choices[0].message.content\n",
    "\n",
    "def answer_question(question, index):\n",
    "    # create embeddings for a question\n",
    "    question_embeddings = np.array([get_text_embedding(question)])\n",
    "    # retrieve similar chunks from the vector database\n",
    "    D, I = index.search(question_embeddings, k=2)\n",
    "    retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
    "    # generate response based on the retrieved relevant text chunks\n",
    "    response = run_mistral(\n",
    "        prompt.format(retrieved_chunk=retrieved_chunk, question=question)\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='100e683e-602a-479d-b71c-d53532d773e0'>\n",
       "  <div id=\"d9b4d490-97ce-4e43-be2e-fa1b1502bb63\" data-root-id=\"100e683e-602a-479d-b71c-d53532d773e0\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"688af9f8-6b64-4465-8732-7cb4d8be192b\":{\"version\":\"3.4.1\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.layout.Card\",\"id\":\"100e683e-602a-479d-b71c-d53532d773e0\",\"attributes\":{\"name\":\"Card00207\",\"css_classes\":[\"chat-interface\"],\"styles\":{\"type\":\"map\",\"entries\":[[\"padding\",\"0px\"]]},\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.4.3/dist/css/loading.css\"}},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"b095b32e-1391-424b-9909-d97c62a62738\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.4.3/dist/css/listpanel.css\"}},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.4.3/dist/bundled/theme/default.css\"}},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.4.3/dist/bundled/theme/native.css\"}},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"3216c4d4-528b-4ad6-955e-562cb0a9e941\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.4.3/dist/css/chat_interface.css\"}}],\"min_width\":45,\"margin\":5,\"sizing_mode\":\"stretch_both\",\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"Row\",\"id\":\"b7ea297d-b845-43e9-9bc6-0b29d6d8c01b\",\"attributes\":{\"name\":\"Row00206\",\"css_classes\":[\"card-header-row\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"b095b32e-1391-424b-9909-d97c62a62738\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"}],\"margin\":0,\"sizing_mode\":\"stretch_width\",\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"panel.models.markup.HTML\",\"id\":\"90f106b0-fcf2-4211-9975-bfa602166cf5\",\"attributes\":{\"css_classes\":[\"chat-feed-title\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"}],\"margin\":[5,0],\"align\":\"start\",\"text\":\"&amp;#8203;\",\"disable_math\":true}}]}},{\"type\":\"object\",\"name\":\"panel.models.feed.Feed\",\"id\":\"6d8345d7-a3fe-4734-bf51-6212e51a65e1\",\"attributes\":{\"name\":\"Feed00204\",\"subscribed_events\":{\"type\":\"set\",\"entries\":[\"scroll_button_click\"]},\"css_classes\":[\"chat-feed-log\",\"scroll-vertical\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"b095b32e-1391-424b-9909-d97c62a62738\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"},{\"id\":\"3216c4d4-528b-4ad6-955e-562cb0a9e941\"}],\"min_width\":35,\"margin\":0,\"sizing_mode\":\"stretch_width\",\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"Row\",\"id\":\"d4dcd747-56bb-4357-b139-1636e93da1f1\",\"attributes\":{\"name\":\"Row00251\",\"css_classes\":[\"chat-message\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"b095b32e-1391-424b-9909-d97c62a62738\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"5dfa2ea9-0af6-4a63-835d-62ee72ec6c60\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.4.3/dist/css/chat_message.css\"}}],\"min_width\":35,\"max_width\":1200,\"margin\":[5,10],\"sizing_mode\":\"stretch_width\",\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"panel.models.layout.Column\",\"id\":\"1b3d4ebc-ffbe-41a3-b796-2f148d6b2450\",\"attributes\":{\"name\":\"Column00240\",\"css_classes\":[\"left\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"b095b32e-1391-424b-9909-d97c62a62738\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"},{\"id\":\"5dfa2ea9-0af6-4a63-835d-62ee72ec6c60\"}],\"height\":100,\"min_height\":100,\"max_width\":60,\"margin\":0,\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"panel.models.markup.HTML\",\"id\":\"04dd8df0-e57e-43e3-929f-aa8c413d5e37\",\"attributes\":{\"css_classes\":[\"avatar\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"}],\"margin\":[5,10],\"align\":\"start\",\"text\":\"\\u2699\\ufe0f\",\"disable_math\":true}}]}},{\"type\":\"object\",\"name\":\"panel.models.layout.Column\",\"id\":\"5d3e9929-c8f0-42ab-8594-52902167215b\",\"attributes\":{\"name\":\"Column00250\",\"css_classes\":[\"right\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"b095b32e-1391-424b-9909-d97c62a62738\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"},{\"id\":\"5dfa2ea9-0af6-4a63-835d-62ee72ec6c60\"}],\"min_width\":15,\"margin\":0,\"sizing_mode\":\"stretch_width\",\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"Row\",\"id\":\"d92cc6ac-aebd-48cc-878a-b6fb462f29af\",\"attributes\":{\"name\":\"Row00246\",\"css_classes\":[\"header\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"b095b32e-1391-424b-9909-d97c62a62738\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"},{\"id\":\"5dfa2ea9-0af6-4a63-835d-62ee72ec6c60\"}],\"min_width\":15,\"margin\":0,\"sizing_mode\":\"stretch_width\",\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"panel.models.markup.HTML\",\"id\":\"26cdba21-7df8-4983-a374-172ed81bdc8e\",\"attributes\":{\"css_classes\":[\"name\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"},{\"id\":\"5dfa2ea9-0af6-4a63-835d-62ee72ec6c60\"}],\"height\":20,\"min_height\":20,\"margin\":[5,10],\"align\":\"start\",\"text\":\"System\",\"disable_math\":true}},{\"type\":\"object\",\"name\":\"panel.models.reactive_html.ReactiveHTML\",\"id\":\"9794155e-9cf5-4f2d-bf8f-692199ae78ad\",\"attributes\":{\"subscribed_events\":{\"type\":\"set\",\"entries\":[\"dom_event\"]},\"css_classes\":[\"copy-icon\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"35ecd894-4ac9-4635-8501-3974820a4549\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.4.3/dist/css/chat_copy_icon.css\"}},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"},{\"id\":\"5dfa2ea9-0af6-4a63-835d-62ee72ec6c60\"}],\"width\":15,\"height\":15,\"margin\":0,\"sizing_mode\":\"fixed\",\"align\":\"start\",\"attrs\":{\"type\":\"map\",\"entries\":[[\"copy-button\",[[\"onclick\",[],\"{script('copy_to_clipboard')}\"]]],[\"copy-icon\",[[\"fill\",[\"fill\"],\"{fill}\"]]]]},\"callbacks\":{\"type\":\"map\",\"entries\":[[\"copy-button\",[[\"onclick\",\"script('copy_to_clipboard')\"]]]]},\"data\":{\"type\":\"object\",\"name\":\"copy_to_clipboard1\",\"id\":\"e1649bc1-3c68-49f9-940b-f1bb754d9e71\",\"attributes\":{\"name\":\"ChatCopyIcon00234\",\"value\":\"Send a message to get a reply from Mistral!\"}},\"html\":\"\\n&lt;div\\n    type=&quot;button&quot;\\n    id=&quot;copy-button-${id}&quot;\\n    onclick=&quot;${script(&#x27;copy_to_clipboard&#x27;)}&quot;\\n    style=&quot;cursor: pointer; width: ${model.width}px; height: ${model.height}px;&quot;\\n    title=&quot;Copy message to clipboard&quot;\\n&gt;\\n    &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; class=&quot;icon icon-tabler icon-tabler-copy&quot; id=&quot;copy-icon-${id}&quot;\\n        width=&quot;${model.width}&quot; height=&quot;${model.height}&quot; viewBox=&quot;0 0 24 24&quot;\\n        stroke-width=&quot;2&quot; stroke=&quot;currentColor&quot; fill=${fill} stroke-linecap=&quot;round&quot; stroke-linejoin=&quot;round&quot;\\n    &gt;\\n        &lt;path stroke=&quot;none&quot; d=&quot;M0 0h24v24H0z&quot; fill=&quot;none&quot;&gt;&lt;/path&gt;\\n        &lt;path d=&quot;M8 8m0 2a2 2 0 0 1 2 -2h8a2 2 0 0 1 2 2v8a2 2 0 0 1 -2 2h-8a2 2 0 0 1 -2 -2z&quot;&gt;&lt;/path&gt;\\n        &lt;path d=&quot;M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2&quot;&gt;&lt;/path&gt;\\n    &lt;/svg&gt;\\n&lt;/div&gt;\\n\",\"nodes\":[\"copy-button\",\"copy-icon\"],\"scripts\":{\"type\":\"map\",\"entries\":[[\"copy_to_clipboard\",[\"navigator.clipboard.writeText(`${data.value}`);\\ndata.fill = &quot;currentColor&quot;;\\nsetTimeout(() =&gt; data.fill = &quot;none&quot;, 50);\"]]]}}},{\"type\":\"object\",\"name\":\"panel.models.markup.HTML\",\"id\":\"56cbe2e5-0974-48ae-afe3-93c29381a6e7\",\"attributes\":{\"visible\":false,\"css_classes\":[\"activity-dot\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"},{\"id\":\"5dfa2ea9-0af6-4a63-835d-62ee72ec6c60\"}],\"margin\":[5,10],\"align\":\"start\",\"text\":\"\\u25cf\",\"disable_math\":true}}]}},{\"type\":\"object\",\"name\":\"Row\",\"id\":\"d767a8bf-9989-4768-b046-4b25669017f1\",\"attributes\":{\"name\":\"Row00243\",\"css_classes\":[\"center\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"b095b32e-1391-424b-9909-d97c62a62738\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"},{\"id\":\"5dfa2ea9-0af6-4a63-835d-62ee72ec6c60\"}],\"margin\":0,\"sizing_mode\":\"stretch_width\",\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"panel.models.markup.HTML\",\"id\":\"13d3727b-5dc5-4f96-8fe5-0a606fcbf8c1\",\"attributes\":{\"css_classes\":[\"markdown\",\"message\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"83a93052-bbe2-4e89-a3bf-308f8c811905\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.4.3/dist/css/markdown.css\"}},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"},{\"id\":\"5dfa2ea9-0af6-4a63-835d-62ee72ec6c60\"}],\"margin\":[5,10],\"align\":\"start\",\"text\":\"&lt;p&gt;Send a message to get a reply from Mistral!&lt;/p&gt;\\n\"}},{\"type\":\"object\",\"name\":\"panel.models.layout.Column\",\"id\":\"19b2d6bc-18d8-46dd-8dd5-b8967436f48f\",\"attributes\":{\"name\":\"Column00230\",\"css_classes\":[\"reaction-icons\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"b095b32e-1391-424b-9909-d97c62a62738\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"},{\"id\":\"5dfa2ea9-0af6-4a63-835d-62ee72ec6c60\"}],\"width\":15,\"height\":15,\"margin\":[5,10],\"sizing_mode\":\"stretch_width\",\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"panel.models.icon.ToggleIcon\",\"id\":\"16d38dc0-6727-4584-a5a3-5fe0ac2260a5\",\"attributes\":{\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"7a1ecdef-b364-46c9-88b9-f0f2f0a672ae\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.4.3/dist/css/icon.css\"}},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"}],\"margin\":0,\"align\":\"start\"}}]}}]}},{\"type\":\"object\",\"name\":\"panel.models.layout.Column\",\"id\":\"4c3fa839-b19d-4e30-b1a6-f50ab5445653\",\"attributes\":{\"name\":\"Column00249\",\"css_classes\":[\"footer\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"b095b32e-1391-424b-9909-d97c62a62738\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"},{\"id\":\"5dfa2ea9-0af6-4a63-835d-62ee72ec6c60\"}],\"margin\":0,\"sizing_mode\":\"stretch_width\",\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"panel.models.markup.HTML\",\"id\":\"80afcc42-08b1-4caa-a5dd-1fc31f421120\",\"attributes\":{\"css_classes\":[\"timestamp\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"}],\"margin\":[5,10],\"align\":\"start\",\"text\":\"19:45\",\"disable_math\":true}}]}}]}}]}}],\"auto_scroll_limit\":200,\"scroll_button_threshold\":100,\"view_latest\":true}},{\"type\":\"object\",\"name\":\"Spacer\",\"id\":\"19f7463c-89e5-479f-ac8c-e42fa4ea010a\",\"attributes\":{\"name\":\"VSpacer00205\",\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"}],\"margin\":0,\"sizing_mode\":\"stretch_height\",\"align\":\"start\"}},{\"type\":\"object\",\"name\":\"Row\",\"id\":\"0fc7940d-a3e3-44b5-afcc-fb662b4f94a5\",\"attributes\":{\"name\":\"Row00210\",\"css_classes\":[\"chat-interface-input-container\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"b095b32e-1391-424b-9909-d97c62a62738\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"},{\"id\":\"3216c4d4-528b-4ad6-955e-562cb0a9e941\"}],\"margin\":0,\"sizing_mode\":\"stretch_width\",\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"Row\",\"id\":\"140bbfbc-8a0d-4487-b18e-ab11797c5995\",\"attributes\":{\"name\":\"Row00227\",\"css_classes\":[\"chat-interface-input-row\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"b095b32e-1391-424b-9909-d97c62a62738\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"},{\"id\":\"3216c4d4-528b-4ad6-955e-562cb0a9e941\"}],\"margin\":0,\"sizing_mode\":\"stretch_width\",\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"panel.models.chatarea_input.ChatAreaInput\",\"id\":\"51e3d0da-bbed-4a13-b306-a4b10627b469\",\"attributes\":{\"subscribed_events\":{\"type\":\"set\",\"entries\":[\"chat_message_event\"]},\"css_classes\":[\"chat-interface-input-widget\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"}],\"width\":300,\"margin\":[5,10],\"sizing_mode\":\"stretch_width\",\"align\":\"start\",\"resizable\":\"height\",\"placeholder\":\"Send a message\",\"max_length\":5000,\"rows\":1,\"auto_grow\":true,\"max_rows\":10}},{\"type\":\"object\",\"name\":\"panel.models.widgets.Button\",\"id\":\"cc4866c7-3a16-4d77-9a06-a73f15fc708b\",\"attributes\":{\"subscribed_events\":{\"type\":\"set\",\"entries\":[\"button_click\"]},\"css_classes\":[\"solid\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"8cbf0f8e-8022-4f08-bb44-8bf0735a33a5\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.4.3/dist/css/button.css\"}},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"}],\"max_width\":90,\"max_height\":50,\"margin\":[0,5,0,0],\"sizing_mode\":\"stretch_width\",\"align\":\"center\",\"label\":\"Send\",\"icon\":{\"type\":\"object\",\"name\":\"TablerIcon\",\"id\":\"43ab28bd-3bba-4251-9faa-c751bce2c9b9\",\"attributes\":{\"icon_name\":\"send\"}}}},{\"type\":\"object\",\"name\":\"panel.models.widgets.Button\",\"id\":\"c2301800-8594-434a-bc2a-0ddbbea7f6cc\",\"attributes\":{\"subscribed_events\":{\"type\":\"set\",\"entries\":[\"button_click\"]},\"visible\":false,\"css_classes\":[\"solid\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"8cbf0f8e-8022-4f08-bb44-8bf0735a33a5\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"}],\"max_width\":90,\"max_height\":50,\"margin\":[0,5,0,0],\"sizing_mode\":\"stretch_width\",\"align\":\"center\",\"label\":\"Stop\",\"icon\":{\"type\":\"object\",\"name\":\"TablerIcon\",\"id\":\"c7de2948-acb4-4eab-a4f0-c376841c762a\",\"attributes\":{\"icon_name\":\"player-stop\"}}}},{\"type\":\"object\",\"name\":\"panel.models.widgets.Button\",\"id\":\"6b858e68-e020-4bf7-ac89-17fbd50ea844\",\"attributes\":{\"subscribed_events\":{\"type\":\"set\",\"entries\":[\"button_click\"]},\"css_classes\":[\"solid\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"8cbf0f8e-8022-4f08-bb44-8bf0735a33a5\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"}],\"max_width\":90,\"max_height\":50,\"margin\":[0,5,0,0],\"sizing_mode\":\"stretch_width\",\"align\":\"center\",\"label\":\"Rerun\",\"icon\":{\"type\":\"object\",\"name\":\"TablerIcon\",\"id\":\"b69960ee-fdea-46e2-8e13-4ad2256f2171\",\"attributes\":{\"icon_name\":\"repeat\"}}}},{\"type\":\"object\",\"name\":\"panel.models.widgets.Button\",\"id\":\"f3e8ee4e-1dee-4bdf-b105-2d535748a859\",\"attributes\":{\"subscribed_events\":{\"type\":\"set\",\"entries\":[\"button_click\"]},\"css_classes\":[\"solid\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"8cbf0f8e-8022-4f08-bb44-8bf0735a33a5\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"}],\"max_width\":90,\"max_height\":50,\"margin\":[0,5,0,0],\"sizing_mode\":\"stretch_width\",\"align\":\"center\",\"label\":\"Undo\",\"icon\":{\"type\":\"object\",\"name\":\"TablerIcon\",\"id\":\"e4a1e739-ed37-4e79-8332-670e96e49ee3\",\"attributes\":{\"icon_name\":\"arrow-back\"}}}},{\"type\":\"object\",\"name\":\"panel.models.widgets.Button\",\"id\":\"1cc47c9d-1d1c-4229-bfde-cf2a48043768\",\"attributes\":{\"subscribed_events\":{\"type\":\"set\",\"entries\":[\"button_click\"]},\"css_classes\":[\"solid\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"8cbf0f8e-8022-4f08-bb44-8bf0735a33a5\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"}],\"max_width\":90,\"max_height\":50,\"margin\":[0,5,0,0],\"sizing_mode\":\"stretch_width\",\"align\":\"center\",\"label\":\"Clear\",\"icon\":{\"type\":\"object\",\"name\":\"TablerIcon\",\"id\":\"8e7f1514-871c-4c94-a72f-823ff8026b0a\",\"attributes\":{\"icon_name\":\"trash\"}}}}]}}]}}],\"active_header_background\":\"\",\"button_css_classes\":[\"card-button\"],\"collapsed\":false,\"collapsible\":false,\"header_background\":\"\",\"header_color\":\"\",\"header_css_classes\":[\"chat-feed-header\"],\"hide_header\":true}},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"a035a8e8-875a-4e7b-9835-e104a344c7f2\",\"attributes\":{\"plot_id\":\"100e683e-602a-479d-b71c-d53532d773e0\",\"comm_id\":\"8a35e5580c8243d485b4b70186ee7f9e\",\"client_comm_id\":\"4c0fa89db55d47eb8129501d8c9625dc\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"copy_to_clipboard1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":null}]}]}};\n",
       "  var render_items = [{\"docid\":\"688af9f8-6b64-4465-8732-7cb4d8be192b\",\"roots\":{\"100e683e-602a-479d-b71c-d53532d773e0\":\"d9b4d490-97ce-4e43-be2e-fa1b1502bb63\"},\"root_ids\":[\"100e683e-602a-479d-b71c-d53532d773e0\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  async function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    await Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && id_el.children[0].hasAttribute('data-root-id')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t  for (const child of root_el.children) {\n",
       "            // Ensure JupyterLab does not capture keyboard shortcuts\n",
       "            // see: https://jupyterlab.readthedocs.io/en/4.1.x/extension/notebook.html#keyboard-interaction-model\n",
       "\t    child.setAttribute('data-lm-suppress-shortcuts', 'true')\n",
       "\t  }\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined)\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ],
      "text/plain": [
       "ChatInterface(_button_data={'send': _ChatButtonData(i...}, _buttons={'send': Button(align='cen...}, _input_container=Row, _input_layout=Row, _placeholder=ChatMessage, _widgets={'ChatAreaInput': ChatArea...}, callback=<function answer_question ..., callback_user='Mistral', show_button_name=True, sizing_mode='stretch_width', widgets=[ChatAreaInput(css_classes...])\n",
       "    [0] ChatMessage(str, avatar='⚙️', reaction_icons=ChatReactionIcons, timestamp=datetime.datetime(2024, ..., user='System')"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "100e683e-602a-479d-b71c-d53532d773e0"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_interface = pn.chat.ChatInterface(\n",
    "    callback=answer_question,\n",
    "    callback_user=\"Mistral\",\n",
    ")\n",
    "chat_interface.send(\n",
    "    \"Send a message to get a reply from Mistral!\", \n",
    "    user=\"System\", \n",
    "    respond=False\n",
    ")\n",
    "chat_interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import param\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "# Replace with your actual API key\n",
    "client = MistralClient(api_key=\"ImsUHfFLA6OjlX6mARbnM1YcDOy7ujsq\")\n",
    "\n",
    "def get_text_embedding(txt):\n",
    "    client = MistralClient(api_key=api_key)\n",
    "    embeddings_batch_response = client.embeddings(model=\"mistral-embed\", input=txt)\n",
    "    return embeddings_batch_response.data[0].embedding\n",
    "\n",
    "class cbfs(param.Parameterized):\n",
    "    chat_history = param.List(default=[])\n",
    "    answer = param.String(default=\"\")\n",
    "    \n",
    "    def __init__(self, **params):\n",
    "        super(cbfs, self).__init__(**params)\n",
    "    \n",
    "    def convchain(self, query):\n",
    "        if not query:\n",
    "            return\n",
    "        question_embeddings = np.array([get_text_embedding(query)])\n",
    "        D, I = index.search(question_embeddings, k=2)\n",
    "        retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Context information is below.\n",
    "        ---------------------\n",
    "        {retrieved_chunk}\n",
    "        ---------------------\n",
    "        Given the context information and not prior knowledge, answer the query.\n",
    "        Query: {query}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "        messages = [ChatMessage(role=\"user\", content=prompt)]\n",
    "        chat_response = client.chat(model=\"mistral-small-latest\", messages=messages)\n",
    "        answer = chat_response.choices[0].message.content\n",
    "\n",
    "        self.chat_history.append((query, answer))\n",
    "        self.answer = answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatInterface:\n",
    "    def __init__(self, cbfs_instance):\n",
    "        self.cbfs_instance = cbfs_instance\n",
    "        self.user_input = widgets.Text(\n",
    "            placeholder='Type your question here...',\n",
    "            description='You:',\n",
    "            disabled=False\n",
    "        )\n",
    "        self.send_button = widgets.Button(description='Send')\n",
    "        self.chat_output = widgets.Output()\n",
    "\n",
    "        self.send_button.on_click(self.on_send)\n",
    "        self.user_input.on_submit(self.on_send_enter)  # Use on_submit for the enter key event\n",
    "        display(widgets.VBox([self.user_input, self.send_button, self.chat_output]))\n",
    "\n",
    "    def on_send(self, b):\n",
    "        self.process_user_input()\n",
    "\n",
    "    def on_send_enter(self, widget):  # Handler for the Enter key\n",
    "        self.process_user_input()\n",
    "\n",
    "    def process_user_input(self):\n",
    "        user_query = self.user_input.value.strip()\n",
    "        if user_query:\n",
    "            self.cbfs_instance.convchain(user_query)\n",
    "            with self.chat_output:\n",
    "                self.display_chat_history()\n",
    "            self.user_input.value = ''  # Clear input after sending\n",
    "\n",
    "    def display_chat_history(self):\n",
    "        self.chat_output.clear_output()\n",
    "        with self.chat_output:\n",
    "            for query, response in self.cbfs_instance.chat_history:\n",
    "                print(f'User: {query}')\n",
    "                print(f'AI: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30224/2273792851.py:13: DeprecationWarning: on_submit is deprecated. Instead, set the .continuous_update attribute to False and observe the value changing with: mywidget.observe(callback, 'value').\n",
      "  self.user_input.on_submit(self.on_send_enter)  # Use on_submit for the enter key event\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915643372afc4aeebb44ab49382e9012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='', description='You:', placeholder='Type your question here...'), Button(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize and display the chat interface with a cbfs instance\n",
    "cb_instance = cbfs()\n",
    "chat_interface = ChatInterface(cb_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Fast language models are a type of artificial intelligence (AI) that can process and understand human language quickly and efficiently. The importance of fast language models lies in their ability to enable rapid natural language processing (NLP) capabilities, which have numerous benefits in various applications. Here are some reasons why fast language models are important:\n",
      "\n",
      "1. **Real-time interactions**: Fast language models enable real-time interactions between humans and machines, making them essential for applications like chatbots, virtual assistants, and speech recognition systems.\n",
      "2. **Improved user experience**: Fast language models can quickly respond to user queries, providing a more seamless and responsive user experience in applications like customer service chatbots, voice assistants, and language translation apps.\n",
      "3. **Increased productivity**: Fast language models can automate many NLP tasks, freeing up human time and effort for more creative and strategic work. This leads to increased productivity and efficiency in industries like content creation, writing, and editing.\n",
      "4. **Enhanced decision-making**: Fast language models can rapidly analyze large amounts of text data, enabling faster decision-making in areas like sentiment analysis, risk assessment, and predictive analytics.\n",
      "5. **Scalability**: Fast language models can handle large volumes of text data, making them suitable for large-scale applications like social media monitoring, customer feedback analysis, and text summarization.\n",
      "6. **Cost-effective**: Fast language models can reduce the computational resources and energy required for NLP tasks, making them a cost-effective solution for businesses and organizations.\n",
      "7. **Accessibility**: Fast language models can make NLP capabilities more accessible to people with disabilities, such as those who are visually or hearing impaired, by enabling faster and more accurate text-to-speech and speech-to-text systems.\n",
      "8. **Cybersecurity**: Fast language models can help detect and respond to cyber threats in real-time, enabling more effective cybersecurity measures to protect against attacks and data breaches.\n",
      "9. **Healthcare applications**: Fast language models can facilitate rapid analysis of medical literature, enabling healthcare professionals to quickly identify patterns and make informed decisions in high-stakes situations.\n",
      "10. **Advancements in AI research**: Fast language models drive innovation in AI research, pushing the boundaries of what is possible with NLP and paving the way for future breakthroughs in areas like multimodal understanding and human-AI collaboration.\n",
      "\n",
      "In summary, fast language models are crucial for a wide range of applications, from improving user experiences to enhancing decision-making and driving innovation in AI research.\n",
      "--------------------\n",
      "Fast language models are crucial in today's computing landscape, and their importance can be understood from several perspectives:\n",
      "\n",
      "1. **Real-time Applications**: Fast language models enable real-time processing of natural language inputs, which is crucial for applications like:\n",
      "\t* Chatbots and virtual assistants, which require instant responses to user queries.\n",
      "\t* Sentiment analysis and opinion mining, where timely insights are essential for business decision-making.\n",
      "\t* Language translation and localization, where speed is vital for effective communication.\n",
      "2. **Efficient Computing**: Fast language models reduce the computational resources and energy required for processing, leading to:\n",
      "\t* Cost savings: Reduced energy consumption and lower infrastructure costs.\n",
      "\t* Environmental benefits: Less energy consumption contributes to a more sustainable future.\n",
      "3. **Scalability**: Fast language models can handle large volumes of data, making them suitable for:\n",
      "\t* Big data analytics: Processing massive datasets quickly and efficiently.\n",
      "\t* High-volume NLP applications: Handling large quantities of text data in applications like customer service chatbots.\n",
      "4. **Improved User Experience**: Fast language models enable:\n",
      "\t* Instant feedback and responses, leading to a more seamless user experience.\n",
      "\t* Faster development and deployment of NLP-based applications, reducing time-to-market.\n",
      "5. **Competitive Advantage**: Organizations that leverage fast language models can:\n",
      "\t* Gain a competitive edge in their respective industries.\n",
      "\t* Stay ahead of competitors in terms of innovation and responsiveness.\n",
      "6. **Research and Development**: Fast language models accelerate the R&D process by:\n",
      "\t* Enabling researchers to test and iterate on models more quickly.\n",
      "\t* Reducing the time and resources required for experimentation.\n",
      "7. **Edge Computing and IoT**: Fast language models are essential for edge computing and IoT applications, where:\n",
      "\t* Low-latency processing is critical for real-time decision-making.\n",
      "\t* Limited computing resources and energy constraints require efficient processing.\n",
      "8. **Multilingual Support**: Fast language models can handle multiple languages, facilitating:\n",
      "\t* Global communication and collaboration.\n",
      "\t* International business and trade.\n",
      "9. **Accessibility**: Fast language models can improve accessibility for people with disabilities, such as:\n",
      "\t* Real-time speech-to-text systems for individuals with speech or hearing impairments.\n",
      "\t* Faster translation and interpretation for people who are deaf or hard of hearing.\n",
      "10. **Future-Proofing**: As language models continue to evolve, fast language models will be essential for:\n",
      "\t* Integrating with emerging technologies like augmented reality, virtual reality, and brain-computer interfaces.\n",
      "\t* Adapting to new languages and dialects as they emerge.\n",
      "\n",
      "In summary, fast language models are critical for various applications, enabling real-time processing, efficient computing, scalability, and improved user experiences. They also provide a competitive advantage, accelerate research and development, and facilitate multilingual support, accessibility, and future-proofing.\n",
      "--------------------\n",
      "Fast language models, also known as efficient language models or lightweight language models, are compact and computationally efficient artificial neural networks designed for natural language processing (NLP) tasks. The importance of fast language models lies in their ability to provide accurate results while reducing computational resources, energy consumption, and latency. Here are some key reasons why fast language models are important:\n",
      "\n",
      "1. **Real-time processing**: Fast language models enable real-time processing of natural language inputs, which is essential for applications like chatbots, voice assistants, and sentiment analysis. Faster processing times enable quicker response times, leading to better user experiences.\n",
      "2. **Energy efficiency**: Fast language models consume less power and reduce the carbon footprint of NLP applications. This is crucial for devices with limited power supplies, such as mobile devices, IoT devices, or edge computing architectures.\n",
      "3. **Deployment on resource-constrained devices**: Fast language models can be deployed on devices with limited computational resources, such as smart home devices, wearables, or embedded systems. This expands the reach of NLP technology to a broader range of devices and applications.\n",
      "4. **Low-latency inference**: Fast language models reduce the latency between input and output, enabling applications that require rapid responses, such as language translation, question answering, or text summarization.\n",
      "5. **Improved scalability**: Fast language models can handle a larger volume of requests, making them suitable for large-scale NLP applications, such as customer service chatbots or sentiment analysis for social media platforms.\n",
      "6. **Cost-effectiveness**: By reducing computational resources and energy consumption, fast language models can lead to significant cost savings for organizations and individuals deploying NLP applications.\n",
      "7. **Enhanced user experience**: Fast language models enable more responsive and interactive user experiences, such as conversational interfaces, voice-controlled systems, and language-based games.\n",
      "8. **Increased accessibility**: Fast language models can enable NLP applications for people with disabilities, such as speech-to-text systems for individuals with mobility or dexterity impairments.\n",
      "9. **Edge AI and decentralized AI**: Fast language models are essential for Edge AI and decentralized AI applications, where data is processed locally on devices or at the edge of networks, reducing latency and bandwidth requirements.\n",
      "10. **Advancements in NLP research**: The development of fast language models drives innovation in NLP research, encouraging the exploration of new architectures, techniques, and applications that can benefit from efficient processing of natural language inputs.\n",
      "\n",
      "In summary, fast language models are crucial for various applications that require efficient processing of natural language inputs, enabling real-time responses, reducing energy consumption, and improving user experiences.\n",
      "--------------------\n",
      "Fast language models have revolutionized the field of Natural Language Processing (NLP) and have numerous applications in various industries. The importance of fast language models can be summarized as follows:\n",
      "\n",
      "1. **Real-time Applications**: Fast language models enable real-time language processing, which is crucial for applications that require immediate responses, such as:\n",
      "\t* Virtual assistants (e.g., Siri, Alexa, Google Assistant)\n",
      "\t* Chatbots for customer support\n",
      "\t* Sentiment analysis for social media monitoring\n",
      "2. **Efficient Computation**: Fast language models reduce the computational resources required for processing large amounts of text data, making them more efficient and cost-effective. This is particularly important for:\n",
      "\t* Large-scale NLP applications, such as text classification, sentiment analysis, and entity extraction\n",
      "\t* Edge AI applications, where computational resources are limited\n",
      "3. **Improved User Experience**: Faster language models lead to faster response times, which enhance the user experience in various applications, such as:\n",
      "\t* Language translation apps (e.g., Google Translate)\n",
      "\t* AI-powered writing assistants (e.g., Grammarly)\n",
      "\t* Conversational interfaces for customer service\n",
      "4. **Enhanced Accuracy**: Fast language models can process larger datasets and perform more iterations, leading to more accurate models that:\n",
      "\t* Better capture nuances of language\n",
      "\t* Improve language understanding and generation capabilities\n",
      "\t* Enhance performance in downstream NLP tasks\n",
      "5. **Facilitate Research and Development**: Fast language models accelerate the research and development process in NLP, enabling researchers to:\n",
      "\t* Explore new ideas and concepts more quickly\n",
      "\t* Iterate on models and experiments faster\n",
      "\t* Investigate new applications and use cases\n",
      "6. **Enable Edge AI and IoT Applications**: Fast language models are essential for Edge AI applications, where models need to process data in real-time on resource-constrained devices, such as:\n",
      "\t* Smart home devices\n",
      "\t* Autonomous vehicles\n",
      "\t* Wearable devices\n",
      "7. **Support Multimodal Processing**: Fast language models can process multimodal data (e.g., text, speech, images) more efficiently, enabling applications that combine language understanding with computer vision and speech recognition, such as:\n",
      "\t* Multimodal chatbots\n",
      "\t* Voice-controlled interfaces\n",
      "\t* Visual question answering systems\n",
      "8. **Improved Scalability**: Fast language models can handle large volumes of data and scale to meet the demands of high-traffic applications, making them ideal for:\n",
      "\t* Social media analytics\n",
      "\t* Customer feedback analysis\n",
      "\t* Sentiment analysis for market research\n",
      "\n",
      "In summary, fast language models have a significant impact on various aspects of NLP, from improving user experience and enhancing accuracy to facilitating research and development and enabling emerging technologies like Edge AI.\n",
      "--------------------\n",
      "Fast language models are crucial in today's natural language processing (NLP) landscape, and their importance cannot be overstated. Here are some reasons why:\n",
      "\n",
      "1. **Real-time Processing**: Fast language models enable real-time processing of large volumes of text data, making them essential for applications that require swift responses, such as:\n",
      "\t* Chatbots and virtual assistants, which need to respond quickly to user queries.\n",
      "\t* Sentiment analysis and opinion mining, where timely insights are critical.\n",
      "\t* Language translation, where rapid processing is necessary for synchronous communication.\n",
      "2. **Scalability**: Fast language models can handle massive amounts of data, making them suitable for large-scale applications, such as:\n",
      "\t* Social media analytics, where processing vast volumes of user-generated content is necessary.\n",
      "\t* Text classification, where large datasets need to be processed quickly.\n",
      "\t* Recommendation systems, which rely on fast processing of user behavior data.\n",
      "3. **Low Latency**: Fast language models reduce latency, enabling applications that require instantaneous responses, such as:\n",
      "\t* Voice assistants, which need to respond quickly to voice commands.\n",
      "\t* Real-time language translation, where latency can impact the quality of communication.\n",
      "\t* Conversational AI, which requires rapid responses to maintain a natural flow of conversation.\n",
      "4. **Improved User Experience**: Fast language models lead to a better user experience by:\n",
      "\t* Providing quicker responses, which enhances user engagement and satisfaction.\n",
      "\t* Enabling more interactive and dynamic applications, such as chatbots and voice assistants.\n",
      "\t* Facilitating more accurate and timely insights, which inform better decision-making.\n",
      "5. **Resource Efficiency**: Fast language models can operate with limited computational resources, making them suitable for:\n",
      "\t* Edge computing, where computing resources are limited.\n",
      "\t* Mobile devices, where processing power and memory are constrained.\n",
      "\t* Real-time analytics, where quick processing is required without compromising on accuracy.\n",
      "6. **Advancements in AI Research**: Fast language models contribute to the development of more sophisticated AI models, as they:\n",
      "\t* Enable researchers to explore larger and more complex models.\n",
      "\t* Facilitate the creation of more accurate and efficient models.\n",
      "\t* Pave the way for breakthroughs in areas like explainability, robustness, and fairness.\n",
      "7. **Industrial Applications**: Fast language models have numerous industrial applications, including:\n",
      "\t* Customer service chatbots, which require rapid responses to customer inquiries.\n",
      "\t* Sentiment analysis for social media monitoring and brand reputation management.\n",
      "\t* Content generation and language translation for e-learning platforms.\n",
      "\n",
      "In summary, fast language models are essential for various applications, as they enable real-time processing, scalability, low latency, improved user experience, resource efficiency, advancements in AI research, and numerous industrial applications.\n",
      "--------------------\n",
      "Fast language models are crucial in various aspects of natural language processing (NLP) and artificial intelligence (AI) applications. Their importance can be summarized as follows:\n",
      "\n",
      "1. **Response Time**: Fast language models enable real-time or near-real-time processing, which is essential for applications like chatbots, virtual assistants, and conversational AI. Quick responses ensure a seamless user experience, improving engagement and satisfaction.\n",
      "2. **Scalability**: Fast models can handle large volumes of text data, making them suitable for applications that require processing massive datasets, such as text classification, sentiment analysis, and topic modeling.\n",
      "3. **Efficient Computing**: Fast language models reduce the computational resources required for processing, which leads to:\n",
      "\t* Lower energy consumption\n",
      "\t* Reduced server costs\n",
      "\t* Increased processing capacity\n",
      "4. **Improved Accuracy**: Faster models can be trained on larger datasets, leading to improved accuracy and better generalization capabilities.\n",
      "5. **Real-time Insights**: Fast language models enable real-time analytics, allowing for timely decision-making in applications like:\n",
      "\t* Sentiment analysis for customer feedback or social media monitoring\n",
      "\t* Topic modeling for trend identification and market research\n",
      "6. **Enhanced User Experience**: Fast language models can:\n",
      "\t* Provide instant suggestions and corrections for language translation, writing assistance, or content creation\n",
      "\t* Power virtual assistants and chatbots that can respond quickly to user queries\n",
      "7. **Competitive Advantage**: In applications like customer service, fast language models can help companies respond quickly to customer inquiries, providing a competitive advantage in terms of customer satisfaction and loyalty.\n",
      "8. **Research and Development**: Fast language models can accelerate research in NLP, enabling researchers to experiment with larger datasets, more complex models, and novel architectures.\n",
      "9. **Edge AI and IoT**: Fast language models are essential for edge AI and IoT applications, where processing power is limited, and latency is critical, such as in autonomous vehicles, smart homes, or wearables.\n",
      "10. **Environmental Impact**: By reducing the computational resources required for NLP tasks, fast language models can help minimize the environmental impact of large-scale AI deployments.\n",
      "\n",
      "In summary, fast language models are crucial for various AI and NLP applications, as they provide real-time processing, scalability, efficiency, and improved accuracy, ultimately leading to enhanced user experiences, competitive advantages, and environmental sustainability.\n",
      "--------------------\n",
      "Fast language models are crucial in today's natural language processing (NLP) landscape, and their importance can be seen in several aspects:\n",
      "\n",
      "1. **Efficient Inference**: Fast language models enable rapid processing of large amounts of text data, which is essential for many real-world applications, such as text classification, sentiment analysis, and language translation.\n",
      "\n",
      "2. **Real-time Applications**: Fast language models power real-time applications like chatbots, virtual assistants, and language translation systems that require instantaneous responses.\n",
      "\n",
      "3. **Scalability**: Fast language models can handle large volumes of text data, making them ideal for large-scale NLP applications, such as analyzing social media feeds, customer feedback, or vast amounts of text data.\n",
      "\n",
      "4. **Lower Computational Resources**: Fast language models reduce the computational resources required, making them more environmentally friendly and cost-effective.\n",
      "\n",
      "5. **Improved User Experience**: Fast language models provide a seamless user experience, enabling users to receive instant responses, which is critical in applications like customer service, language translation, and text-based interfaces.\n",
      "\n",
      "6. **Enhanced Productivity**: Fast language models streamline workflows, increasing the productivity of NLP developers, researchers, and practitioners by allowing them to experiment, test, and deploy models more quickly.\n",
      "\n",
      "7. **Advancements in NLP Research**: Fast language models accelerate the pace of NLP research, enabling researchers to explore new ideas, test hypotheses, and refine models more rapidly.\n",
      "\n",
      "Would you like me to elaborate on any of these points or explore other aspects of fast language models?\n",
      "--------------------\n",
      "Fast language models are crucial in today's natural language processing (NLP) landscape. Here are some reasons why:\n",
      "\n",
      "**Real-time Conversational Interactions**: Fast language models enable applications to respond quickly to user input, making conversations feel more natural and human-like. This is particularly important for virtual assistants, chatbots, and other applications that require rapid responses.\n",
      "\n",
      "**Improving User Experience**: Fast language models help reduce latency, which is critical for interactive systems like voice assistants, online helpdesks, or language translation apps. This leads to a better user experience, as users don't have to wait for the system to respond.\n",
      "\n",
      "**Handling Large Volumes of Data**: Fast language models can process large amounts of text data quickly, making them ideal for applications that require analyzing vast amounts of text data, such as social media monitoring, sentiment analysis, or information retrieval systems.\n",
      "\n",
      "**Enhanced Efficiency**: Fast language models can reduce computational resources, energy consumption, and infrastructure costs, leading to more efficient and cost-effective systems.\n",
      "\n",
      "**Applications in High-Stakes Environments**: Fast language models are essential in high-stakes environments, such as:\n",
      "\n",
      "1. **Healthcare**: Quick responses can be critical in medical applications, like diagnosing diseases or providing emergency medical information.\n",
      "2. **Finance**: Fast language models can help with real-time financial analysis, risk assessment, and trading decisions.\n",
      "3. **Security**: Rapid language processing is vital in cybersecurity, where timely threat detection and response are crucial.\n",
      "\n",
      "**Advancements in AI Research**: Fast language models can facilitate breakthroughs in AI research, as they enable researchers to experiment and iterate quickly, leading to faster development of new NLP models and techniques.\n",
      "\n",
      "**Competitive Advantage**: In industry, fast language models can provide a competitive edge by enabling companies to respond quickly to customer inquiries, process large amounts of data, or develop new products and services.\n",
      "\n",
      "**Real-world Examples**:\n",
      "\n",
      "* Amazon's Alexa and Google Assistant rely on fast language models to respond quickly to user queries.\n",
      "* Sentiment analysis tools, like Brandwatch, use fast language models to analyze vast amounts of social media data in real-time.\n",
      "* Language translation apps, such as Google Translate, rely on fast language models to provide instant translations.\n",
      "\n",
      "In summary, fast language models are essential for building responsive, efficient, and effective NLP applications that can process large amounts of data quickly, providing a competitive edge, improving user experience, and driving advancements in AI research.\n",
      "--------------------\n",
      "Fast language models are crucial in today's natural language processing (NLP) landscape, and their importance can be seen in several areas:\n",
      "\n",
      "1. **Real-time applications**: Fast language models enable real-time applications such as chatbots, virtual assistants, and language translation systems to provide instant responses to users. This responsiveness is essential in today's fast-paced digital landscape.\n",
      "2. **Improved user experience**: By providing rapid responses, fast language models enhance the user experience, making interactions more engaging, efficient, and appealing. This leads to increased user satisfaction, loyalty, and ultimately, business success.\n",
      "3. **Scalability**: Fast language models enable large-scale NLP applications to handle massive volumes of data, supporting millions of users, and processing vast amounts of text data in real-time.\n",
      "4. **Competitive advantage**: In today's competitive digital landscape, organizations that can respond quickly to user requests or provide rapid insights gain a significant competitive advantage over those that cannot.\n",
      "5. **Cost savings**: By reducing the computational resources required to process language, fast language models lead to cost savings and increased efficiency in terms of infrastructure, energy consumption, and maintenance.\n",
      "6. **Faster iteration and experimentation**: Fast language models enable developers and researchers to experiment and iterate faster, accelerating the development of new NLP-based products and services.\n",
      "7. **Enhanced security**: Fast language models can quickly detect and respond to security threats, such as spam, phishing, or malicious attacks, protecting users and systems from harm.\n",
      "8. **Improved accessibility**: By providing rapid language understanding and generation capabilities, fast language models can improve accessibility for people with disabilities, language barriers, or limited literacy.\n",
      "9. **Scientific discovery**: Fast language models can facilitate faster scientific discovery in areas like biomedical research, where rapid text analysis and insight generation can lead to breakthroughs in disease diagnosis, treatment, and prevention.\n",
      "10. **Societal impact**: Fast language models have the potential to positively impact society by enabling rapid language translation, improving communication, and fostering global understanding.\n",
      "\n",
      "To achieve fast language models, researchers and developers employ various techniques, including:\n",
      "\n",
      "1. Model pruning and knowledge distillation to reduce model size and complexity.\n",
      "2. Quantization and binarization to reduce computational requirements.\n",
      "3. Parallel processing and distributed computing to accelerate processing.\n",
      "4. Efficient neural network architectures, such as transformers, that can process sequences in parallel.\n",
      "5. Caching and memoization to reduce redundant computations.\n",
      "\n",
      "By pushing the boundaries of language model speed and efficiency, we can unlock new applications, improve user experiences, and drive innovation in various fields.\n",
      "--------------------\n",
      "Fast language models are crucial in today's natural language processing (NLP) landscape, and their importance can be understood from several perspectives:\n",
      "\n",
      "1. **Real-time Applications**: Fast language models enable real-time processing of text data, which is essential for applications that require instant responses, such as:\n",
      "\t* Chatbots and virtual assistants that need to respond quickly to user queries.\n",
      "\t* Sentiment analysis and opinion mining applications that require timely insights.\n",
      "\t* Real-time language translation systems that need to process and translate text on the fly.\n",
      "2. **Increased Throughput**: Fast language models can process large volumes of text data quickly, making them suitable for applications that require high-throughput processing, such as:\n",
      "\t* Text classification and clustering applications that need to process large datasets.\n",
      "\t* Information retrieval systems that need to index and retrieve large amounts of text data quickly.\n",
      "3. **Efficient Use of Resources**: Fast language models can reduce the computational resources required for NLP tasks, leading to:\n",
      "\t* Lower computational costs and energy consumption.\n",
      "\t* Faster development and deployment of NLP-based applications.\n",
      "\t* Improved scalability for large-scale NLP applications.\n",
      "4. **Improved User Experience**: Fast language models can provide a more responsive and engaging user experience, particularly in applications that require interactive text-based interfaces, such as:\n",
      "\t* Conversational AI systems that need to respond quickly to user input.\n",
      "\t* Language translation systems that need to provide instant translations.\n",
      "5. **Enhanced Security**: Fast language models can help detect and respond to security threats more quickly, such as:\n",
      "\t* Anomaly detection and threat detection systems that need to analyze large amounts of text data in real-time.\n",
      "\t* Spam and malware detection systems that require fast text analysis.\n",
      "6. **Research and Development**: Fast language models can accelerate research and development in NLP, enabling researchers to:\n",
      "\t* Explore new techniques and models more quickly.\n",
      "\t* Experiment with larger datasets and more complex models.\n",
      "\t* Develop and deploy NLP-based applications more rapidly.\n",
      "7. **Edge AI and IoT Applications**: Fast language models are essential for edge AI and IoT applications, where processing power and data storage are limited, such as:\n",
      "\t* Smart home devices that need to process and respond to voice commands quickly.\n",
      "\t* Autonomous vehicles that require fast processing of sensor data and text-based inputs.\n",
      "\n",
      "In summary, fast language models are essential for building responsive, efficient, and scalable NLP-based applications that can process large volumes of text data quickly and accurately.\n",
      "--------------------\n",
      "Fast language models have revolutionized the field of natural language processing (NLP) and have numerous applications in various industries. The importance of fast language models can be summarized as follows:\n",
      "\n",
      "1. **Efficient Processing**: Fast language models can process vast amounts of text data quickly, enabling applications that require real-time or near-real-time processing, such as chatbots, sentiment analysis, and language translation.\n",
      "2. **Improved User Experience**: Fast language models enable responsive and interactive systems, allowing users to receive immediate feedback and responses, which is essential for applications like virtual assistants, customer service chatbots, and language-based interfaces.\n",
      "3. **Scalability**: Fast language models can handle large volumes of data and scale to meet the demands of high-traffic applications, making them suitable for large-scale industrial and enterprise applications.\n",
      "4. **Cost-Effective**: Fast language models can reduce computational resources and energy consumption, leading to cost savings and environmental benefits.\n",
      "5. **Real-time Insights**: Fast language models enable real-time analytics and insights, allowing businesses to respond quickly to changing market trends, customer sentiment, and other time-sensitive information.\n",
      "6. **Enhanced Security**: Fast language models can detect and respond to security threats, such as spam, phishing, and malicious activities, in real-time, protecting sensitive data and preventing cyber attacks.\n",
      "7. **Language Understanding**: Fast language models can better understand nuances of language, context, and intent, enabling more accurate language translation, sentiment analysis, and text summarization.\n",
      "8. **Multimodal Interaction**: Fast language models can integrate with multimodal interfaces, such as speech, vision, and gestures, enabling more natural and intuitive human-computer interactions.\n",
      "9. **Domain Adaptation**: Fast language models can adapt quickly to new domains, languages, and dialects, making them suitable for applications in diverse cultural and linguistic contexts.\n",
      "10. **Advancements in AI**: Fast language models can accelerate research and development in AI, enabling breakthroughs in areas like conversational AI, natural language understanding, and human-AI collaboration.\n",
      "11. **Improved Accessibility**: Fast language models can facilitate language accessibility for people with disabilities, enabling more inclusive and equitable communication.\n",
      "12. **Enhanced Creativity**: Fast language models can generate creative content, such as text, stories, and dialogue, inspiring new forms of artistic expression and entertainment.\n",
      "\n",
      "In summary, fast language models are essential for building efficient, scalable, and responsive AI systems that can process and understand human language in real-time, enabling a wide range of applications that transform the way we work, communicate, and interact with technology.\n",
      "--------------------\n",
      "Fast language models are crucial in today's natural language processing (NLP) and artificial intelligence (AI) landscape. Here are some reasons why:\n",
      "\n",
      "1. **Real-time Applications**: Fast language models enable real-time applications, such as chatbots, virtual assistants, and language translation systems, to respond quickly and efficiently to user queries.\n",
      "2. **Efficient Processing**: Fast language models can process large amounts of text data quickly, making them ideal for applications that require rapid analysis and decision-making, such as sentiment analysis, text classification, and information retrieval.\n",
      "3. **Scalability**: Fast language models can handle large volumes of data and scale easily, making them suitable for applications that require processing massive amounts of text data, such as social media monitoring, customer feedback analysis, and text summarization.\n",
      "4. **Low Latency**: Fast language models enable low-latency applications, which are critical in applications like speech recognition, machine translation, and conversational AI, where responsiveness is essential.\n",
      "5. **Improved User Experience**: Fast language models provide a better user experience by reducing wait times, enabling more interactive systems, and fostering more engaging user interactions.\n",
      "6. **Competitive Advantage**: Fast language models can provide a competitive advantage in industries like customer service, marketing, and sales, where prompt and accurate responses are essential for customer satisfaction and conversion rates.\n",
      "7. **Research and Development**: Fast language models accelerate research and development in NLP and AI, enabling scientists and engineers to experiment, iterate, and innovate more quickly.\n",
      "8. **Resource Efficiency**: Fast language models can reduce computational resources and energy consumption, making them more environmentally friendly and cost-effective.\n",
      "9. **Edge Computing**: Fast language models are suitable for edge computing applications, where data processing occurs on the edge of the network, closer to the source of the data, reducing latency and improving real-time processing.\n",
      "10. **Future-Proofing**: Fast language models future-proof applications, enabling them to adapt to increasing data volumes, processing demands, and the growing complexity of language-based interactions.\n",
      "\n",
      "In summary, fast language models are essential for building responsive, efficient, and scalable NLP and AI applications that can process large amounts of data quickly, providing a competitive advantage, improving user experience, and driving innovation.\n",
      "--------------------\n",
      "Fast language models are crucial in today's natural language processing (NLP) landscape, and their importance can be attributed to several factors:\n",
      "\n",
      "1. **Real-time Processing**: Fast language models enable real-time processing of text data, which is essential for applications that require instant responses, such as chatbots, virtual assistants, and language translation systems.\n",
      "2. **Scalability**: Fast language models can handle large volumes of data, making them suitable for applications that involve massive datasets, such as social media monitoring, sentiment analysis, and text classification.\n",
      "3. **Improved User Experience**: Fast language models can significantly improve the user experience in applications like language translation, text summarization, and question-answering systems, by providing rapid responses and reducing latency.\n",
      "4. **Increased Efficiency**: Fast language models can process large amounts of text data quickly, reducing the computational resources required, and enabling faster development and deployment of NLP-based applications.\n",
      "5. **Enhanced Decision-Making**: Fast language models can provide rapid insights from text data, enabling quick decision-making in applications like news analysis, customer sentiment analysis, and financial forecasting.\n",
      "6. **Cost-Effective**: Fast language models can reduce computational costs by minimizing the time and resources required to process large datasets, making them a cost-effective solution for organizations.\n",
      "7. **Competitive Advantage**: Fast language models can provide a competitive advantage in various industries, such as customer service, marketing, and healthcare, by enabling rapid analysis and response to large volumes of text data.\n",
      "8. **Improved Accuracy**: Fast language models can achieve higher accuracy by processing large datasets quickly, which can lead to better predictions, classifications, and recommendations.\n",
      "9. **Real-world Applications**: Fast language models are essential for real-world applications like:\n",
      "\t* Sentiment analysis for customer feedback and product reviews\n",
      "\t* Text summarization for news articles and documents\n",
      "\t* Language translation for cross-border communication\n",
      "\t* Chatbots and virtual assistants for customer support\n",
      "\t* Speech recognition for voice-controlled devices\n",
      "10. **Advancements in AI Research**: Fast language models can accelerate research in AI, enabling scientists to explore new NLP applications, and push the boundaries of AI capabilities.\n",
      "\n",
      "In summary, fast language models are critical for various NLP applications, enabling real-time processing, scalability, and improved user experiences, while also driving innovation and efficiency in various industries.\n",
      "--------------------\n",
      "Fast language models have revolutionized the field of natural language processing (NLP) in recent years, and their importance cannot be overstated. Here are some reasons why fast language models are crucial:\n",
      "\n",
      "1. **Efficient Inference**: Fast language models enable rapid inference, which is critical in many real-world applications, such as:\n",
      "\t* Real-time chatbots and virtual assistants\n",
      "\t* Sentiment analysis and opinion mining\n",
      "\t* Text classification and categorization\n",
      "\t* Language translation and localization\n",
      "2. **Scalability**: Fast language models can handle large volumes of data, making them essential for applications that require processing massive amounts of text data, such as:\n",
      "\t* Analyzing social media posts and user feedback\n",
      "\t* Processing medical records and clinical notes\n",
      "\t* Extracting insights from customer reviews and ratings\n",
      "3. **Low Latency**: Fast language models are essential for applications that require immediate responses, such as:\n",
      "\t* Conversational AI and dialogue systems\n",
      "\t* Real-time language translation and interpretation\n",
      "\t* Quick response systems for customer support and service\n",
      "4. **Energy Efficiency**: Fast language models can reduce energy consumption and carbon footprint, making them an attractive solution for:\n",
      "\t* Edge computing and IoT devices\n",
      "\t* Mobile devices and embedded systems\n",
      "\t* Sustainability-focused applications\n",
      "5. **Improved Accuracy**: Fast language models can lead to improved accuracy by enabling more iterations, hyperparameter tuning, and ensemble methods, which are critical in applications like:\n",
      "\t* Sentiment analysis and emotion detection\n",
      "\t* Named entity recognition and information extraction\n",
      "\t* Question answering and natural language inference\n",
      "6. **Enabling New Applications**: Fast language models have opened up new possibilities for applications that were previously not feasible, such as:\n",
      "\t* Real-time language understanding and generation\n",
      "\t* Multimodal language processing (e.g., speech, vision, and language)\n",
      "\t* Human-AI collaboration and augmented intelligence\n",
      "7. **Reducing Computational Costs**: Fast language models can significantly reduce computational costs, making AI more accessible and affordable for a broader range of organizations and individuals.\n",
      "8. **Democratizing AI**: Fast language models can democratize access to AI technology, enabling smaller organizations and individuals to build and deploy AI-powered applications.\n",
      "9. **Enhancing User Experience**: Fast language models can improve user experience by providing faster and more accurate responses, leading to increased user engagement and satisfaction.\n",
      "10. **Advancing AI Research**: Fast language models can accelerate AI research by enabling faster experimentation, prototyping, and testing of new ideas and algorithms.\n",
      "\n",
      "In summary, fast language models are essential for building scalable, efficient, and accurate AI systems that can process large amounts of language data in real-time. They have far-reaching implications for various industries, applications, and research areas, and their importance will only continue to grow as AI becomes more pervasive.\n",
      "--------------------\n",
      "Fast language models are crucial in natural language processing (NLP) and have numerous applications in various industries. The importance of fast language models lies in their ability to efficiently process and analyze large amounts of text data, enabling:\n",
      "\n",
      "1. **Real-time processing**: Fast language models enable real-time processing of text data, allowing for instant responses and timely insights in applications like chatbots, virtual assistants, and sentiment analysis.\n",
      "2. **Scalability**: Fast language models can handle large volumes of data, making them suitable for large-scale applications like language translation, text summarization, and topic modeling.\n",
      "3. **Low latency**: Fast language models reduce the latency between input and response, enhancing user experience in applications like conversational AI, language translation, and speech recognition.\n",
      "4. **Improved accuracy**: Faster language models can process more data, leading to improved accuracy in tasks like language modeling, text classification, and named entity recognition.\n",
      "5. **Reduced computational resources**: Fast language models require less computational power, reducing the need for expensive hardware and making them more accessible to a wider range of organizations.\n",
      "6. **Increased accessibility**: Fast language models enable the development of more inclusive applications, such as language translation for people with disabilities or language barriers.\n",
      "7. **Enhanced customer experience**: Fast language models facilitate quicker responses to customer inquiries, leading to improved customer satisfaction and loyalty in applications like customer service chatbots.\n",
      "8. **Competitive advantage**: Fast language models provide a competitive advantage in applications like sentiment analysis, entity recognition, and text classification, enabling businesses to make data-driven decisions more quickly.\n",
      "9. **Improved decision-making**: Fast language models enable rapid analysis of large datasets, facilitating informed decision-making in fields like healthcare, finance, and marketing.\n",
      "10. **Advancements in AI research**: Fast language models accelerate the development of AI research, enabling researchers to explore new techniques, models, and applications more quickly.\n",
      "\n",
      "Applications of fast language models include:\n",
      "\n",
      "1. Conversational AI\n",
      "2. Language translation\n",
      "3. Sentiment analysis\n",
      "4. Text summarization\n",
      "5. Topic modeling\n",
      "6. Named entity recognition\n",
      "7. Speech recognition\n",
      "8. Chatbots and virtual assistants\n",
      "9. Customer service platforms\n",
      "10. Search engines and recommender systems\n",
      "\n",
      "In summary, fast language models are essential for various industries, enabling real-time processing, scalability, low latency, and improved accuracy, while also reducing computational resources and increasing accessibility.\n",
      "--------------------\n",
      "Fast language models are crucial in today's natural language processing (NLP) landscape, and their importance can be seen in several aspects:\n",
      "\n",
      "1. **Real-time Applications**: Fast language models enable real-time processing of text data, which is essential for applications that require immediate responses, such as:\n",
      "\t* Chatbots and virtual assistants, which need to respond quickly to user inputs.\n",
      "\t* Sentiment analysis and opinion mining, where timely insights can inform business decisions.\n",
      "\t* Language translation and localization, where speed is critical for international communication.\n",
      "2. **Efficient Processing**: Fast language models can process large volumes of text data quickly, making them ideal for:\n",
      "\t* Handling massive datasets, such as social media posts, customer feedback, or product reviews.\n",
      "\t* Performing tasks like text classification, named entity recognition, and part-of-speech tagging at scale.\n",
      "3. **Improved User Experience**: Fast language models can enhance the user experience in various applications, such as:\n",
      "\t* Auto-complete and Suggestions: Providing instant suggestions or completions for search queries, emails, or text messages.\n",
      "\t* Real-time Language Translation: Enabling instant translation of text, speech, or sign language.\n",
      "\t* Smart Writing Assistants: Offering timely grammar, spell, and style suggestions to enhance writing quality.\n",
      "4. **Competitive Advantage**: In industries like customer service, marketing, and sales, fast language models can provide a competitive advantage by:\n",
      "\t* Enabling rapid response times for customer inquiries, fostering positive customer experiences.\n",
      "\t* Analyzing and responding to customer feedback in real-time, helping businesses improve their offerings.\n",
      "\t* Facilitating quick market research, sentiment analysis, and competitor analysis to inform business strategies.\n",
      "5. **Research and Development**: Fast language models can accelerate the research and development process in NLP by:\n",
      "\t* Enabling rapid experimentation and prototyping of new models and techniques.\n",
      "\t* Facilitating the processing of large datasets, which is essential for training and testing complex models.\n",
      "6. **Edge Computing and IoT**: Fast language models are particularly important for edge computing and IoT applications, where:\n",
      "\t* Low latency and real-time processing are critical for tasks like voice assistants, autonomous vehicles, or smart home devices.\n",
      "\t* Limited computational resources and bandwidth require efficient language models that can operate within these constraints.\n",
      "7. **Scalability and Cost-Effectiveness**: Fast language models can help reduce computational resources and costs by:\n",
      "\t* Processing large datasets more efficiently, reducing the need for expensive hardware or cloud resources.\n",
      "\t* Enabling the use of smaller, more cost-effective models that can still achieve good performance.\n",
      "\n",
      "In summary, fast language models are essential for various applications, from real-time chatbots and sentiment analysis to efficient text processing and competitive advantage in business.\n",
      "--------------------\n",
      "Fast language models are crucial in various applications, and their importance can be summarized as follows:\n",
      "\n",
      "1. **Real-time Processing**: Fast language models enable real-time processing of large volumes of text data, which is essential for applications like sentiment analysis, chatbots, and language translation.\n",
      "2. **Efficient Inference**: Fast models reduce the computational resources required for inference, making them suitable for deployment on resource-constrained devices, such as mobile devices or embedded systems.\n",
      "3. **Scalability**: Fast models can handle large datasets and scale to meet the demands of big data, enabling applications like naturallanguage processing (NLP) at scale.\n",
      "4. **Low Latency**: Fast language models enable low-latency responses, which are critical in applications like conversational AI, voice assistants, and real-time feedback systems.\n",
      "5. **Enhanced User Experience**: Fast language models can provide instant responses, enhancing user experience in applications like chatbots, virtual assistants, and language translation apps.\n",
      "6. **Cost-Effective**: Fast language models can reduce computational costs, making them a cost-effective solution for businesses and organizations.\n",
      "7. **Edge AI**: Fast language models can be deployed on edge devices, enabling AI applications on devices with limited computational resources, such as smart home devices or autonomous vehicles.\n",
      "8. **Real-time Feedback**: Fast language models enable real-time feedback and correction, which is critical in applications like language learning platforms, writing assistance tools, and grammar checking software.\n",
      "9. **Enhanced Accessibility**: Fast language models can improve accessibility for people with disabilities, enabling faster and more accurate communication tools, such as speech-to-text systems.\n",
      "10. **Competitive Advantage**: Fast language models can provide a competitive advantage in various industries, such as customer service, healthcare, and finance, where rapid response times are critical.\n",
      "11. **Improved Accuracy**: Fast language models can lead to improved accuracy in applications like language translation, sentiment analysis, and text summarization, as they can process larger datasets and incorporate more contextual information.\n",
      "12. **Research and Development**: Fast language models can accelerate research in NLP, enabling researchers to experiment and iterate faster, driving innovation in the field.\n",
      "\n",
      "In summary, fast language models are essential for various applications that require rapid processing, scalability, and low latency, ultimately enhancing user experience, improving efficiency, and driving innovation.\n",
      "--------------------\n",
      "Fast language models are computationally efficient natural language processing (NLP) models that can process and respond to inputs quickly, often in real-time. The importance of fast language models lies in their ability to enable a wide range of applications that require rapid language understanding and generation. Here are some reasons why fast language models are important:\n",
      "\n",
      "1. **Real-time Applications**: Fast language models enable real-time applications such as chatbots, virtual assistants, and customer service bots to respond quickly to user queries, improving user experience and engagement.\n",
      "2. **Interactive Systems**: Fast language models are essential for interactive systems like voice assistants, voice-controlled devices, and language translation systems, which require rapid response times to provide a seamless user experience.\n",
      "3. **Conversational AI**: Fast language models are critical for conversational AI systems, such as dialogue systems and conversational interfaces, which need to process and respond to user input quickly to maintain the flow of conversation.\n",
      "4. **Low-Latency Requirements**: Certain applications, like speech recognition, language translation, and sentiment analysis, require fast language models to meet low-latency requirements and ensure timely decision-making.\n",
      "5. **Scalability**: Fast language models enable large-scale NLP applications, such as language translation platforms, to handle a high volume of requests without compromising performance.\n",
      "6. **Efficient Inference**: Fast language models can perform inference efficiently, reducing computational resources and energy consumption, making them suitable for deployment on resource-constrained devices.\n",
      "7. **Real-time Analytics**: Fast language models can facilitate real-time analytics, enabling businesses to promptly respond to customer feedback, sentiment, and opinions, and make data-driven decisions.\n",
      "8. **Accessibility**: Fast language models can improve accessibility for people with disabilities, such as those who rely on text-to-speech systems or language translation tools.\n",
      "9. **Emergency Response Systems**: Fast language models can be used in emergency response systems, such as crisis hotlines, to provide rapid language understanding and response.\n",
      "10. **Advancements in NLP Research**: The development of fast language models drives innovation in NLP research, enabling researchers to explore new applications and techniques that can benefit from rapid language processing.\n",
      "11. **Improved User Experience**: Fast language models can lead to improved user experiences in various applications, such as text-based interfaces, sentiment analysis, and language translation, by providing faster response times and more accurate results.\n",
      "12. **Cost-Effectiveness**: Fast language models can reduce computational costs and energy consumption, making them a cost-effective solution for large-scale NLP applications.\n",
      "\n",
      "In summary, fast language models are essential for various applications that require rapid language understanding and generation, including real-time applications, interactive systems, conversational AI, and low-latency requirements. Their importance lies in their ability to improve user experience, scalability, and efficiency while driving innovation in NLP research.\n",
      "--------------------\n",
      "Fast language models are crucial in natural language processing (NLP) and have numerous applications in various industries. The importance of fast language models can be summarized as follows:\n",
      "\n",
      "1. **Real-time Processing**: Fast language models enable real-time processing of large volumes of text data, making them suitable for applications that require immediate responses, such as:\n",
      "\t* Chatbots and virtual assistants\n",
      "\t* Sentiment analysis and opinion mining\n",
      "\t* Text classification and categorization\n",
      "2. **Improved User Experience**: Fast language models facilitate swift interaction with users, providing instant responses to their queries, and improving overall user experience in:\n",
      "\t* Virtual assistants (e.g., Siri, Alexa, Google Assistant)\n",
      "\t* Customer service chatbots\n",
      "\t* Language translation apps\n",
      "3. **Enhanced Productivity**: Fast language models can process large amounts of text data quickly, enabling:\n",
      "\t* Rapid document summarization and analysis\n",
      "\t* Efficient information retrieval and filtering\n",
      "\t* Streamlined content generation and writing assistance\n",
      "4. **Cost-Effective**: Fast language models can reduce computational resources and energy consumption, leading to cost savings and environmental benefits in:\n",
      "\t* Cloud computing and data centers\n",
      "\t* Edge computing and IoT devices\n",
      "5. **Scalability**: Fast language models can handle large volumes of text data, making them suitable for applications that require processing massive amounts of data, such as:\n",
      "\t* Social media monitoring and analysis\n",
      "\t* Sentiment analysis of large datasets\n",
      "\t* Text classification in big data analytics\n",
      "6. **Competitive Advantage**: Organizations that leverage fast language models can gain a competitive advantage in their respective industries, as they can:\n",
      "\t* Rapidly respond to changing market trends and customer needs\n",
      "\t* Improve customer satisfaction and loyalty\n",
      "\t* Enhance their reputation and brand image\n",
      "7. **Research and Development**: Fast language models facilitate rapid prototyping, experimentation, and testing of new NLP models and techniques, accelerating:\n",
      "\t* Research in NLP and AI\n",
      "\t* Development of new language models and applications\n",
      "8. **Societal Impact**: Fast language models can have a significant impact on various aspects of society, such as:\n",
      "\t* Healthcare: enabling rapid analysis of medical texts and improving patient care\n",
      "\t* Education: providing instant feedback and assessment tools for students\n",
      "\t* Accessibility: facilitating communication and enabling people with disabilities to interact with technology more easily\n",
      "9. **Edge AI and IoT**: Fast language models are essential for edge AI and IoT applications, where real-time processing and low latency are critical, such as:\n",
      "\t* Autonomous vehicles\n",
      "\t* Smart homes and cities\n",
      "\t* Industrial automation and monitoring\n",
      "10. **Future-Proofing**: As the volume and complexity of text data continue to grow, fast language models will be essential for future-proofing applications and ensuring they can handle the increasing demands of NLP tasks.\n",
      "\n",
      "In summary, fast language models are crucial for a wide range of applications, enabling real-time processing, improving user experience, enhancing productivity, and providing a competitive advantage.\n",
      "--------------------\n",
      "Fast language models are revolutionizing the field of natural language processing (NLP) by enabling rapid and efficient processing of vast amounts of text data. Here are some reasons why fast language models are important:\n",
      "\n",
      "1. **Real-time Applications**: Fast language models enable real-time applications such as chatbots, virtual assistants, and language translation systems to respond quickly and accurately to user queries.\n",
      "2. **Scalability**: With the ability to process large amounts of data rapidly, fast language models can handle massive datasets, making them ideal for applications that require analyzing vast amounts of text data.\n",
      "3. **Improved Accuracy**: Fast language models can be trained on larger datasets, leading to improved accuracy and better performance in various NLP tasks such as language translation, sentiment analysis, and text classification.\n",
      "4. **Enhanced User Experience**: Fast language models provide a seamless user experience in applications such as language translation, sentiment analysis, and text summarization, leading to increased user satisfaction and engagement.\n",
      "5. **Competitive Advantage**: Organizations that adopt fast language models can gain a competitive advantage over those that rely on slower models, enabling them to respond faster to changing market conditions and customer needs.\n",
      "6. **Research and Development**: Fast language models accelerate research and development in NLP, enabling researchers to experiment with new ideas and techniques quickly, leading to faster breakthroughs and innovations.\n",
      "7. ** Edge Computing**: With the proliferation of IoT devices, fast language models can be deployed at the edge, enabling real-time processing of text data on resource-constrained devices.\n",
      "8. **Low-Latency Applications**: Fast language models enable low-latency applications such as speech recognition, voice assistants, and real-time sentiment analysis, which require instantaneous processing of text data.\n",
      "9. **Big Data Analytics**: Fast language models can handle the massive amounts of unstructured text data generated by social media, IoT devices, and other sources, enabling organizations to extract insights and value from this data.\n",
      "10. **Environmental Benefits**: By reducing the computational resources required to process text data, fast language models can help reduce the carbon footprint of NLP applications, contributing to a more sustainable future.\n",
      "\n",
      "To achieve fast language models, researchers and developers employ various techniques, including:\n",
      "\n",
      "1. **Parallelization**: Distributing computations across multiple processors or GPUs to accelerate processing.\n",
      "2. **Optimization**: Applying optimization techniques, such as pruning, quantization, and knowledge distillation, to reduce model size and computational requirements.\n",
      "3. **Efficient Architectures**: Designing models with efficient architectures, such as Transformers, that can process text data rapidly.\n",
      "4. **Specialized Hardware**: Utilizing specialized hardware, such as TPUs or GPUs, optimized for NLP computations.\n",
      "\n",
      "By leveraging these techniques, fast language models can unlock new possibilities in NLP, enabling organizations to process, analyze, and respond to vast amounts of text data in real-time, driving innovation and business value.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "# Set the environment variable in the script\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "client = Groq(api_key=os.environ.get(\"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\"))\n",
    "\n",
    "for i in range(20):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Explain the importance of fast language models\",\n",
    "            }\n",
    "        ],\n",
    "        model=\"llama3-70b-8192\",\n",
    "    )\n",
    "    print(\"-\"*20)\n",
    "    print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
