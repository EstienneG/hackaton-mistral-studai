{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# RAGAS: Automated Evaluation of Retrieval Augmented Generation Shahul Es † , Jithin James † , Luis Espinosa-Anke ∗♢ , Steven Schockaert ∗ † Exploding Gradients ∗ CardiffNLP, Cardiff University, United Kingdom ♢ AMPLYFI, United Kingdom shahules786@gmail.com,jamesjithin97@gmail.com {espinosa-ankel,schockaerts1}@cardiff.ac.uk Abstract We introduce RAGA S ( R etrieval A ugmented G eneration As sessment), a framework for reference-free evaluation of Retrieval Aug- mented Generation (RAG) pipelines.\n",
      "RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natu- ral language layer between a user and textual databases, reducing the risk of hallucinations.\n",
      "Evaluating RAG architectures is, however, chal- lenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the gener- ation itself. With RAGA S , we put forward a suite of metrics which can be used to evaluate these different dimensions without having to rely on ground truth human annotations . We posit that such a framework can crucially con- tribute to faster evaluation cycles of RAG archi- tectures, which is especially important given the fast adoption of LLMs.\n",
      "1 Introduction Language Models (LMs) capture a vast amount of knowledge about the world, which allows them to answer questions without accessing any exter- nal sources. This idea of LMs as repositories of knowledge emerged shortly after the introduction of BERT ( Devlin et al.\n",
      ", 2019 ) and became more firmly established with the introduction of ever larger LMs ( Roberts et al.\n",
      ", 2020 ). While the most recent Large Language Models (LLMs) capture enough knowledge to rival human performance across a wide variety of question answering bench- marks ( Bubeck et al.\n",
      ", 2023 ), the idea of using LLMs as knowledge bases still has two fundamen- tal limitations. First, LLMs are not able to answer questions about events that have happened after they were trained. Second, even the largest models struggle to memorise knowledge that is only rarely mentioned in the training corpus ( Kandpal et al.\n",
      ", 2022 ; Mallen et al.\n",
      ", 2023 ). The standard solution to these issues is to rely on Retrieval Augmented Generation (RAG) ( Lee et al.\n",
      ", 2019 ; Lewis et al.\n",
      ", 2020 ; Guu et al.\n",
      ", 2020 ). Answering a question then essentially involves retrieving relevant pas- sages from a corpus and feeding these passages, along with the original question, to the LM. While initial approaches relied on specialised LMs for retrieval-augmented language modelling ( Khandel- wal et al.\n",
      ", 2020 ; Borgeaud et al.\n",
      ", 2022 ), recent work has suggested that simply adding retrieved docu- ments to the input of a standard LM can also work well ( Khattab et al.\n",
      ", 2022 ; Ram et al.\n",
      ", 2023 ; Shi et al.\n",
      ", 2023 ), thus making it possible to use retrieval- augmented strategies in combination with LLMs that are only available through APIs.\n",
      "While the usefulness of retrieval-augmented strategies is clear, their implementation requires a significant amount of tuning, as the overall per- formance will be affected by the retrieval model, the considered corpus, the LM, or the prompt for- mulation, among others. Automated evaluation of retrieval-augmented systems is thus paramount. In practice, RAG systems are often evaluated in terms of the language modelling task itself, i.e. by mea- suring perplexity on some reference corpus. How- ever, such evaluations are not always predictive of downstream performance ( Wang et al.\n",
      ", 2023c ).\n",
      "Moreover, this evaluation strategy relies on the LM probabilities, which are not accessible for some closed models (e.g. ChatGPT and GPT-4). Ques- tion answering is another common evaluation task, but usually only datasets with short extractive an- swers are considered, which may not be represen- tative of how the system will be used.\n",
      "To address these issues, in this paper we present RAGA S 1 , a framework for the automated assess- 1 RAGA S is available at https://github.com/ explodinggradients/ragas .\n",
      "# arXiv:2309.15217v1  [cs.CL]  26 Sep 2023ment of retrieval augmented generation systems.\n",
      "We focus on settings where reference answers may not be available, and where we want to estimate different proxies for correctness, in addition to the usefulness of the retrieved passages. The RAGA S framework provides an integration with both llama- index and Langchain , the most widely used frame- works for building RAG solutions, thus enabling developers to easily integrate RAGA S into their standard workflow.\n",
      "2 Related Work Estimating faithfulness using LLMs The prob- lem of detecting hallucinations in LLM generated responses has been extensively studied ( Ji et al.\n",
      ", 2023 ). Several authors have suggested the idea of predicting factuality using a few-shot prompt- ing strategy ( Zhang et al.\n",
      ", 2023 ). Recent analy- ses, however, suggest that existing models struggle with detecting hallucination when using standard prompting strategies ( Li et al.\n",
      ", 2023 ; Azaria and Mitchell , 2023 ). Other approaches rely on linking the generated responses to facts from an external knowledge base ( Min et al.\n",
      ", 2023 ), but this is not always possible.\n",
      "Yet another strategy is to inspect the probabili- ties assigned to individual tokens, where we would expect the model to be less confident in halluci- nated answers than in factual ones. For instance, BARTScore ( Yuan et al.\n",
      ", 2021 ) estimates factuality by looking at the conditional probability of the gen- erated text given the input.\n",
      "Kadavath et al.\n",
      "( 2022 ) use a variation of this idea. Starting from the ob- servation that LLMs provide well-calibrated proba- bilities when answering multiple-choice questions, they essentially convert the problem of validating model generated answers into a multiple-choice question which asks whether the answer is true or false. Rather than looking at the output probabil- ities, Azaria and Mitchell ( 2023 ) propose to train a supervised classifier on the weights from one of the hidden layers of the LLM, to predict whether a given statement is true or not. While the approach performs well, the need to access the hidden states of the model makes it unsuitable for systems that access LLMs through an API.\n",
      "For models that do not provide access to token probabilities, such as ChatGPT and GPT-4, differ- ent methods are needed. SelfCheckGPT ( Manakul et al.\n",
      ", 2023 ) addresses this problem by instead sam- pling multiple answers. Their core idea is that factual answers are more stable: when an answer is factual, we can expect that different samples will tend to be semantically similar, whereas this is less likely to be the case for hallucinated answers.\n",
      "Automated evaluation of text generation systems LLMs have also been leveraged to automatically evaluate other aspects of generated text fragments, beyond factuality. For instance, GPTScore ( Fu et al.\n",
      ", 2023 ) uses a prompt that specifies the consid- ered aspect (e.g. fluency) and then scores passages based on the average probability of the generated tokens, according to a given autoregressive LM.\n",
      "This idea of using prompts was previously also considered by Yuan et al.\n",
      "( 2021 ), although they used a smaller fine-tuned LM (i.e. BART) and did not observe a clear benefit from using prompts. An- other approach directly asks ChatGPT to evaluate a particular aspect of the given answer by provid- ing a score between 0 and 100, or by providing a rating on a 5-star scale ( Wang et al.\n",
      ", 2023a ). Re- markably, strong results can be obtained in this way, although it comes with the limitation of being sensitive to the design of the prompt. Rather than scoring individual answers, some authors have also focused on using an LLM to select the best answer among a number of candidates ( Wang et al.\n",
      ", 2023b ), typically to compare the performance of different LLMs. However, care is needed with this approach, as the order in which the answers is presented can influence the result ( Wang et al.\n",
      ", 2023b ).\n",
      "In terms of how ground truth answers or, more generally, generations, have been typically used in the literature, most approaches have relied on the availability of one or more reference answers.\n",
      "For instance, BERTScore ( Zhang et al.\n",
      ", 2020 ) and MoverScore ( Zhao et al.\n",
      ", 2019 ) use contex- tualised embeddings, produced by a pre-trained BERT model, to compare the similarity between the generated answer and the reference answers.\n",
      "BARTScore ( Yuan et al.\n",
      ", 2021 ) similarly uses refer- ence answers to compute aspects such as precision (estimated as the probability of generating the gen- erated answer given the reference) and recall (esti- mated as the probability of generating the reference given the generated answer).\n",
      "3 Evaluation Strategies We consider a standard RAG setting, where given a question q , the system first retrieves some context c ( q ) and then uses the retrieved context to generate an answer a s ( q ) . When building a RAG system,we usually do not have access to human-annotated datasets or reference answers. We therefore fo- cus on metrics that are fully self-contained and reference-free. We focus in particular three quality aspects, which we argue are of central importance.\n",
      "First, Faithfulness refers to the idea that the an- swer should be grounded in the given context. This is important to avoid hallucinations, and to ensure that the retrieved context can act as a justification for the generated answer. Indeed, RAG systems are often used in applications where the factual con- sistency of the generated text w.r.t. the grounded sources is highly important, e.g. in domains such as law, where information is constantly evolving. Sec- ond, Answer Relevance refers to the idea that the generated answer should address the actual ques- tion that was provided. Finally, Context Relevance refers to the idea that the retrieved context should be focused, containing as little irrelevant informa- tion as possible. This is important given the cost associated with feeding long context passages to LLMs. Moreover, when context passages are too long, LLMs are often less effective in exploiting that context, especially for information that is pro- vided in the middle of the context passage ( Liu et al.\n",
      ", 2023 ).\n",
      "We now explain how these three quality aspects can be measured in a fully automated way, by prompting an LLM. In our implementation and experiments, all prompts are evaluated using the gpt-3.5-turbo-16k model, which is available through the OpenAI API 2 .\n",
      "Faithfulness We say that the answer a s ( q ) is faithful to the context c ( q ) if the claims that are made in the answer can be inferred from the con- text. To estimate faithfulness, we first use an LLM to extract a set of statements, S ( a s ( q )) . The aim of this step is to decompose longer sentences into shorter and more focused assertions. We use the following prompt for this step 3 : Given a question and answer, create one or more statements from each sentence in the given answer.\n",
      "question: [question] answer: [answer] where [question] and [answer] refer to the given question and answer. For each statement s i 2 https://platform.openai.com 3 To help clarify the task, we include a demonstration as part of the prompt. This demonstration is not explicitly shown in the listing of the prompts throughout this paper.\n",
      "in S , the LLM determines if s i can be inferred from c ( q ) using a verification function v ( s i , c ( q )) . This verification step is carried out using the following prompt: Consider the given context and following statements, then determine whether they are supported by the information present in the context. Provide a brief explana- tion for each statement before arriving at the verdict (Yes/No). Provide a final verdict for each statement in order at the end in the given format. Do not deviate from the specified format.\n",
      "statement: [statement 1] ...\n",
      "statement: [statement n ] The final faithfulness score, F , is then computed as F = | V | | S | , where | V | is the number of statements that were supported according to the LLM and | S | is the total number of statements.\n",
      "Answer relevance We say that the answer a s ( q ) is relevant if it directly addresses the question in an appropriate way. In particular, our assessment of answer relevance does not take into account fac- tuality, but penalises cases where the answer is incomplete or where it contains redundant informa- tion. To estimate answer relevance, for the given answer a s ( q ) , we prompt the LLM to generate n potential questions q i based on a s ( q ) , as follows: Generate a question for the given answer.\n",
      "answer : [answer] We then obtain embeddings for all questions us- ing the text-embedding-ada-002 model, avail- able from the OpenAI API. For each q i , we cal- culate the similarity sim ( q, q i ) with the original question q , as the cosine between the correspond- ing embeddings. The answer relevance score, AR , for question q is then computed as: AR = 1 n n X i =1 sim ( q, q i ) (1) This metric evaluates how closely the generated answer aligns with the initial question or instruc- tion.\n",
      "Context relevance The context c ( q ) is consid- ered relevant to the extent that it exclusively con- tains information that is needed to answer the ques- tion. In particular, this metric aims to penalise theinclusion of redundant information. To estimate context relevance, given a question q and its con- text c ( q ) , the LLM extracts a subset of sentences, S ext , from c ( q ) that are crucial to answer q , using the following prompt: Please extract relevant sentences from the provided context that can potentially help answer the following question. If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return the phrase \"Insufficient Information\". While extract- ing candidate sentences you’re not al- lowed to make any changes to sentences from given context.\n",
      "The context relevance score is then computed as: CR = number of extracted sentences total number of sentences in c ( q ) (2) 4 The WikiEval Dataset To evaluate the proposed framework, we ideally need examples of question-context-answer triples which are annotated with human judgments. We can then verify to what extent our metrics agree with human assessments of faithfulness, answer relevance and context relevance. Since we are not aware of any publicly available datasets that could be used for this purpose, we created a new dataset, which we refer to as WikiEval 4 . To construct the dataset, we first selected 50 Wikipedia pages cov- ering events that have happened since the start of 2022 5 . In selecting these pages, we prioritised those with recent edits. For each of the 50 pages, we then asked ChatGPT to suggest a question that can be answered based on the introductory section of the page, using the following prompt: Your task is to formulate a question from given context satisfying the rules given below: 1. The question should be fully answered from the given context.\n",
      "2. The question should be framed from a part that contains non-trivial informa- tion.\n",
      "3. The answer should not contain any 4 https://huggingface.co/datasets/ explodinggradients/WikiEval 5 That is, beyond the reported training cutoff of the model we used in our experiments.\n",
      "links.\n",
      "4. The question should be of moderate difficulty.\n",
      "5. The question must be reasonable and must be understood and responded to by humans.\n",
      "6. Do not use phrases that ’provided con- text’, etc in the question context: We also used ChatGPT to answer the generated question, when given the corresponding introduc- tory section as context, using the following prompt: Answer the question using the informa- tion from the given context.\n",
      "question: [question] context: [context] All questions were annotated along the three con- sidered quality dimensions by two annotators. Both annotators were fluent in English and were given clear instructions about the meaning of the three considered quality dimensions. For faithfulness and context relevance, the two annotators agreed in around 95% of cases. For answer relevance, they agreed in around 90% of the cases. Disagreements were resolved after a discussion between the anno- tators.\n",
      "Faithfulness To obtain human judgements about faithfulness, we first used ChatGPT to answer the question without access to any additional context.\n",
      "We then asked the annotators to judge which of the two answers was the most faithful (i.e. the standard one or the one generated without context), given the question and corresponding Wikipedia page.\n",
      "Answer relevance We first used ChatGPT to obtain candidate answers with lower answer rel- evance, using the following prompt: Answer the given question in an incom- plete manner.\n",
      "question: [question] We then asked human annotators to compare this answer, and indicate which of the two answers had the highest answer relevance.\n",
      "Context relevance To measure this aspect, we first added additional sentences to the context by scraping back-links to the corresponding Wikipedia page. In this way, we were able to add information to the context that was related but less relevant forFaith.\n",
      "Ans. Rel.\n",
      "Cont. Rel.\n",
      "RAGAs 0.95 0.78 0.70 GPT Score 0.72 0.52 0.63 GPT Ranking 0.54 0.40 0.52 Table 1: Agreement with human annotators in pairwise comparisons of faithfulness, answer relevance and con- text relevance, using the WikEval dataset (accuracy).\n",
      "answering the question. For the few pages with- out any back-links, we instead used ChatGPT to complete the given context.\n",
      "5 Experiments Table 1 analyses the agreement between the met- rics proposed in Section 3 and the human assess- ments from the proposed WikiEval dataset. Each WikiEval instance requires the model to compare two answers or two context fragments. We count how often the answer/context preferred by the model (i.e. with highest estimated faithfulness, an- swer relevance, or context relevance) coincides with the answer/context preferred by the human annotators. We report the results in terms of ac- curacy (i.e. the fraction of instances on which the model agrees with the annotators).\n",
      "To put the results in context, we compare our proposed metrics (shown as RAGAs in Table 1 ) with two baseline methods. For the first method, shown as GPT Score , we ask ChatGPT to assign a score between 0 and 10 for the three quality dimensions.\n",
      "To this end, we use a prompt that describes the meaning of the quality metric and then asks to score the given answer/context in line with that definition. For instance, for evaluating faithfulness, we used the following prompt: Faithfulness measures the information consistency of the answer against the given context. Any claims that are made in the answer that cannot be deduced from context should be penalized.\n",
      "Given an answer and context, assign a score for faithfulness in the range 0-10.\n",
      "context : [context] answer : [answer] Ties, where the same score is assigned by the LLM to both answer candidates, were broken randomly.\n",
      "The second baseline, shown as GPT Ranking , in- stead asks ChatGPT to select the preferred answer/- context. In this case, the prompt again includes a definition of the considered quality metric. For instance, for evaluating answer relevance, we used the following prompt: Answer Relevancy measures the degree to which a response directly addresses and is appropriate for a given question.\n",
      "It penalizes the present of redundant in- formation or incomplete answers given a question. Given an question and answer, rank each answer based on Answer Rele- vancy.\n",
      "question : [question] answer 1 : [answer 1] answer 2 : [answer 2] The results in Table 1 show that our proposed metrics are much closer aligned with the human judgements than the predictions from the two base- lines. For faithfulness, the RAGAs prediction are in general highly accurate. For answer relevance, the agreement is lower, but this is largely due to the fact that the differences between the two candidate answers are often very subtle. We found context relevance to be the hardest quality dimension to evaluate. In particular, we observed that ChatGPT often struggles with the task of selecting the sen- tences from the context that are crucial, especially for longer contexts.\n",
      "6 Conclusions We have highlighted the need for automated reference-free evaluation of RAG systems. In par- ticular, we have argued the need for an evaluation framework that can assess faithfulness (i.e. is the answer grounded in the retrieved context), answer relevance (i.e. does the answer address the ques- tion) and context relevance (i.e. is the retrieved context sufficiently focused). To support the devel- opment of such a framework, we have introduced WikiEval , a dataset which human judgements of these three different aspects. Finally, we have also described RAGAs, our implementation of the three considered quality aspects. This framework is easy to use and can provide deverlopers of RAG sys- tems with valuable insights, even in the absence of any ground truth. Our evaluation on WikiEval has shown that the predictions from RAGAs are closely aligned with human predictions, especially for faithfulness and answer relevance.References Amos Azaria and Tom M. Mitchell. 2023.\n",
      "The inter- nal state of an LLM knows when its lying .\n",
      "CoRR , abs/2304.13734.\n",
      "Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Si- monyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.\n",
      "2022.\n",
      "Improving language models by retrieving from trillions of tokens . In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Bal- timore, Maryland, USA , volume 162 of Proceedings of Machine Learning Research , pages 2206–2240.\n",
      "PMLR.\n",
      "Sébastien Bubeck, Varun Chandrasekaran, Ronen El- dan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund- berg, et al. 2023. Sparks of artificial general intelli- gence: Early experiments with gpt-4.\n",
      "arXiv preprint arXiv:2303.12712 .\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.\n",
      "BERT: Pre-training of deep bidirectional transformers for language under- standing . In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers) , pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.\n",
      "Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023.\n",
      "Gptscore: Evaluate as you desire .\n",
      "CoRR , abs/2302.04166.\n",
      "Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu- pat, and Mingwei Chang. 2020. Retrieval augmented language model pre-training. In International confer- ence on machine learning , pages 3929–3938. PMLR.\n",
      "Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of halluci- nation in natural language generation.\n",
      "ACM Comput- ing Surveys , 55(12):1–38.\n",
      "Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jack- son Kernion, Shauna Kravec, Liane Lovitt, Ka- mal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. 2022.\n",
      "Language models (mostly) know what they know .\n",
      "CoRR , abs/2207.05221.\n",
      "Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2022.\n",
      "Large language models struggle to learn long-tail knowledge .\n",
      "CoRR , abs/2211.08411.\n",
      "Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020.\n",
      "Generalization through memorization: Nearest neighbor language models . In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net.\n",
      "Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. 2022.\n",
      "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive NLP .\n",
      "CoRR , abs/2212.14024.\n",
      "Kenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n",
      "2019. Latent retrieval for weakly supervised open do- main question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 6086–6096.\n",
      "Patrick S. H. Lewis, Ethan Perez, Aleksandra Pik- tus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020.\n",
      "Retrieval-augmented generation for knowledge-intensive NLP tasks . In Advances in Neu- ral Information Processing Systems 33: Annual Con- ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual .\n",
      "Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023.\n",
      "Halueval: A large- scale hallucination evaluation benchmark for large language models .\n",
      "CoRR , abs/2305.11747.\n",
      "Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran- jape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023.\n",
      "Lost in the middle: How language models use long contexts .\n",
      "Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023.\n",
      "When not to trust language models: Investigating effectiveness of parametric and non-parametric mem- ories . In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers) , pages 9802–9822, Toronto, Canada. Association for Computational Linguistics.\n",
      "Potsawee Manakul, Adian Liusie, and Mark J. F. Gales.\n",
      "2023.\n",
      "Selfcheckgpt: Zero-resource black-box hal- lucination detection for generative large language models .\n",
      "CoRR , abs/2303.08896.\n",
      "Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.\n",
      "Factscore: Fine-grained atomic evaluation of fac- tual precision in long form text generation .\n",
      "CoRR , abs/2305.14251.Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023.\n",
      "In-context retrieval-augmented lan- guage models .\n",
      "CoRR , abs/2302.00083.\n",
      "Adam Roberts, Colin Raffel, and Noam Shazeer. 2020.\n",
      "How much knowledge can you pack into the param- eters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 5418–5426, Online. Association for Computational Linguistics.\n",
      "Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023.\n",
      "REPLUG: retrieval-augmented black-box language models .\n",
      "CoRR , abs/2301.12652.\n",
      "Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxi- ang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023a.\n",
      "Is chatgpt a good NLG evaluator? A preliminary study .\n",
      "CoRR , abs/2303.04048.\n",
      "Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui.\n",
      "2023b.\n",
      "Large language models are not fair evaluators .\n",
      "CoRR , abs/2305.17926.\n",
      "Shufan Wang, Yixiao Song, Andrew Drozdov, Aparna Garimella, Varun Manjunatha, and Mohit Iyyer.\n",
      "2023c.\n",
      "KNN-LM does not improve open-ended text generation .\n",
      "CoRR , abs/2305.14625.\n",
      "Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\n",
      "Bartscore: Evaluating generated text as text genera- tion . In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Informa- tion Processing Systems 2021, NeurIPS 2021, De- cember 6-14, 2021, virtual , pages 27263–27277.\n",
      "Tianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei Fang, Luc Gaitskell, Thomas Hartvigsen, Xixin Wu, Danny Fox, Helen Meng, and James R. Glass. 2023.\n",
      "Interpretable unified language checking .\n",
      "CoRR , abs/2304.03728.\n",
      "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\n",
      "Weinberger, and Yoav Artzi. 2020.\n",
      "Bertscore: Evalu- ating text generation with BERT . In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenRe- view.net.\n",
      "Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris- tian M. Meyer, and Steffen Eger. 2019.\n",
      "MoverScore: Text generation evaluating with contextualized em- beddings and earth mover distance . In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna- tional Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP) , pages 563–578, Hong Kong, China. Association for Computational Lin- guistics.\n",
      "A Examples from WikiEval Tables 2 , 3 and 4 show examples from the WikiEval dataset, focusing in particular on answers with high and low faithfulness (Table 2 ), high and low answer relevance (Table 3 ), and high and low context rele- vance (Table 4 ).Question Context Answer Who directed the film Op- penheimer and who stars as J. Robert Oppenheimer in the film? Oppenheimer is a 2023 biographical thriller film written and directed by Christopher Nolan. Based on the 2005 biography American Prometheus by Kai Bird and Mar- tin J. Sherwin, the film chronicles the life of J. Robert Oppenheimer, a theoretical physicist who was pivotal in developing the first nuclear weapons as part of the Man- hattan Project, and thereby ushering in the Atomic Age.\n",
      "Cillian Murphy stars as Oppenheimer, with Emily Blunt as Oppenheimer’s wife Katherine \"Kitty\" Oppenheimer.\n",
      "High Faithfulness : Christopher Nolan directed the film Oppen- heimer. Cillian Murphy stars as J.\n",
      "Robert Oppenheimer in the film.\n",
      "Low Faithfulness : James Cameron directed the film Op- penheimer. Tom Cruise stars as J.\n",
      "Robert Oppenheimer in the film.\n",
      "Table 2: Example from WikiEval, showing answers with high and low faithfulness.\n",
      "Question Answer When is the scheduled launch date and time for the PSLV-C56 mission, and where will it be launched from? High answer relevance : The PSLV-C56 mission is scheduled to be launched on Sunday, 30 July 2023 at 06:30 IST / 01:00 UTC. It will be launched from the Satish Dhawan Space Centre, Sriharikota, Andhra Pradesh, India.\n",
      "Low answer relevance : The scheduled launch date and time for the PSLV-C56 mission have not been provided.The PSLV-C56 mission is an important space mission for India. It aims to launch a satellite into orbit to study weather patterns.\n",
      "Table 3: Example from WikiEval, showing answers with high and low answer relevance.\n",
      "Question Context When was the Chimnabai Clock Tower completed, and who was it named af- ter? High context relevance : The Chimnabai Clock Tower, also known as the Raopura Tower, is a clock tower situated in the Raopura area of Vadodara, Gujarat, India. It was completed in 1896 and named in memory of Chimnabai I (1864–1885), a queen and the first wife of Sayajirao Gaekwad III of Baroda State.\n",
      "Low context relevance : The Chimnabai Clock Tower, also known as the Raopura Tower, is a clock tower situated in the Raopura area of Vadodara, Gujarat, India. It was completed in 1896 and named in memory of Chimnabai I (1864–1885), a queen and the first wife of Sayajirao Gaekwad III of Baroda State. It was built in Indo-Saracenic architecture style.\n",
      "History. Chimnabai Clock Tower was built in 1896. The tower was named after Chimnabai I (1864–1885), a queen and the first wife of Sayajirao Gaekwad III of Baroda State. It was inaugurated by Mir Kamaluddin Hussainkhan, the last Nawab of Baroda. During the rule of Gaekwad, it was a stoppage for horse drawn trams. The clock tower was erected at the cost of 25,000 (equivalent to 9.2 million or USD 120,000 in 2023).\n",
      "Table 4: Example from WikiEval, showing answers with high and low context relevance.\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "\n",
    "def clean_extracted_text(text):\n",
    "    import re  # Import regular expressions library\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    last_line_was_heading = False  # To track consecutive heading lines\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # Handle consecutive heading lines\n",
    "        current_line_is_heading = line.startswith(\"#\")\n",
    "        if current_line_is_heading and last_line_was_heading:\n",
    "           line = line.replace('#', '')  # Remove all '#' symbols from the line\n",
    "        last_line_was_heading = current_line_is_heading\n",
    "\n",
    "        # Use a regular expression to match lines starting with '#' followed by one or several spaces and 'CHAPITRE'\n",
    "        if re.match(r'^#\\s+CHAPITRE', line):\n",
    "            line = \"\\n\\n\" + line  # Prepend two line breaks\n",
    "\n",
    "        # Decide on line breaks based on punctuation and specific cases\n",
    "        if line.endswith('.'):\n",
    "            # End of sentence or apostrophe, allow for normal line break\n",
    "            cleaned_lines.append(line + \"\\n\")\n",
    "        else:\n",
    "            # No line break for continuing sentences or lines ending with \"l'\"\n",
    "            cleaned_lines.append(line + \" \")\n",
    "\n",
    "    return ''.join(cleaned_lines).strip()\n",
    "\n",
    "\n",
    "def extract_and_clean_text(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "\n",
    "    for page in doc:\n",
    "        text = \"\"\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        for block in blocks:\n",
    "            if \"lines\" in block:\n",
    "                for line in block[\"lines\"]:\n",
    "                    for span in line[\"spans\"]:\n",
    "                        # Check for bold and italic\n",
    "                        is_bold = span['flags'] & 16  # Bold flag\n",
    "                        is_italic = span['flags'] & 2  # Italic flag\n",
    "                        # if span['size'] == 11.0 and is_bold and is_italic:\n",
    "                        #     text += \"## \" + span['text'] + \"\\n\"  # Consider as subtitle\n",
    "                        if span['size'] >= 14.0:  # Title\n",
    "                            text += \"# \" + span['text'] + \"\\n\"\n",
    "                        # elif span['size'] >= 11.0:  # Subtitle\n",
    "                        #     text += \"## \" + span['text'] + \"\\n\"\n",
    "                        else:\n",
    "                            text += span['text'] + \"\\n\"\n",
    "        # Clean the extracted text before adding it to the full_text\n",
    "        cleaned_text = clean_extracted_text(text)\n",
    "        full_text += cleaned_text\n",
    "\n",
    "    return full_text\n",
    "\n",
    "\n",
    "# Specify the path to your PDF here\n",
    "pdf_path = \"hackaton-mistral-studai/data/RAGAS_09_2023.pdf\"\n",
    "text = extract_and_clean_text(pdf_path)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({unstructured.documents.elements.NarrativeText: 59,\n",
       "         unstructured.documents.elements.ListItem: 28,\n",
       "         unstructured.documents.elements.Title: 12,\n",
       "         unstructured.documents.elements.Text: 5,\n",
       "         unstructured.documents.elements.Table: 4,\n",
       "         unstructured.documents.elements.Formula: 2,\n",
       "         unstructured.documents.elements.Header: 1})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "from collections import Counter\n",
    "\n",
    "# unstructured = os.path.join(\"hackaton-mistral-studai/data/\", \"RAGAS_09_2023.pdf\")\n",
    "elements = partition_pdf(\"hackaton-mistral-studai/data/RAGAS_09_2023.pdf\", strategy=\"hi_res\")\n",
    "display(Counter(type(element) for element in elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RAGAS: Automated Evaluation of Retrieval Augmented Generation',\n",
       " 'Abstract',\n",
       " 'Introduction',\n",
       " '2 Related Work',\n",
       " '3 Evaluation Strategies',\n",
       " '4 The WikiEval Dataset',\n",
       " '5 Experiments',\n",
       " '6 Conclusions',\n",
       " 'References',\n",
       " 'A Examples from WikiEval',\n",
       " 'Question',\n",
       " 'Context']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unstructured\n",
    "# for i in [(type(element), element.text) for element in elements[:] if type(element)==unstructured.documents.elements.Title]:\n",
    "#     print(i)\n",
    "\n",
    "# Create tuples (type(element), element.text) ensuring the elements are Title objects\n",
    "elements_tuples = [(type(element), element.text) for element in elements if isinstance(element, unstructured.documents.elements.Title)]\n",
    "\n",
    "# Extract and print the second column of each tuple\n",
    "chapters = [element[1] for element in elements_tuples]\n",
    "chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chapter: RAGAS: Automated Evaluation of Retrieval Augmented Generation\\nContent: Shahul Es † , Jithin James † , Luis Espinosa-Anke ∗♢ , Steven Schockaert ∗ † Exploding Gradients ∗ CardiffNLP, Cardiff University, United Kingdom ♢ AMPLYFI, United Kingdom shahules786@gmail.com,jamesjithin97@gmail.com {espinosa-ankel,schockaerts1}@cardiff.ac.uk',\n",
       " 'Chapter: Abstract\\nContent: We introduce RAGA S ( R etrieval A ugmented G eneration As sessment), a framework for reference-free evaluation of Retrieval Aug- mented Generation (RAG) pipelines.\\nRAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natu- ral language layer between a user and textual databases, reducing the risk of hallucinations.\\nEvaluating RAG architectures is, however, chal- lenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the gener- ation itself. With RAGA S , we put forward a suite of metrics which can be used to evaluate these different dimensions without having to rely on ground truth human annotations . We posit that such a framework can crucially con- tribute to faster evaluation cycles of RAG archi- tectures, which is especially important given the fast adoption of LLMs.\\n1',\n",
       " 'Chapter: Introduction\\nContent: Language Models (LMs) capture a vast amount of knowledge about the world, which allows them to answer questions without accessing any exter- nal sources. This idea of LMs as repositories of knowledge emerged shortly after the introduction of BERT ( Devlin et al.\\n, 2019 ) and became more firmly established with the introduction of ever larger LMs ( Roberts et al.\\n, 2020 ). While the most recent Large Language Models (LLMs) capture enough knowledge to rival human performance across a wide variety of question answering bench- marks ( Bubeck et al.\\n, 2023 ), the idea of using LLMs as knowledge bases still has two fundamen- tal limitations. First, LLMs are not able to answer questions about events that have happened after they were trained. Second, even the largest models struggle to memorise knowledge that is only rarely mentioned in the training corpus ( Kandpal et al.\\n, 2022 ; Mallen et al.\\n, 2023 ). The standard solution to these issues is to rely on Retrieval Augmented Generation (RAG) ( Lee et al.\\n, 2019 ; Lewis et al.\\n, 2020 ; Guu et al.\\n, 2020 ). Answering a question then essentially involves retrieving relevant pas- sages from a corpus and feeding these passages, along with the original question, to the LM. While initial approaches relied on specialised LMs for retrieval-augmented language modelling ( Khandel- wal et al.\\n, 2020 ; Borgeaud et al.\\n, 2022 ), recent work has suggested that simply adding retrieved docu- ments to the input of a standard LM can also work well ( Khattab et al.\\n, 2022 ; Ram et al.\\n, 2023 ; Shi et al.\\n, 2023 ), thus making it possible to use retrieval- augmented strategies in combination with LLMs that are only available through APIs.\\nWhile the usefulness of retrieval-augmented strategies is clear, their implementation requires a significant amount of tuning, as the overall per- formance will be affected by the retrieval model, the considered corpus, the LM, or the prompt for- mulation, among others. Automated evaluation of retrieval-augmented systems is thus paramount. In practice, RAG systems are often evaluated in terms of the language modelling task itself, i.e. by mea- suring perplexity on some reference corpus. How- ever, such evaluations are not always predictive of downstream performance ( Wang et al.\\n, 2023c ).\\nMoreover, this evaluation strategy relies on the LM probabilities, which are not accessible for some closed models (e.g. ChatGPT and GPT-4). Ques- tion answering is another common evaluation task, but usually only datasets with short extractive an- swers are considered, which may not be represen- tative of how the system will be used.\\nTo address these issues, in this paper we present RAGA S 1 , a framework for the automated assess- 1 RAGA S is available at https://github.com/ explodinggradients/ragas .\\n# arXiv:2309.15217v1  [cs.CL]  26 Sep 2023ment of retrieval augmented generation systems.\\nWe focus on settings where reference answers may not be available, and where we want to estimate different proxies for correctness, in addition to the usefulness of the retrieved passages. The RAGA S framework provides an integration with both llama- index and Langchain , the most widely used frame- works for building RAG solutions, thus enabling developers to easily integrate RAGA S into their standard workflow.',\n",
       " 'Chapter: 2 Related Work\\nContent: Estimating faithfulness using LLMs The prob- lem of detecting hallucinations in LLM generated responses has been extensively studied ( Ji et al.\\n, 2023 ). Several authors have suggested the idea of predicting factuality using a few-shot prompt- ing strategy ( Zhang et al.\\n, 2023 ). Recent analy- ses, however, suggest that existing models struggle with detecting hallucination when using standard prompting strategies ( Li et al.\\n, 2023 ; Azaria and Mitchell , 2023 ). Other approaches rely on linking the generated responses to facts from an external knowledge base ( Min et al.\\n, 2023 ), but this is not always possible.\\nYet another strategy is to inspect the probabili- ties assigned to individual tokens, where we would expect the model to be less confident in halluci- nated answers than in factual ones. For instance, BARTScore ( Yuan et al.\\n, 2021 ) estimates factuality by looking at the conditional probability of the gen- erated text given the input.\\nKadavath et al.\\n( 2022 ) use a variation of this idea. Starting from the ob- servation that LLMs provide well-calibrated proba- bilities when answering multiple-choice questions, they essentially convert the problem of validating model generated answers into a multiple-choice question which asks whether the answer is true or false. Rather than looking at the output probabil- ities, Azaria and Mitchell ( 2023 ) propose to train a supervised classifier on the weights from one of the hidden layers of the LLM, to predict whether a given statement is true or not. While the approach performs well, the need to access the hidden states of the model makes it unsuitable for systems that access LLMs through an API.\\nFor models that do not provide access to token probabilities, such as ChatGPT and GPT-4, differ- ent methods are needed. SelfCheckGPT ( Manakul et al.\\n, 2023 ) addresses this problem by instead sam- pling multiple answers. Their core idea is that factual answers are more stable: when an answer is factual, we can expect that different samples will tend to be semantically similar, whereas this is less likely to be the case for hallucinated answers.\\nAutomated evaluation of text generation systems LLMs have also been leveraged to automatically evaluate other aspects of generated text fragments, beyond factuality. For instance, GPTScore ( Fu et al.\\n, 2023 ) uses a prompt that specifies the consid- ered aspect (e.g. fluency) and then scores passages based on the average probability of the generated tokens, according to a given autoregressive LM.\\nThis idea of using prompts was previously also considered by Yuan et al.\\n( 2021 ), although they used a smaller fine-tuned LM (i.e. BART) and did not observe a clear benefit from using prompts. An- other approach directly asks ChatGPT to evaluate a particular aspect of the given answer by provid- ing a score between 0 and 100, or by providing a rating on a 5-star scale ( Wang et al.\\n, 2023a ). Re- markably, strong results can be obtained in this way, although it comes with the limitation of being sensitive to the design of the prompt. Rather than scoring individual answers, some authors have also focused on using an LLM to select the best answer among a number of candidates ( Wang et al.\\n, 2023b ), typically to compare the performance of different LLMs. However, care is needed with this approach, as the order in which the answers is presented can influence the result ( Wang et al.\\n, 2023b ).\\nIn terms of how ground truth answers or, more generally, generations, have been typically used in the literature, most approaches have relied on the availability of one or more reference answers.\\nFor instance, BERTScore ( Zhang et al.\\n, 2020 ) and MoverScore ( Zhao et al.\\n, 2019 ) use contex- tualised embeddings, produced by a pre-trained BERT model, to compare the similarity between the generated answer and the reference answers.\\nBARTScore ( Yuan et al.\\n, 2021 ) similarly uses refer- ence answers to compute aspects such as precision (estimated as the probability of generating the gen- erated answer given the reference) and recall (esti- mated as the probability of generating the reference given the generated answer).',\n",
       " 'Chapter: 3 Evaluation Strategies\\nContent: We consider a standard RAG setting, where given a question q , the system first retrieves some context c ( q ) and then uses the retrieved context to generate an answer a s ( q ) . When building a RAG system,we usually do not have access to human-annotated datasets or reference answers. We therefore fo- cus on metrics that are fully self-contained and reference-free. We focus in particular three quality aspects, which we argue are of central importance.\\nFirst, Faithfulness refers to the idea that the an- swer should be grounded in the given context. This is important to avoid hallucinations, and to ensure that the retrieved context can act as a justification for the generated answer. Indeed, RAG systems are often used in applications where the factual con- sistency of the generated text w.r.t. the grounded sources is highly important, e.g. in domains such as law, where information is constantly evolving. Sec- ond, Answer Relevance refers to the idea that the generated answer should address the actual ques- tion that was provided. Finally,',\n",
       " 'Chapter: 4 The WikiEval Dataset\\nContent: Relevance refers to the idea that the retrieved context should be focused, containing as little irrelevant informa- tion as possible. This is important given the cost associated with feeding long context passages to LLMs. Moreover, when context passages are too long, LLMs are often less effective in exploiting that context, especially for information that is pro- vided in the middle of the context passage ( Liu et al.\\n, 2023 ).\\nWe now explain how these three quality aspects can be measured in a fully automated way, by prompting an LLM. In our implementation and experiments, all prompts are evaluated using the gpt-3.5-turbo-16k model, which is available through the OpenAI API 2 .\\nFaithfulness We say that the answer a s ( q ) is faithful to the context c ( q ) if the claims that are made in the answer can be inferred from the con- text. To estimate faithfulness, we first use an LLM to extract a set of statements, S ( a s ( q )) . The aim of this step is to decompose longer sentences into shorter and more focused assertions. We use the following prompt for this step 3 : Given a question and answer, create one or more statements from each sentence in the given answer.\\nquestion: [question] answer: [answer] where [question] and [answer] refer to the given question and answer. For each statement s i 2 https://platform.openai.com 3 To help clarify the task, we include a demonstration as part of the prompt. This demonstration is not explicitly shown in the listing of the prompts throughout this paper.\\nin S , the LLM determines if s i can be inferred from c ( q ) using a verification function v ( s i , c ( q )) . This verification step is carried out using the following prompt: Consider the given context and following statements, then determine whether they are supported by the information present in the context. Provide a brief explana- tion for each statement before arriving at the verdict (Yes/No). Provide a final verdict for each statement in order at the end in the given format. Do not deviate from the specified format.\\nstatement: [statement 1] ...\\nstatement: [statement n ] The final faithfulness score, F , is then computed as F = | V | | S | , where | V | is the number of statements that were supported according to the LLM and | S | is the total number of statements.\\nAnswer relevance We say that the answer a s ( q ) is relevant if it directly addresses the question in an appropriate way. In particular, our assessment of answer relevance does not take into account fac- tuality, but penalises cases where the answer is incomplete or where it contains redundant informa- tion. To estimate answer relevance, for the given answer a s ( q ) , we prompt the LLM to generate n potential questions q i based on a s ( q ) , as follows: Generate a question for the given answer.\\nanswer : [answer] We then obtain embeddings for all questions us- ing the text-embedding-ada-002 model, avail- able from the OpenAI API. For each q i , we cal- culate the similarity sim ( q, q i ) with the original question q , as the cosine between the correspond- ing embeddings. The answer relevance score, AR , for question q is then computed as: AR = 1 n n X i =1 sim ( q, q i ) (1) This metric evaluates how closely the generated answer aligns with the initial question or instruc- tion.',\n",
       " 'Chapter: 5 Experiments\\nContent: relevance The context c ( q ) is consid- ered relevant to the extent that it exclusively con- tains information that is needed to answer the ques- tion. In particular, this metric aims to penalise theinclusion of redundant information. To estimate context relevance, given a question q and its con- text c ( q ) , the LLM extracts a subset of sentences, S ext , from c ( q ) that are crucial to answer q , using the following prompt: Please extract relevant sentences from the provided context that can potentially help answer the following question. If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return the phrase \"Insufficient Information\". While extract- ing candidate sentences you’re not al- lowed to make any changes to sentences from given context.\\nThe context relevance score is then computed as: CR = number of extracted sentences total number of sentences in c ( q ) (2)',\n",
       " 'Chapter: 6 Conclusions\\nContent: To evaluate the proposed framework, we ideally need examples of question-context-answer triples which are annotated with human judgments. We can then verify to what extent our metrics agree with human assessments of faithfulness, answer relevance and context relevance. Since we are not aware of any publicly available datasets that could be used for this purpose, we created a new dataset, which we refer to as WikiEval 4 . To construct the dataset, we first selected 50 Wikipedia pages cov- ering events that have happened since the start of 2022 5 . In selecting these pages, we prioritised those with recent edits. For each of the 50 pages, we then asked ChatGPT to suggest a question that can be answered based on the introductory section of the page, using the following prompt: Your task is to formulate a question from given context satisfying the rules given below: 1. The question should be fully answered from the given context.\\n2. The question should be framed from a part that contains non-trivial informa- tion.\\n3. The answer should not contain any 4 https://huggingface.co/datasets/ explodinggradients/WikiEval 5 That is, beyond the reported training cutoff of the model we used in our experiments.\\nlinks.\\n4. The question should be of moderate difficulty.\\n5. The question must be reasonable and must be understood and responded to by humans.\\n6. Do not use phrases that ’provided con- text’, etc in the question context: We also used ChatGPT to answer the generated question, when given the corresponding introduc- tory section as context, using the following prompt: Answer the question using the informa- tion from the given context.\\nquestion: [question] context: [context] All questions were annotated along the three con- sidered quality dimensions by two annotators. Both annotators were fluent in English and were given clear instructions about the meaning of the three considered quality dimensions. For faithfulness and context relevance, the two annotators agreed in around 95% of cases. For answer relevance, they agreed in around 90% of the cases. Disagreements were resolved after a discussion between the anno- tators.\\nFaithfulness To obtain human judgements about faithfulness, we first used ChatGPT to answer the question without access to any additional context.\\nWe then asked the annotators to judge which of the two answers was the most faithful (i.e. the standard one or the one generated without context), given the question and corresponding Wikipedia page.\\nAnswer relevance We first used ChatGPT to obtain candidate answers with lower answer rel- evance, using the following prompt: Answer the given question in an incom- plete manner.\\nquestion: [question] We then asked human annotators to compare this answer, and indicate which of the two answers had the highest answer relevance.',\n",
       " 'Chapter: References\\nContent: relevance To measure this aspect, we first added additional sentences to the context by scraping back-links to the corresponding Wikipedia page. In this way, we were able to add information to the context that was related but less relevant forFaith.\\nAns. Rel.\\nCont. Rel.\\nRAGAs 0.95 0.78 0.70 GPT Score 0.72 0.52 0.63 GPT Ranking 0.54 0.40 0.52 Table 1: Agreement with human annotators in pairwise comparisons of faithfulness, answer relevance and con- text relevance, using the WikEval dataset (accuracy).\\nanswering the question. For the few pages with- out any back-links, we instead used ChatGPT to complete the given context.',\n",
       " 'Chapter: A Examples from WikiEval\\nContent: Table 1 analyses the agreement between the met- rics proposed in Section 3 and the human assess- ments from the proposed WikiEval dataset. Each WikiEval instance requires the model to compare two answers or two context fragments. We count how often the answer/context preferred by the model (i.e. with highest estimated faithfulness, an- swer relevance, or context relevance) coincides with the answer/context preferred by the human annotators. We report the results in terms of ac- curacy (i.e. the fraction of instances on which the model agrees with the annotators).\\nTo put the results in context, we compare our proposed metrics (shown as RAGAs in Table 1 ) with two baseline methods. For the first method, shown as GPT Score , we ask ChatGPT to assign a score between 0 and 10 for the three quality dimensions.\\nTo this end, we use a prompt that describes the meaning of the quality metric and then asks to score the given answer/context in line with that definition. For instance, for evaluating faithfulness, we used the following prompt: Faithfulness measures the information consistency of the answer against the given context. Any claims that are made in the answer that cannot be deduced from context should be penalized.\\nGiven an answer and context, assign a score for faithfulness in the range 0-10.\\ncontext : [context] answer : [answer] Ties, where the same score is assigned by the LLM to both answer candidates, were broken randomly.\\nThe second baseline, shown as GPT Ranking , in- stead asks ChatGPT to select the preferred answer/- context. In this case, the prompt again includes a definition of the considered quality metric. For instance, for evaluating answer relevance, we used the following prompt: Answer Relevancy measures the degree to which a response directly addresses and is appropriate for a given question.\\nIt penalizes the present of redundant in- formation or incomplete answers given a question. Given an question and answer, rank each answer based on Answer Rele- vancy.\\nquestion : [question] answer 1 : [answer 1] answer 2 : [answer 2] The results in Table 1 show that our proposed metrics are much closer aligned with the human judgements than the predictions from the two base- lines. For faithfulness, the RAGAs prediction are in general highly accurate. For answer relevance, the agreement is lower, but this is largely due to the fact that the differences between the two candidate answers are often very subtle. We found context relevance to be the hardest quality dimension to evaluate. In particular, we observed that ChatGPT often struggles with the task of selecting the sen- tences from the context that are crucial, especially for longer contexts.',\n",
       " 'Chapter: Question\\nContent: We have highlighted the need for automated reference-free evaluation of RAG systems. In par- ticular, we have argued the need for an evaluation framework that can assess faithfulness (i.e. is the answer grounded in the retrieved context), answer relevance (i.e. does the answer address the ques- tion) and context relevance (i.e. is the retrieved context sufficiently focused). To support the devel- opment of such a framework, we have introduced WikiEval , a dataset which human judgements of these three different aspects. Finally, we have also described RAGAs, our implementation of the three considered quality aspects. This framework is easy to use and can provide deverlopers of RAG sys- tems with valuable insights, even in the absence of any ground truth. Our evaluation on WikiEval has shown that the predictions from RAGAs are closely aligned with human predictions, especially for faithfulness and answer relevance.',\n",
       " 'Chapter: Context\\nContent: Amos Azaria and Tom M. Mitchell. 2023.\\nThe inter- nal state of an LLM knows when its lying .\\nCoRR , abs/2304.13734.\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Si- monyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.\\n2022.\\nImproving language models by retrieving from trillions of tokens . In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Bal- timore, Maryland, USA , volume 162 of Proceedings of Machine Learning Research , pages 2206–2240.\\nPMLR.\\nSébastien Bubeck, Varun Chandrasekaran, Ronen El- dan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund- berg, et al. 2023. Sparks of artificial general intelli- gence: Early experiments with gpt-4.\\narXiv preprint arXiv:2303.12712 .\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.\\nBERT: Pre-training of deep bidirectional transformers for language under- standing . In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers) , pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.\\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023.\\nGptscore: Evaluate as you desire .\\nCoRR , abs/2302.04166.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu- pat, and Mingwei Chang. 2020. Retrieval augmented language model pre-training. In International confer- ence on machine learning , pages 3929–3938. PMLR.\\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of halluci- nation in natural language generation.\\nACM Comput- ing Surveys , 55(12):1–38.\\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jack- son Kernion, Shauna Kravec, Liane Lovitt, Ka- mal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. 2022.\\nLanguage models (mostly) know what they know .\\nCoRR , abs/2207.05221.\\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2022.\\nLarge language models struggle to learn long-tail knowledge .\\nCoRR , abs/2211.08411.\\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020.\\nGeneralization through memorization: Nearest neighbor language models . In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net.\\nOmar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. 2022.\\nDemonstrate-search-predict: Composing retrieval and language models for knowledge-intensive NLP .\\nCoRR , abs/2212.14024.\\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\\n2019. Latent retrieval for weakly supervised open do- main question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 6086–6096.\\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik- tus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020.\\nRetrieval-augmented generation for knowledge-intensive NLP tasks . In Advances in Neu- ral Information Processing Systems 33: Annual Con- ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual .\\nJunyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023.\\nHalueval: A large- scale hallucination evaluation benchmark for large language models .\\nCoRR , abs/2305.11747.\\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran- jape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023.\\nLost in the middle: How language models use long contexts .\\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023.\\nWhen not to trust language models: Investigating effectiveness of parametric and non-parametric mem- ories . In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers) , pages 9802–9822, Toronto, Canada. Association for Computational Linguistics.\\nPotsawee Manakul, Adian Liusie, and Mark J. F. Gales.\\n2023.\\nSelfcheckgpt: Zero-resource black-box hal- lucination detection for generative large language models .\\nCoRR , abs/2303.08896.\\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.\\nFactscore: Fine-grained atomic evaluation of fac- tual precision in long form text generation .\\nCoRR , abs/2305.14251.Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023.\\nIn-context retrieval-augmented lan- guage models .\\nCoRR , abs/2302.00083.\\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\\nHow much knowledge can you pack into the param- eters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 5418–5426, Online. Association for Computational Linguistics.\\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023.\\nREPLUG: retrieval-augmented black-box language models .\\nCoRR , abs/2301.12652.\\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxi- ang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023a.\\nIs chatgpt a good NLG evaluator? A preliminary study .\\nCoRR , abs/2303.04048.\\nPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui.\\n2023b.\\nLarge language models are not fair evaluators .\\nCoRR , abs/2305.17926.\\nShufan Wang, Yixiao Song, Andrew Drozdov, Aparna Garimella, Varun Manjunatha, and Mohit Iyyer.\\n2023c.\\nKNN-LM does not improve open-ended text generation .\\nCoRR , abs/2305.14625.\\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\\nBartscore: Evaluating generated text as text genera- tion . In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Informa- tion Processing Systems 2021, NeurIPS 2021, De- cember 6-14, 2021, virtual , pages 27263–27277.\\nTianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei Fang, Luc Gaitskell, Thomas Hartvigsen, Xixin Wu, Danny Fox, Helen Meng, and James R. Glass. 2023.\\nInterpretable unified language checking .\\nCoRR , abs/2304.03728.\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\\nWeinberger, and Yoav Artzi. 2020.\\nBertscore: Evalu- ating text generation with BERT . In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenRe- view.net.\\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris- tian M. Meyer, and Steffen Eger. 2019.\\nMoverScore: Text generation evaluating with contextualized em- beddings and earth mover distance . In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna- tional Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP) , pages 563–578, Hong Kong, China. Association for Computational Lin- guistics.']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a regular expression pattern to match the section titles\n",
    "pattern = '|'.join([re.escape(section) for section in chapters])\n",
    "split_text = re.split(pattern, text)\n",
    "\n",
    "# The split_text list will contain empty strings at the start and end, and the text of sections in between\n",
    "# Remove empty strings from the list\n",
    "split_text = [section for section in split_text if section.strip()]\n",
    "\n",
    "# Create a dictionary with section titles as keys and section texts as values\n",
    "section_texts = {}\n",
    "for i, section in enumerate(chapters):\n",
    "    if i < len(split_text):\n",
    "        section_texts[section] = split_text[i+1].strip()\n",
    "\n",
    "structured_text = []\n",
    "# Print the sections and their corresponding texts\n",
    "for section, content in section_texts.items():\n",
    "    structured_text.append(f\"Chapter: {section}\\nContent: {content}\")\n",
    "    # print(f\"Section: {section}\\nContent: {content}\\n{'-'*40}\")\n",
    "\n",
    "structured_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to transform the array\n",
    "def transform_array(array):\n",
    "    transformed_array = []\n",
    "    \n",
    "    for entry in array:\n",
    "        # Extract chapter title\n",
    "        chapter_title_match = re.match(r'Chapter: ([^\\n]+)', entry)\n",
    "        if not chapter_title_match:\n",
    "            continue\n",
    "        chapter_title = chapter_title_match.group(1)\n",
    "        \n",
    "        # Extract content\n",
    "        content_match = re.search(r'Content: (.+)', entry, re.DOTALL)\n",
    "        if not content_match:\n",
    "            continue\n",
    "        content = content_match.group(1)\n",
    "        \n",
    "        # Split content into sentences\n",
    "        sentences = re.split(r'(?<=\\.) ', content)\n",
    "        \n",
    "        # Prepend chapter title to each sentence and add to transformed array\n",
    "        for sentence in sentences:\n",
    "            transformed_array.append(f'Chapter: {chapter_title}\\n{sentence.strip()}')\n",
    "    \n",
    "    return transformed_array\n",
    "\n",
    "# Transform the array\n",
    "structured_text = transform_array(structured_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Chapter: RAGAS: Automated Evaluation of Retrieval Augmented Generation\\nShahul Es † , Jithin James † , Luis Espinosa-Anke ∗♢ , Steven Schockaert ∗ † Exploding Gradients ∗ CardiffNLP, Cardiff University, United Kingdom ♢ AMPLYFI, United Kingdom shahules786@gmail.com,jamesjithin97@gmail.com {espinosa-ankel,schockaerts1}@cardiff.ac.uk '),\n",
       " Document(page_content='Chapter: Abstract\\nWe introduce RAGA S ( R etrieval A ugmented G eneration As sessment), a framework for reference-free evaluation of Retrieval Aug- mented Generation (RAG) pipelines.\\nRAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natu- ral language layer between a user and textual databases, reducing the risk of hallucinations.\\nEvaluating RAG architectures is, however, chal- lenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the gener- ation itself. '),\n",
       " Document(page_content='Chapter: Abstract\\nWith RAGA S , we put forward a suite of metrics which can be used to evaluate these different dimensions without having to rely on ground truth human annotations . '),\n",
       " Document(page_content='Chapter: Abstract\\nWe posit that such a framework can crucially con- tribute to faster evaluation cycles of RAG archi- tectures, which is especially important given the fast adoption of LLMs.\\n1 '),\n",
       " Document(page_content='Chapter: Introduction\\nLanguage Models (LMs) capture a vast amount of knowledge about the world, which allows them to answer questions without accessing any exter- nal sources. '),\n",
       " Document(page_content='Chapter: Introduction\\nThis idea of LMs as repositories of knowledge emerged shortly after the introduction of BERT ( Devlin et al.\\n, 2019 ) and became more firmly established with the introduction of ever larger LMs ( Roberts et al.\\n, 2020 ). '),\n",
       " Document(page_content='Chapter: Introduction\\nWhile the most recent Large Language Models (LLMs) capture enough knowledge to rival human performance across a wide variety of question answering bench- marks ( Bubeck et al.\\n, 2023 ), the idea of using LLMs as knowledge bases still has two fundamen- tal limitations. '),\n",
       " Document(page_content='Chapter: Introduction\\nFirst, LLMs are not able to answer questions about events that have happened after they were trained. '),\n",
       " Document(page_content='Chapter: Introduction\\nSecond, even the largest models struggle to memorise knowledge that is only rarely mentioned in the training corpus ( Kandpal et al.\\n, 2022 ; Mallen et al.\\n, 2023 ). '),\n",
       " Document(page_content='Chapter: Introduction\\nThe standard solution to these issues is to rely on Retrieval Augmented Generation (RAG) ( Lee et al.\\n, 2019 ; Lewis et al.\\n, 2020 ; Guu et al.\\n, 2020 ). '),\n",
       " Document(page_content='Chapter: Introduction\\nAnswering a question then essentially involves retrieving relevant pas- sages from a corpus and feeding these passages, along with the original question, to the LM. '),\n",
       " Document(page_content='Chapter: Introduction\\nWhile initial approaches relied on specialised LMs for retrieval-augmented language modelling ( Khandel- wal et al.\\n, 2020 ; Borgeaud et al.\\n, 2022 ), recent work has suggested that simply adding retrieved docu- ments to the input of a standard LM can also work well ( Khattab et al.\\n, 2022 ; Ram et al.\\n, 2023 ; Shi et al.\\n, 2023 ), thus making it possible to use retrieval- augmented strategies in combination with LLMs that are only available through APIs.\\nWhile the usefulness of retrieval-augmented strategies is clear, their implementation requires a significant amount of tuning, as the overall per- formance will be affected by the retrieval model, the considered corpus, the LM, or the prompt for- mulation, among others. '),\n",
       " Document(page_content='Chapter: Introduction\\nAutomated evaluation of retrieval-augmented systems is thus paramount.'),\n",
       " Document(page_content='Chapter: Introduction\\nIn practice, RAG systems are often evaluated in terms of the language modelling task itself, i.e. '),\n",
       " Document(page_content='Chapter: Introduction\\nby mea- suring perplexity on some reference corpus.'),\n",
       " Document(page_content='Chapter: Introduction\\nHow- ever, such evaluations are not always predictive of downstream performance ( Wang et al.\\n, 2023c ).\\nMoreover, this evaluation strategy relies on the LM probabilities, which are not accessible for some closed models (e.g. '),\n",
       " Document(page_content='Chapter: Introduction\\nChatGPT and GPT-4).'),\n",
       " Document(page_content='Chapter: Introduction\\nQues- tion answering is another common evaluation task, but usually only datasets with short extractive an- swers are considered, which may not be represen- tative of how the system will be used.\\nTo address these issues, in this paper we present RAGA S 1 , a framework for the automated assess- 1 RAGA S is available at https://github.com/ explodinggradients/ragas .\\n# arXiv:2309.15217v1  [cs.CL]  26 Sep 2023ment of retrieval augmented generation systems.\\nWe focus on settings where reference answers may not be available, and where we want to estimate different proxies for correctness, in addition to the usefulness of the retrieved passages. '),\n",
       " Document(page_content='Chapter: Introduction\\nThe RAGA S framework provides an integration with both llama- index and Langchain , the most widely used frame- works for building RAG solutions, thus enabling developers to easily integrate RAGA S into their standard workflow. '),\n",
       " Document(page_content='Chapter: 2 Related Work\\nEstimating faithfulness using LLMs The prob- lem of detecting hallucinations in LLM generated responses has been extensively studied ( Ji et al.\\n, 2023 ). '),\n",
       " Document(page_content='Chapter: 2 Related Work\\nSeveral authors have suggested the idea of predicting factuality using a few-shot prompt- ing strategy ( Zhang et al.\\n, 2023 ). '),\n",
       " Document(page_content='Chapter: 2 Related Work\\nRecent analy- ses, however, suggest that existing models struggle with detecting hallucination when using standard prompting strategies ( Li et al.\\n, 2023 ; Azaria and Mitchell , 2023 ). '),\n",
       " Document(page_content='Chapter: 2 Related Work\\nOther approaches rely on linking the generated responses to facts from an external knowledge base ( Min et al.\\n, 2023 ), but this is not always possible.\\nYet another strategy is to inspect the probabili- ties assigned to individual tokens, where we would expect the model to be less confident in halluci- nated answers than in factual ones. '),\n",
       " Document(page_content='Chapter: 2 Related Work\\nFor instance, BARTScore ( Yuan et al.\\n, 2021 ) estimates factuality by looking at the conditional probability of the gen- erated text given the input.\\nKadavath et al.\\n( 2022 ) use a variation of this idea. '),\n",
       " Document(page_content='Chapter: 2 Related Work\\nStarting from the ob- servation that LLMs provide well-calibrated proba- bilities when answering multiple-choice questions, they essentially convert the problem of validating model generated answers into a multiple-choice question which asks whether the answer is true or false. '),\n",
       " Document(page_content='Chapter: 2 Related Work\\nRather than looking at the output probabil- ities, Azaria and Mitchell ( 2023 ) propose to train a supervised classifier on the weights from one of the hidden layers of the LLM, to predict whether a given statement is true or not. '),\n",
       " Document(page_content='Chapter: 2 Related Work\\nWhile the approach performs well, the need to access the hidden states of the model makes it unsuitable for systems that access LLMs through an API.\\nFor models that do not provide access to token probabilities, such as ChatGPT and GPT-4, differ- ent methods are needed. '),\n",
       " Document(page_content='Chapter: 2 Related Work\\nSelfCheckGPT ( Manakul et al.\\n, 2023 ) addresses this problem by instead sam- pling multiple answers. '),\n",
       " Document(page_content='Chapter: 2 Related Work\\nTheir core idea is that factual answers are more stable: when an answer is factual, we can expect that different samples will tend to be semantically similar, whereas this is less likely to be the case for hallucinated answers.\\nAutomated evaluation of text generation systems LLMs have also been leveraged to automatically evaluate other aspects of generated text fragments, beyond factuality. '),\n",
       " Document(page_content='Chapter: 2 Related Work\\nFor instance, GPTScore ( Fu et al.\\n, 2023 ) uses a prompt that specifies the consid- ered aspect (e.g. '),\n",
       " Document(page_content='Chapter: 2 Related Work\\nfluency) and then scores passages based on the average probability of the generated tokens, according to a given autoregressive LM.\\nThis idea of using prompts was previously also considered by Yuan et al.\\n( 2021 ), although they used a smaller fine-tuned LM (i.e. '),\n",
       " Document(page_content='Chapter: 2 Related Work\\nBART) and did not observe a clear benefit from using prompts.'),\n",
       " Document(page_content='Chapter: 2 Related Work\\nAn- other approach directly asks ChatGPT to evaluate a particular aspect of the given answer by provid- ing a score between 0 and 100, or by providing a rating on a 5-star scale ( Wang et al.\\n, 2023a ). '),\n",
       " Document(page_content='Chapter: 2 Related Work\\nRe- markably, strong results can be obtained in this way, although it comes with the limitation of being sensitive to the design of the prompt. '),\n",
       " Document(page_content='Chapter: 2 Related Work\\nRather than scoring individual answers, some authors have also focused on using an LLM to select the best answer among a number of candidates ( Wang et al.\\n, 2023b ), typically to compare the performance of different LLMs. '),\n",
       " Document(page_content='Chapter: 2 Related Work\\nHowever, care is needed with this approach, as the order in which the answers is presented can influence the result ( Wang et al.\\n, 2023b ).\\nIn terms of how ground truth answers or, more generally, generations, have been typically used in the literature, most approaches have relied on the availability of one or more reference answers.\\nFor instance, BERTScore ( Zhang et al.\\n, 2020 ) and MoverScore ( Zhao et al.\\n, 2019 ) use contex- tualised embeddings, produced by a pre-trained BERT model, to compare the similarity between the generated answer and the reference answers.\\nBARTScore ( Yuan et al.\\n, 2021 ) similarly uses refer- ence answers to compute aspects such as precision (estimated as the probability of generating the gen- erated answer given the reference) and recall (esti- mated as the probability of generating the reference given the generated answer). '),\n",
       " Document(page_content='Chapter: 3 Evaluation Strategies\\nWe consider a standard RAG setting, where given a question q , the system first retrieves some context c ( q ) and then uses the retrieved context to generate an answer a s ( q ) . '),\n",
       " Document(page_content='Chapter: 3 Evaluation Strategies\\nWhen building a RAG system,we usually do not have access to human-annotated datasets or reference answers. '),\n",
       " Document(page_content='Chapter: 3 Evaluation Strategies\\nWe therefore fo- cus on metrics that are fully self-contained and reference-free. '),\n",
       " Document(page_content='Chapter: 3 Evaluation Strategies\\nWe focus in particular three quality aspects, which we argue are of central importance.\\nFirst, Faithfulness refers to the idea that the an- swer should be grounded in the given context. '),\n",
       " Document(page_content='Chapter: 3 Evaluation Strategies\\nThis is important to avoid hallucinations, and to ensure that the retrieved context can act as a justification for the generated answer. '),\n",
       " Document(page_content='Chapter: 3 Evaluation Strategies\\nIndeed, RAG systems are often used in applications where the factual con- sistency of the generated text w.r.t. '),\n",
       " Document(page_content='Chapter: 3 Evaluation Strategies\\nthe grounded sources is highly important, e.g.'),\n",
       " Document(page_content='Chapter: 3 Evaluation Strategies\\nin domains such as law, where information is constantly evolving.'),\n",
       " Document(page_content='Chapter: 3 Evaluation Strategies\\nSec- ond, Answer Relevance refers to the idea that the generated answer should address the actual ques- tion that was provided. '),\n",
       " Document(page_content='Chapter: 3 Evaluation Strategies\\nFinally,'),\n",
       " Document(page_content='Chapter: 4 The WikiEval Dataset\\nRelevance refers to the idea that the retrieved context should be focused, containing as little irrelevant informa- tion as possible. '),\n",
       " Document(page_content='Chapter: 4 The WikiEval Dataset\\nThis is important given the cost associated with feeding long context passages to LLMs. '),\n",
       " Document(page_content='Chapter: 4 The WikiEval Dataset\\nMoreover, when context passages are too long, LLMs are often less effective in exploiting that context, especially for information that is pro- vided in the middle of the context passage ( Liu et al.\\n, 2023 ).\\nWe now explain how these three quality aspects can be measured in a fully automated way, by prompting an LLM. '),\n",
       " Document(page_content='Chapter: 4 The WikiEval Dataset\\nIn our implementation and experiments, all prompts are evaluated using the gpt-3.5-turbo-16k model, which is available through the OpenAI API 2 .\\nFaithfulness We say that the answer a s ( q ) is faithful to the context c ( q ) if the claims that are made in the answer can be inferred from the con- text. '),\n",
       " Document(page_content='Chapter: 4 The WikiEval Dataset\\nTo estimate faithfulness, we first use an LLM to extract a set of statements, S ( a s ( q )) . '),\n",
       " Document(page_content='Chapter: 4 The WikiEval Dataset\\nThe aim of this step is to decompose longer sentences into shorter and more focused assertions. '),\n",
       " Document(page_content='Chapter: 4 The WikiEval Dataset\\nWe use the following prompt for this step 3 : Given a question and answer, create one or more statements from each sentence in the given answer.\\nquestion: [question] answer: [answer] where [question] and [answer] refer to the given question and answer. '),\n",
       " Document(page_content='Chapter: 4 The WikiEval Dataset\\nFor each statement s i 2 https://platform.openai.com 3 To help clarify the task, we include a demonstration as part of the prompt. '),\n",
       " Document(page_content='Chapter: 4 The WikiEval Dataset\\nThis demonstration is not explicitly shown in the listing of the prompts throughout this paper.\\nin S , the LLM determines if s i can be inferred from c ( q ) using a verification function v ( s i , c ( q )) . '),\n",
       " Document(page_content='Chapter: 4 The WikiEval Dataset\\nThis verification step is carried out using the following prompt: Consider the given context and following statements, then determine whether they are supported by the information present in the context. '),\n",
       " Document(page_content='Chapter: 4 The WikiEval Dataset\\nProvide a brief explana- tion for each statement before arriving at the verdict (Yes/No). '),\n",
       " Document(page_content='Chapter: 4 The WikiEval Dataset\\nProvide a final verdict for each statement in order at the end in the given format. '),\n",
       " Document(page_content='Chapter: 4 The WikiEval Dataset\\nDo not deviate from the specified format.\\nstatement: [statement 1] ...\\nstatement: [statement n ] The final faithfulness score, F , is then computed as F = | V | | S | , where | V | is the number of statements that were supported according to the LLM and | S | is the total number of statements.\\nAnswer relevance We say that the answer a s ( q ) is relevant if it directly addresses the question in an appropriate way. '),\n",
       " Document(page_content='Chapter: 4 The WikiEval Dataset\\nIn particular, our assessment of answer relevance does not take into account fac- tuality, but penalises cases where the answer is incomplete or where it contains redundant informa- tion. '),\n",
       " Document(page_content='Chapter: 4 The WikiEval Dataset\\nTo estimate answer relevance, for the given answer a s ( q ) , we prompt the LLM to generate n potential questions q i based on a s ( q ) , as follows: Generate a question for the given answer.\\nanswer : [answer] We then obtain embeddings for all questions us- ing the text-embedding-ada-002 model, avail- able from the OpenAI API. '),\n",
       " Document(page_content='Chapter: 4 The WikiEval Dataset\\nFor each q i , we cal- culate the similarity sim ( q, q i ) with the original question q , as the cosine between the correspond- ing embeddings. '),\n",
       " Document(page_content='Chapter: 4 The WikiEval Dataset\\nThe answer relevance score, AR , for question q is then computed as: AR = 1 n n X i =1 sim ( q, q i ) (1) This metric evaluates how closely the generated answer aligns with the initial question or instruc- tion. '),\n",
       " Document(page_content='Chapter: 5 Experiments\\nrelevance The context c ( q ) is consid- ered relevant to the extent that it exclusively con- tains information that is needed to answer the ques- tion. '),\n",
       " Document(page_content='Chapter: 5 Experiments\\nIn particular, this metric aims to penalise theinclusion of redundant information. '),\n",
       " Document(page_content='Chapter: 5 Experiments\\nTo estimate context relevance, given a question q and its con- text c ( q ) , the LLM extracts a subset of sentences, S ext , from c ( q ) that are crucial to answer q , using the following prompt: Please extract relevant sentences from the provided context that can potentially help answer the following question. '),\n",
       " Document(page_content='Chapter: 5 Experiments\\nIf no relevant sentences are found, or if you believe the question cannot be answered from the given context, return the phrase \"Insufficient Information\". '),\n",
       " Document(page_content='Chapter: 5 Experiments\\nWhile extract- ing candidate sentences you’re not al- lowed to make any changes to sentences from given context.\\nThe context relevance score is then computed as: CR = number of extracted sentences total number of sentences in c ( q ) (2) '),\n",
       " Document(page_content='Chapter: 6 Conclusions\\nTo evaluate the proposed framework, we ideally need examples of question-context-answer triples which are annotated with human judgments. '),\n",
       " Document(page_content='Chapter: 6 Conclusions\\nWe can then verify to what extent our metrics agree with human assessments of faithfulness, answer relevance and context relevance. '),\n",
       " Document(page_content='Chapter: 6 Conclusions\\nSince we are not aware of any publicly available datasets that could be used for this purpose, we created a new dataset, which we refer to as WikiEval 4 . '),\n",
       " Document(page_content='Chapter: 6 Conclusions\\nTo construct the dataset, we first selected 50 Wikipedia pages cov- ering events that have happened since the start of 2022 5 . '),\n",
       " Document(page_content='Chapter: 6 Conclusions\\nIn selecting these pages, we prioritised those with recent edits.'),\n",
       " Document(page_content='Chapter: 6 Conclusions\\nFor each of the 50 pages, we then asked ChatGPT to suggest a question that can be answered based on the introductory section of the page, using the following prompt: Your task is to formulate a question from given context satisfying the rules given below: 1. '),\n",
       " Document(page_content='Chapter: 6 Conclusions\\nThe question should be fully answered from the given context.\\n2.'),\n",
       " Document(page_content='Chapter: 6 Conclusions\\nThe question should be framed from a part that contains non-trivial informa- tion.\\n3. '),\n",
       " Document(page_content='Chapter: 6 Conclusions\\nThe answer should not contain any 4 https://huggingface.co/datasets/ explodinggradients/WikiEval 5 That is, beyond the reported training cutoff of the model we used in our experiments.\\nlinks.\\n4. '),\n",
       " Document(page_content='Chapter: 6 Conclusions\\nThe question should be of moderate difficulty.\\n5.'),\n",
       " Document(page_content='Chapter: 6 Conclusions\\nThe question must be reasonable and must be understood and responded to by humans.\\n6. '),\n",
       " Document(page_content='Chapter: 6 Conclusions\\nDo not use phrases that ’provided con- text’, etc in the question context: We also used ChatGPT to answer the generated question, when given the corresponding introduc- tory section as context, using the following prompt: Answer the question using the informa- tion from the given context.\\nquestion: [question] context: [context] All questions were annotated along the three con- sidered quality dimensions by two annotators. '),\n",
       " Document(page_content='Chapter: 6 Conclusions\\nBoth annotators were fluent in English and were given clear instructions about the meaning of the three considered quality dimensions. '),\n",
       " Document(page_content='Chapter: 6 Conclusions\\nFor faithfulness and context relevance, the two annotators agreed in around 95% of cases. '),\n",
       " Document(page_content='Chapter: 6 Conclusions\\nFor answer relevance, they agreed in around 90% of the cases.'),\n",
       " Document(page_content='Chapter: 6 Conclusions\\nDisagreements were resolved after a discussion between the anno- tators.\\nFaithfulness To obtain human judgements about faithfulness, we first used ChatGPT to answer the question without access to any additional context.\\nWe then asked the annotators to judge which of the two answers was the most faithful (i.e. '),\n",
       " Document(page_content='Chapter: 6 Conclusions\\nthe standard one or the one generated without context), given the question and corresponding Wikipedia page.\\nAnswer relevance We first used ChatGPT to obtain candidate answers with lower answer rel- evance, using the following prompt: Answer the given question in an incom- plete manner.\\nquestion: [question] We then asked human annotators to compare this answer, and indicate which of the two answers had the highest answer relevance. '),\n",
       " Document(page_content='Chapter: References\\nrelevance To measure this aspect, we first added additional sentences to the context by scraping back-links to the corresponding Wikipedia page. '),\n",
       " Document(page_content='Chapter: References\\nIn this way, we were able to add information to the context that was related but less relevant forFaith.\\nAns. '),\n",
       " Document(page_content='Chapter: References\\nRel.\\nCont.'),\n",
       " Document(page_content='Chapter: References\\nRel.\\nRAGAs 0.95 0.78 0.70 GPT Score 0.72 0.52 0.63 GPT Ranking 0.54 0.40 0.52 Table 1: Agreement with human annotators in pairwise comparisons of faithfulness, answer relevance and con- text relevance, using the WikEval dataset (accuracy).\\nanswering the question. '),\n",
       " Document(page_content='Chapter: References\\nFor the few pages with- out any back-links, we instead used ChatGPT to complete the given context. '),\n",
       " Document(page_content='Chapter: A Examples from WikiEval\\nTable 1 analyses the agreement between the met- rics proposed in Section 3 and the human assess- ments from the proposed WikiEval dataset. '),\n",
       " Document(page_content='Chapter: A Examples from WikiEval\\nEach WikiEval instance requires the model to compare two answers or two context fragments. '),\n",
       " Document(page_content='Chapter: A Examples from WikiEval\\nWe count how often the answer/context preferred by the model (i.e. '),\n",
       " Document(page_content='Chapter: A Examples from WikiEval\\nwith highest estimated faithfulness, an- swer relevance, or context relevance) coincides with the answer/context preferred by the human annotators. '),\n",
       " Document(page_content='Chapter: A Examples from WikiEval\\nWe report the results in terms of ac- curacy (i.e.'),\n",
       " Document(page_content='Chapter: A Examples from WikiEval\\nthe fraction of instances on which the model agrees with the annotators).\\nTo put the results in context, we compare our proposed metrics (shown as RAGAs in Table 1 ) with two baseline methods. '),\n",
       " Document(page_content='Chapter: A Examples from WikiEval\\nFor the first method, shown as GPT Score , we ask ChatGPT to assign a score between 0 and 10 for the three quality dimensions.\\nTo this end, we use a prompt that describes the meaning of the quality metric and then asks to score the given answer/context in line with that definition. '),\n",
       " Document(page_content='Chapter: A Examples from WikiEval\\nFor instance, for evaluating faithfulness, we used the following prompt: Faithfulness measures the information consistency of the answer against the given context. '),\n",
       " Document(page_content='Chapter: A Examples from WikiEval\\nAny claims that are made in the answer that cannot be deduced from context should be penalized.\\nGiven an answer and context, assign a score for faithfulness in the range 0-10.\\ncontext : [context] answer : [answer] Ties, where the same score is assigned by the LLM to both answer candidates, were broken randomly.\\nThe second baseline, shown as GPT Ranking , in- stead asks ChatGPT to select the preferred answer/- context. '),\n",
       " Document(page_content='Chapter: A Examples from WikiEval\\nIn this case, the prompt again includes a definition of the considered quality metric. '),\n",
       " Document(page_content='Chapter: A Examples from WikiEval\\nFor instance, for evaluating answer relevance, we used the following prompt: Answer Relevancy measures the degree to which a response directly addresses and is appropriate for a given question.\\nIt penalizes the present of redundant in- formation or incomplete answers given a question. '),\n",
       " Document(page_content='Chapter: A Examples from WikiEval\\nGiven an question and answer, rank each answer based on Answer Rele- vancy.\\nquestion : [question] answer 1 : [answer 1] answer 2 : [answer 2] The results in Table 1 show that our proposed metrics are much closer aligned with the human judgements than the predictions from the two base- lines. '),\n",
       " Document(page_content='Chapter: A Examples from WikiEval\\nFor faithfulness, the RAGAs prediction are in general highly accurate. '),\n",
       " Document(page_content='Chapter: A Examples from WikiEval\\nFor answer relevance, the agreement is lower, but this is largely due to the fact that the differences between the two candidate answers are often very subtle. '),\n",
       " Document(page_content='Chapter: A Examples from WikiEval\\nWe found context relevance to be the hardest quality dimension to evaluate. '),\n",
       " Document(page_content='Chapter: A Examples from WikiEval\\nIn particular, we observed that ChatGPT often struggles with the task of selecting the sen- tences from the context that are crucial, especially for longer contexts. '),\n",
       " Document(page_content='Chapter: Question\\nWe have highlighted the need for automated reference-free evaluation of RAG systems. '),\n",
       " Document(page_content='Chapter: Question\\nIn par- ticular, we have argued the need for an evaluation framework that can assess faithfulness (i.e. '),\n",
       " Document(page_content='Chapter: Question\\nis the answer grounded in the retrieved context), answer relevance (i.e.'),\n",
       " Document(page_content='Chapter: Question\\ndoes the answer address the ques- tion) and context relevance (i.e.'),\n",
       " Document(page_content='Chapter: Question\\nis the retrieved context sufficiently focused).'),\n",
       " Document(page_content='Chapter: Question\\nTo support the devel- opment of such a framework, we have introduced WikiEval , a dataset which human judgements of these three different aspects. '),\n",
       " Document(page_content='Chapter: Question\\nFinally, we have also described RAGAs, our implementation of the three considered quality aspects. '),\n",
       " Document(page_content='Chapter: Question\\nThis framework is easy to use and can provide deverlopers of RAG sys- tems with valuable insights, even in the absence of any ground truth. '),\n",
       " Document(page_content='Chapter: Question\\nOur evaluation on WikiEval has shown that the predictions from RAGAs are closely aligned with human predictions, especially for faithfulness and answer relevance. '),\n",
       " Document(page_content='Chapter: Context\\nAmos Azaria and Tom M. Chapter: Context\\nMitchell.'),\n",
       " Document(page_content='Chapter: Context\\n2023.\\nThe inter- nal state of an LLM knows when its lying .\\nCoRR , abs/2304.13734.\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Si- monyan, Jack W. '),\n",
       " Document(page_content='Chapter: Context\\nRae, Erich Elsen, and Laurent Sifre.\\n2022.\\nImproving language models by retrieving from trillions of tokens . '),\n",
       " Document(page_content='Chapter: Context\\nIn International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Bal- timore, Maryland, USA , volume 162 of Proceedings of Machine Learning Research , pages 2206–2240.\\nPMLR.\\nSébastien Bubeck, Varun Chandrasekaran, Ronen El- dan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund- berg, et al. '),\n",
       " Document(page_content='Chapter: Context\\n2023.'),\n",
       " Document(page_content='Chapter: Context\\nSparks of artificial general intelli- gence: Early experiments with gpt-4.\\narXiv preprint arXiv:2303.12712 .\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. '),\n",
       " Document(page_content='Chapter: Context\\n2019.\\nBERT: Pre-training of deep bidirectional transformers for language under- standing . '),\n",
       " Document(page_content='Chapter: Context\\nIn Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers) , pages 4171–4186, Minneapolis, Minnesota. '),\n",
       " Document(page_content='Chapter: Context\\nAssociation for Computational Linguistics.\\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. '),\n",
       " Document(page_content='Chapter: Context\\n2023.\\nGptscore: Evaluate as you desire .\\nCoRR , abs/2302.04166.\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu- pat, and Mingwei Chang. '),\n",
       " Document(page_content='Chapter: Context\\n2020. Chapter: Context\\nRetrieval augmented language model pre-training.'),\n",
       " Document(page_content='Chapter: Context\\nIn International confer- ence on machine learning , pages 3929–3938.'),\n",
       " Document(page_content='Chapter: Context\\nPMLR.\\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. '),\n",
       " Document(page_content='Chapter: Context\\n2023.'),\n",
       " Document(page_content='Chapter: Context\\nSurvey of halluci- nation in natural language generation.\\nACM Comput- ing Surveys , 55(12):1–38.\\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jack- son Kernion, Shauna Kravec, Liane Lovitt, Ka- mal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. '),\n",
       " Document(page_content='Chapter: Context\\n2022.\\nLanguage models (mostly) know what they know .\\nCoRR , abs/2207.05221.\\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. '),\n",
       " Document(page_content='Chapter: Context\\n2022.\\nLarge language models struggle to learn long-tail knowledge .\\nCoRR , abs/2211.08411.\\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. '),\n",
       " Document(page_content='Chapter: Context\\n2020.\\nGeneralization through memorization: Nearest neighbor language models .'),\n",
       " Document(page_content='Chapter: Context\\nIn 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . '),\n",
       " Document(page_content='Chapter: Context\\nOpenReview.net.\\nOmar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. '),\n",
       " Document(page_content='Chapter: Context\\n2022.\\nDemonstrate-search-predict: Composing retrieval and language models for knowledge-intensive NLP .\\nCoRR , abs/2212.14024.\\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\\n2019. '),\n",
       " Document(page_content='Chapter: Context\\nLatent retrieval for weakly supervised open do- main question answering.'),\n",
       " Document(page_content='Chapter: Context\\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 6086–6096.\\nPatrick S. '),\n",
       " Document(page_content='Chapter: Context\\nH.'),\n",
       " Document(page_content='Chapter: Context\\nLewis, Ethan Perez, Aleksandra Pik- tus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. '),\n",
       " Document(page_content='Chapter: Context\\n2020.\\nRetrieval-augmented generation for knowledge-intensive NLP tasks .'),\n",
       " Document(page_content='Chapter: Context\\nIn Advances in Neu- ral Information Processing Systems 33: Annual Con- ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual .\\nJunyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. '),\n",
       " Document(page_content='Chapter: Context\\n2023.\\nHalueval: A large- scale hallucination evaluation benchmark for large language models .\\nCoRR , abs/2305.11747.\\nNelson F. '),\n",
       " Document(page_content='Chapter: Context\\nLiu, Kevin Lin, John Hewitt, Ashwin Paran- jape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. '),\n",
       " Document(page_content='Chapter: Context\\n2023.\\nLost in the middle: How language models use long contexts .\\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. '),\n",
       " Document(page_content='Chapter: Context\\n2023.\\nWhen not to trust language models: Investigating effectiveness of parametric and non-parametric mem- ories . '),\n",
       " Document(page_content='Chapter: Context\\nIn Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers) , pages 9802–9822, Toronto, Canada. '),\n",
       " Document(page_content='Chapter: Context\\nAssociation for Computational Linguistics.\\nPotsawee Manakul, Adian Liusie, and Mark J. '),\n",
       " Document(page_content='Chapter: Context\\nF.'),\n",
       " Document(page_content='Chapter: Context\\nGales.\\n2023.\\nSelfcheckgpt: Zero-resource black-box hal- lucination detection for generative large language models .\\nCoRR , abs/2303.08896.\\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. '),\n",
       " Document(page_content='Chapter: Context\\n2023.\\nFactscore: Fine-grained atomic evaluation of fac- tual precision in long form text generation .\\nCoRR , abs/2305.14251.Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. '),\n",
       " Document(page_content='Chapter: Context\\n2023.\\nIn-context retrieval-augmented lan- guage models .\\nCoRR , abs/2302.00083.\\nAdam Roberts, Colin Raffel, and Noam Shazeer. '),\n",
       " Document(page_content='Chapter: Context\\n2020.\\nHow much knowledge can you pack into the param- eters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 5418–5426, Online. '),\n",
       " Document(page_content='Chapter: Context\\nAssociation for Computational Linguistics.\\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. '),\n",
       " Document(page_content='Chapter: Context\\n2023.\\nREPLUG: retrieval-augmented black-box language models .\\nCoRR , abs/2301.12652.\\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxi- ang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. '),\n",
       " Document(page_content='Chapter: Context\\n2023a.\\nIs chatgpt a good NLG evaluator? A preliminary study .\\nCoRR , abs/2303.04048.\\nPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui.\\n2023b.\\nLarge language models are not fair evaluators .\\nCoRR , abs/2305.17926.\\nShufan Wang, Yixiao Song, Andrew Drozdov, Aparna Garimella, Varun Manjunatha, and Mohit Iyyer.\\n2023c.\\nKNN-LM does not improve open-ended text generation .\\nCoRR , abs/2305.14625.\\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. '),\n",
       " Document(page_content='Chapter: Context\\n2021.\\nBartscore: Evaluating generated text as text genera- tion .'),\n",
       " Document(page_content='Chapter: Context\\nIn Advances in Neural Information Processing Systems 34: Annual Conference on Neural Informa- tion Processing Systems 2021, NeurIPS 2021, De- cember 6-14, 2021, virtual , pages 27263–27277.\\nTianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei Fang, Luc Gaitskell, Thomas Hartvigsen, Xixin Wu, Danny Fox, Helen Meng, and James R. '),\n",
       " Document(page_content='Chapter: Context\\nGlass.'),\n",
       " Document(page_content='Chapter: Context\\n2023.\\nInterpretable unified language checking .\\nCoRR , abs/2304.03728.\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\\nWeinberger, and Yoav Artzi. '),\n",
       " Document(page_content='Chapter: Context\\n2020.\\nBertscore: Evalu- ating text generation with BERT .'),\n",
       " Document(page_content='Chapter: Context\\nIn 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . '),\n",
       " Document(page_content='Chapter: Context\\nOpenRe- view.net.\\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris- tian M.'),\n",
       " Document(page_content='Chapter: Context\\nMeyer, and Steffen Eger.'),\n",
       " Document(page_content='Chapter: Context\\n2019.\\nMoverScore: Text generation evaluating with contextualized em- beddings and earth mover distance . '),\n",
       " Document(page_content='Chapter: Context\\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna- tional Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP) , pages 563–578, Hong Kong, China. '),\n",
       " Document(page_content='Chapter: Context\\nAssociation for Computational Lin- guistics.')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Convert list to string with a space separator\n",
    "structured_text = ' '.join(structured_text)\n",
    "\n",
    "# splitter = CharacterTextSplitter(separator=\"Chapter:\", chunk_size=30, chunk_overlap=10)\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20, separators=['Chapter:','\\n\\n'])\n",
    "chunks = splitter.split_text(structured_text)\n",
    "docs = splitter.create_documents(chunks)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manualy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mistralai.client import MistralClient\n",
    "\n",
    "api_key = \"ImsUHfFLA6OjlX6mARbnM1YcDOy7ujsq\"\n",
    "client = MistralClient(api_key=\"ImsUHfFLA6OjlX6mARbnM1YcDOy7ujsq\")\n",
    "\n",
    "def get_text_embedding(txt):\n",
    "    client = MistralClient(api_key=api_key)\n",
    "    embeddings_batch_response = client.embeddings(model=\"mistral-embed\", input=txt)\n",
    "    return embeddings_batch_response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])\n",
    "text_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain + Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maximejabarian/miniconda3/envs/studai_env/lib/python3.11/site-packages/langchain_mistralai/embeddings.py:105: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "attempt to write a readonly database",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m mistral_embeddings \u001b[38;5;241m=\u001b[39m MistralAIEmbeddings(api_key\u001b[38;5;241m=\u001b[39mapi_key)\n\u001b[1;32m      9\u001b[0m mistral_embeddings\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistral-embed\u001b[39m\u001b[38;5;124m\"\u001b[39m  \n\u001b[0;32m---> 10\u001b[0m chroma_db \u001b[38;5;241m=\u001b[39m Chroma\u001b[38;5;241m.\u001b[39mfrom_documents(docs, mistral_embeddings, persist_directory\u001b[38;5;241m=\u001b[39moutput_path)\n",
      "File \u001b[0;32m~/miniconda3/envs/studai_env/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:790\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    788\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    789\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 790\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(\n\u001b[1;32m    791\u001b[0m     texts\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[1;32m    792\u001b[0m     embedding\u001b[38;5;241m=\u001b[39membedding,\n\u001b[1;32m    793\u001b[0m     metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[1;32m    794\u001b[0m     ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[1;32m    795\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39mcollection_name,\n\u001b[1;32m    796\u001b[0m     persist_directory\u001b[38;5;241m=\u001b[39mpersist_directory,\n\u001b[1;32m    797\u001b[0m     client_settings\u001b[38;5;241m=\u001b[39mclient_settings,\n\u001b[1;32m    798\u001b[0m     client\u001b[38;5;241m=\u001b[39mclient,\n\u001b[1;32m    799\u001b[0m     collection_metadata\u001b[38;5;241m=\u001b[39mcollection_metadata,\n\u001b[1;32m    800\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    801\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/studai_env/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:748\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_batches\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m create_batches(\n\u001b[1;32m    743\u001b[0m         api\u001b[38;5;241m=\u001b[39mchroma_collection\u001b[38;5;241m.\u001b[39m_client,\n\u001b[1;32m    744\u001b[0m         ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[1;32m    745\u001b[0m         metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[1;32m    746\u001b[0m         documents\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[1;32m    747\u001b[0m     ):\n\u001b[0;32m--> 748\u001b[0m         chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(\n\u001b[1;32m    749\u001b[0m             texts\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m batch[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m [],\n\u001b[1;32m    750\u001b[0m             metadatas\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m batch[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    751\u001b[0m             ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    752\u001b[0m         )\n\u001b[1;32m    753\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    754\u001b[0m     chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(texts\u001b[38;5;241m=\u001b[39mtexts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, ids\u001b[38;5;241m=\u001b[39mids)\n",
      "File \u001b[0;32m~/miniconda3/envs/studai_env/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:319\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m         embeddings_without_metadatas \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    316\u001b[0m             [embeddings[j] \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m empty_ids] \u001b[38;5;28;01mif\u001b[39;00m embeddings \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    317\u001b[0m         )\n\u001b[1;32m    318\u001b[0m         ids_without_metadatas \u001b[38;5;241m=\u001b[39m [ids[j] \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m empty_ids]\n\u001b[0;32m--> 319\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection\u001b[38;5;241m.\u001b[39mupsert(\n\u001b[1;32m    320\u001b[0m             embeddings\u001b[38;5;241m=\u001b[39membeddings_without_metadatas,\n\u001b[1;32m    321\u001b[0m             documents\u001b[38;5;241m=\u001b[39mtexts_without_metadatas,\n\u001b[1;32m    322\u001b[0m             ids\u001b[38;5;241m=\u001b[39mids_without_metadatas,\n\u001b[1;32m    323\u001b[0m         )\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection\u001b[38;5;241m.\u001b[39mupsert(\n\u001b[1;32m    326\u001b[0m         embeddings\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[1;32m    327\u001b[0m         documents\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[1;32m    328\u001b[0m         ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[1;32m    329\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/studai_env/lib/python3.11/site-packages/chromadb/api/models/Collection.py:487\u001b[0m, in \u001b[0;36mCollection.upsert\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    485\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mimages)\n\u001b[0;32m--> 487\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_upsert(\n\u001b[1;32m    488\u001b[0m     collection_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m    489\u001b[0m     ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[1;32m    490\u001b[0m     embeddings\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[1;32m    491\u001b[0m     metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[1;32m    492\u001b[0m     documents\u001b[38;5;241m=\u001b[39mdocuments,\n\u001b[1;32m    493\u001b[0m     uris\u001b[38;5;241m=\u001b[39muris,\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/studai_env/lib/python3.11/site-packages/chromadb/telemetry/opentelemetry/__init__.py:143\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/studai_env/lib/python3.11/site-packages/chromadb/api/segment.py:474\u001b[0m, in \u001b[0;36mSegmentAPI._upsert\u001b[0;34m(self, collection_id, ids, embeddings, metadatas, documents, uris)\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_embedding_record(coll, r)\n\u001b[1;32m    473\u001b[0m     records_to_submit\u001b[38;5;241m.\u001b[39mappend(r)\n\u001b[0;32m--> 474\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_producer\u001b[38;5;241m.\u001b[39msubmit_embeddings(collection_id, records_to_submit)\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/studai_env/lib/python3.11/site-packages/chromadb/telemetry/opentelemetry/__init__.py:143\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/studai_env/lib/python3.11/site-packages/chromadb/db/mixins/embeddings_queue.py:180\u001b[0m, in \u001b[0;36mSqlEmbeddingsQueue.submit_embeddings\u001b[0;34m(self, collection_id, embeddings)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# The returning clause does not guarantee order, so we need to do reorder\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# the results. https://www.sqlite.org/lang_returning.html\u001b[39;00m\n\u001b[1;32m    179\u001b[0m sql \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m RETURNING seq_id, id\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Pypika doesn't support RETURNING\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m results \u001b[38;5;241m=\u001b[39m cur\u001b[38;5;241m.\u001b[39mexecute(sql, params)\u001b[38;5;241m.\u001b[39mfetchall()\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# Reorder the results\u001b[39;00m\n\u001b[1;32m    182\u001b[0m seq_ids \u001b[38;5;241m=\u001b[39m [cast(SeqId, \u001b[38;5;28;01mNone\u001b[39;00m)] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[1;32m    183\u001b[0m     results\n\u001b[1;32m    184\u001b[0m )  \u001b[38;5;66;03m# Lie to mypy: https://stackoverflow.com/questions/76694215/python-type-casting-when-preallocating-list\u001b[39;00m\n",
      "\u001b[0;31mOperationalError\u001b[0m: attempt to write a readonly database"
     ]
    }
   ],
   "source": [
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "# from langchain_chroma import Chroma\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# vectordb = Chroma.from_documents(documents=chunks_df, embedding=embedding, persist_directory=output_path)\n",
    "\n",
    "output_path = \"hackaton-mistral-studai/data/\"\n",
    "mistral_embeddings = MistralAIEmbeddings(api_key=api_key)\n",
    "mistral_embeddings.model = \"mistral-embed\"  \n",
    "chroma_db = Chroma.from_documents(docs, mistral_embeddings, persist_directory=output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Vector Database \"\"\"\n",
    "import faiss\n",
    "\n",
    "d = text_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(text_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00650787, -0.04208374,  0.06274414, ..., -0.02349854,\n",
       "         0.03463745,  0.02111816]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is RAGAs?\"\n",
    "question_embeddings = np.array([get_text_embedding(question)])\n",
    "question_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18 20]]\n"
     ]
    }
   ],
   "source": [
    "D, I = index.search(question_embeddings, k=2)\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nTo address these issues, in this paper we present RAGA S 1 , a framework for the automated assess- 1 RAGA S is available at https://github.com/ explodinggradients/ragas .', '\\nWe focus on settings where reference answers may not be available, and where we want to estimate different proxies for correctness, in addition to the usefulness of the retrieved passages. The RAGA S framework provides an integration with both llama- index and Langchain , the most widely used frame- works for building RAG solutions, thus enabling developers to easily integrate RAGA S into their standard workflow.']\n"
     ]
    }
   ],
   "source": [
    "retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
    "print(retrieved_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{retrieved_chunk}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "client = MistralClient(api_key=\"ImsUHfFLA6OjlX6mARbnM1YcDOy7ujsq\")\n",
    "\n",
    "def mistral(user_message, model=\"mistral-small-latest\", is_json=False):\n",
    "    messages = [ChatMessage(role=\"user\", content=user_message)]\n",
    "\n",
    "    if is_json:\n",
    "        chat_response = client.chat(\n",
    "            model=model, messages=messages, response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "    else:\n",
    "        chat_response = client.chat(model=model, messages=messages)\n",
    "\n",
    "    return chat_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGA S is a framework for automated assessment, specifically designed for settings where reference answers may not be available. It aims to estimate different proxies for correctness and the usefulness of retrieved passages. The RAGA S framework is available for integration with llama-index and Langchain, which are widely used frameworks for building RAG (Retrieval-Augmented Generation) solutions. You can find RAGA S at this GitHub link: https://github.com/explodinggradients/ragas.\n"
     ]
    }
   ],
   "source": [
    "response = mistral(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Chatbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n  var py_version = '3.4.1'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  var reloading = false;\n  var Bokeh = root.Bokeh;\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      run_callbacks();\n      return null;\n    }\n    if (!reloading) {\n      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {}, 'shim': {}});\n      root._bokeh_is_loading = css_urls.length + 0;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    var existing_stylesheets = []\n    var links = document.getElementsByTagName('link')\n    for (var i = 0; i < links.length; i++) {\n      var link = links[i]\n      if (link.href != null) {\n\texisting_stylesheets.push(link.href)\n      }\n    }\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      if (existing_stylesheets.indexOf(url) !== -1) {\n\ton_load()\n\tcontinue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    var existing_scripts = []\n    var scripts = document.getElementsByTagName('script')\n    for (var i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n\texisting_scripts.push(script.src)\n      }\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      var url = js_exports[name];\n      if (skip.indexOf(url) >= 0 || root[name] != null) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.1.min.js\", \"https://cdn.holoviz.org/panel/1.4.3/dist/panel.min.js\"];\n  var js_modules = [];\n  var js_exports = {};\n  var css_urls = [];\n  var inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n\ttry {\n          inline_js[i].call(root, root.Bokeh);\n\t} catch(e) {\n\t  if (!reloading) {\n\t    throw e;\n\t  }\n\t}\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n\tvar NewBokeh = root.Bokeh;\n\tif (Bokeh.versions === undefined) {\n\t  Bokeh.versions = new Map();\n\t}\n\tif (NewBokeh.version !== Bokeh.version) {\n\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n\t}\n\troot.Bokeh = Bokeh;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n      if (!reloading && !bokeh_loaded) {\n\troot.Bokeh = undefined;\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n\trun_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='017e43a0-eac0-4781-a7fe-c172a126fdcc'>\n",
       "  <div id=\"b10ee6b1-165f-43f2-9193-f4e1f832798e\" data-root-id=\"017e43a0-eac0-4781-a7fe-c172a126fdcc\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"2832549a-e37a-4ab4-95a7-0d72ec759eec\":{\"version\":\"3.4.1\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"017e43a0-eac0-4781-a7fe-c172a126fdcc\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"b5be6c8c-e6dd-44f6-a687-243b1e95f84f\",\"attributes\":{\"plot_id\":\"017e43a0-eac0-4781-a7fe-c172a126fdcc\",\"comm_id\":\"3259a811b7c3468fb3fcb2833a0d9f94\",\"client_comm_id\":\"b7d741870ceb4e38b23959fd8345a468\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"copy_to_clipboard1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":null}]}]}};\n",
       "  var render_items = [{\"docid\":\"2832549a-e37a-4ab4-95a7-0d72ec759eec\",\"roots\":{\"017e43a0-eac0-4781-a7fe-c172a126fdcc\":\"b10ee6b1-165f-43f2-9193-f4e1f832798e\"},\"root_ids\":[\"017e43a0-eac0-4781-a7fe-c172a126fdcc\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  async function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    await Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && id_el.children[0].hasAttribute('data-root-id')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t  for (const child of root_el.children) {\n",
       "            // Ensure JupyterLab does not capture keyboard shortcuts\n",
       "            // see: https://jupyterlab.readthedocs.io/en/4.1.x/extension/notebook.html#keyboard-interaction-model\n",
       "\t    child.setAttribute('data-lm-suppress-shortcuts', 'true')\n",
       "\t  }\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined)\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ]
     },
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "017e43a0-eac0-4781-a7fe-c172a126fdcc"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "from mistralai.client import MistralClient\n",
    "import panel as pn\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mistral(contents, user, chat_interface):\n",
    "    messages = [ChatMessage(role=\"user\", content=contents)]\n",
    "    chat_response = client.chat(\n",
    "        model=\"mistral-large-latest\", \n",
    "        messages=messages)\n",
    "    return chat_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='5712a6f3-4d6b-4925-a2b4-cbc677b0304e'>\n",
       "  <div id=\"c3f776ed-d1bd-4869-b42d-730c8b16e30d\" data-root-id=\"5712a6f3-4d6b-4925-a2b4-cbc677b0304e\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"b8952d1d-4697-4a89-aa0f-c70ebf2c49f8\":{\"version\":\"3.4.1\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.layout.Card\",\"id\":\"5712a6f3-4d6b-4925-a2b4-cbc677b0304e\",\"attributes\":{\"name\":\"Card00144\",\"css_classes\":[\"chat-interface\"],\"styles\":{\"type\":\"map\",\"entries\":[[\"padding\",\"0px\"]]},\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"dd9b8770-149d-4ff3-b55e-dd87ee6b415a\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.4.3/dist/css/loading.css\"}},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"e9bea015-78d1-4fe0-84e1-64c40a0fe0b6\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.4.3/dist/css/listpanel.css\"}},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"96755a2b-0732-48a9-8fb9-3715b5ed2374\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.4.3/dist/bundled/theme/default.css\"}},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"28ba4ebe-bcdc-4446-9b0f-9504f70d0c3f\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.4.3/dist/bundled/theme/native.css\"}},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"08696dcc-657e-496f-a2d8-76f21b6fdfc4\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.4.3/dist/css/chat_interface.css\"}}],\"margin\":5,\"sizing_mode\":\"stretch_both\",\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"Row\",\"id\":\"f803a0fd-870a-4ee1-9c12-154d100a23c9\",\"attributes\":{\"name\":\"Row00143\",\"css_classes\":[\"card-header-row\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"dd9b8770-149d-4ff3-b55e-dd87ee6b415a\"},{\"id\":\"e9bea015-78d1-4fe0-84e1-64c40a0fe0b6\"},{\"id\":\"96755a2b-0732-48a9-8fb9-3715b5ed2374\"},{\"id\":\"28ba4ebe-bcdc-4446-9b0f-9504f70d0c3f\"}],\"margin\":0,\"sizing_mode\":\"stretch_width\",\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"panel.models.markup.HTML\",\"id\":\"68f9ca20-c3c1-43dd-984d-f765c6371cc9\",\"attributes\":{\"css_classes\":[\"chat-feed-title\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"dd9b8770-149d-4ff3-b55e-dd87ee6b415a\"},{\"id\":\"96755a2b-0732-48a9-8fb9-3715b5ed2374\"},{\"id\":\"28ba4ebe-bcdc-4446-9b0f-9504f70d0c3f\"}],\"margin\":[5,0],\"align\":\"start\",\"text\":\"&amp;#8203;\",\"disable_math\":true}}]}},{\"type\":\"object\",\"name\":\"panel.models.feed.Feed\",\"id\":\"4fac9a37-2d52-469d-8dae-8d6632c21c9c\",\"attributes\":{\"name\":\"Feed00141\",\"subscribed_events\":{\"type\":\"set\",\"entries\":[\"scroll_button_click\"]},\"css_classes\":[\"chat-feed-log\",\"scroll-vertical\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"dd9b8770-149d-4ff3-b55e-dd87ee6b415a\"},{\"id\":\"e9bea015-78d1-4fe0-84e1-64c40a0fe0b6\"},{\"id\":\"96755a2b-0732-48a9-8fb9-3715b5ed2374\"},{\"id\":\"28ba4ebe-bcdc-4446-9b0f-9504f70d0c3f\"},{\"id\":\"08696dcc-657e-496f-a2d8-76f21b6fdfc4\"}],\"margin\":0,\"sizing_mode\":\"stretch_width\",\"align\":\"start\",\"auto_scroll_limit\":200,\"scroll_button_threshold\":100,\"view_latest\":true}},{\"type\":\"object\",\"name\":\"Spacer\",\"id\":\"d1578b9d-c145-4558-8391-bf0b3c4a358c\",\"attributes\":{\"name\":\"VSpacer00142\",\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"dd9b8770-149d-4ff3-b55e-dd87ee6b415a\"},{\"id\":\"96755a2b-0732-48a9-8fb9-3715b5ed2374\"},{\"id\":\"28ba4ebe-bcdc-4446-9b0f-9504f70d0c3f\"}],\"margin\":0,\"sizing_mode\":\"stretch_height\",\"align\":\"start\"}},{\"type\":\"object\",\"name\":\"Row\",\"id\":\"d5dc746a-15aa-4b9f-b843-40a9c3b4a371\",\"attributes\":{\"name\":\"Row00147\",\"css_classes\":[\"chat-interface-input-container\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"dd9b8770-149d-4ff3-b55e-dd87ee6b415a\"},{\"id\":\"e9bea015-78d1-4fe0-84e1-64c40a0fe0b6\"},{\"id\":\"96755a2b-0732-48a9-8fb9-3715b5ed2374\"},{\"id\":\"28ba4ebe-bcdc-4446-9b0f-9504f70d0c3f\"},{\"id\":\"08696dcc-657e-496f-a2d8-76f21b6fdfc4\"}],\"margin\":0,\"sizing_mode\":\"stretch_width\",\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"Row\",\"id\":\"17a5d1a5-12f7-487a-a848-343d28a2c727\",\"attributes\":{\"name\":\"Row00164\",\"css_classes\":[\"chat-interface-input-row\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"dd9b8770-149d-4ff3-b55e-dd87ee6b415a\"},{\"id\":\"e9bea015-78d1-4fe0-84e1-64c40a0fe0b6\"},{\"id\":\"96755a2b-0732-48a9-8fb9-3715b5ed2374\"},{\"id\":\"28ba4ebe-bcdc-4446-9b0f-9504f70d0c3f\"},{\"id\":\"08696dcc-657e-496f-a2d8-76f21b6fdfc4\"}],\"margin\":0,\"sizing_mode\":\"stretch_width\",\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"panel.models.chatarea_input.ChatAreaInput\",\"id\":\"8632d8c2-816e-41eb-af3e-65c0ae252eee\",\"attributes\":{\"subscribed_events\":{\"type\":\"set\",\"entries\":[\"chat_message_event\"]},\"css_classes\":[\"chat-interface-input-widget\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"dd9b8770-149d-4ff3-b55e-dd87ee6b415a\"},{\"id\":\"96755a2b-0732-48a9-8fb9-3715b5ed2374\"},{\"id\":\"28ba4ebe-bcdc-4446-9b0f-9504f70d0c3f\"}],\"width\":300,\"margin\":[5,10],\"sizing_mode\":\"stretch_width\",\"align\":\"start\",\"resizable\":\"height\",\"placeholder\":\"Send a message\",\"max_length\":5000,\"rows\":1,\"auto_grow\":true,\"max_rows\":10}},{\"type\":\"object\",\"name\":\"panel.models.widgets.Button\",\"id\":\"5c0b644a-1f2e-4a74-9d88-e2a1fcdef473\",\"attributes\":{\"subscribed_events\":{\"type\":\"set\",\"entries\":[\"button_click\"]},\"css_classes\":[\"solid\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"dd9b8770-149d-4ff3-b55e-dd87ee6b415a\"},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"2410c7c6-1024-4fdd-ae60-2235f91e04a8\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.4.3/dist/css/button.css\"}},{\"id\":\"96755a2b-0732-48a9-8fb9-3715b5ed2374\"},{\"id\":\"28ba4ebe-bcdc-4446-9b0f-9504f70d0c3f\"}],\"max_width\":90,\"max_height\":50,\"margin\":[0,5,0,0],\"sizing_mode\":\"stretch_width\",\"align\":\"center\",\"label\":\"Send\",\"icon\":{\"type\":\"object\",\"name\":\"TablerIcon\",\"id\":\"60591615-a10e-45c0-bb15-278f6290faad\",\"attributes\":{\"icon_name\":\"send\"}}}},{\"type\":\"object\",\"name\":\"panel.models.widgets.Button\",\"id\":\"d58462f6-a493-44bd-be4c-9b0864a190be\",\"attributes\":{\"subscribed_events\":{\"type\":\"set\",\"entries\":[\"button_click\"]},\"visible\":false,\"css_classes\":[\"solid\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"dd9b8770-149d-4ff3-b55e-dd87ee6b415a\"},{\"id\":\"2410c7c6-1024-4fdd-ae60-2235f91e04a8\"},{\"id\":\"96755a2b-0732-48a9-8fb9-3715b5ed2374\"},{\"id\":\"28ba4ebe-bcdc-4446-9b0f-9504f70d0c3f\"}],\"max_width\":90,\"max_height\":50,\"margin\":[0,5,0,0],\"sizing_mode\":\"stretch_width\",\"align\":\"center\",\"label\":\"Stop\",\"icon\":{\"type\":\"object\",\"name\":\"TablerIcon\",\"id\":\"5cc3abc9-bb52-4da1-85e9-5ad5bba844ad\",\"attributes\":{\"icon_name\":\"player-stop\"}}}},{\"type\":\"object\",\"name\":\"panel.models.widgets.Button\",\"id\":\"87f97382-32de-43f7-9592-659b2a9957c8\",\"attributes\":{\"subscribed_events\":{\"type\":\"set\",\"entries\":[\"button_click\"]},\"css_classes\":[\"solid\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"dd9b8770-149d-4ff3-b55e-dd87ee6b415a\"},{\"id\":\"2410c7c6-1024-4fdd-ae60-2235f91e04a8\"},{\"id\":\"96755a2b-0732-48a9-8fb9-3715b5ed2374\"},{\"id\":\"28ba4ebe-bcdc-4446-9b0f-9504f70d0c3f\"}],\"max_width\":90,\"max_height\":50,\"margin\":[0,5,0,0],\"sizing_mode\":\"stretch_width\",\"align\":\"center\",\"label\":\"Rerun\",\"icon\":{\"type\":\"object\",\"name\":\"TablerIcon\",\"id\":\"67960f6c-15ca-4ed0-92ec-e992c7c8172a\",\"attributes\":{\"icon_name\":\"repeat\"}}}},{\"type\":\"object\",\"name\":\"panel.models.widgets.Button\",\"id\":\"80afe1d6-0758-483a-a6cd-6efafeb2b34a\",\"attributes\":{\"subscribed_events\":{\"type\":\"set\",\"entries\":[\"button_click\"]},\"css_classes\":[\"solid\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"dd9b8770-149d-4ff3-b55e-dd87ee6b415a\"},{\"id\":\"2410c7c6-1024-4fdd-ae60-2235f91e04a8\"},{\"id\":\"96755a2b-0732-48a9-8fb9-3715b5ed2374\"},{\"id\":\"28ba4ebe-bcdc-4446-9b0f-9504f70d0c3f\"}],\"max_width\":90,\"max_height\":50,\"margin\":[0,5,0,0],\"sizing_mode\":\"stretch_width\",\"align\":\"center\",\"label\":\"Undo\",\"icon\":{\"type\":\"object\",\"name\":\"TablerIcon\",\"id\":\"8a039656-f3bf-42fb-bd03-7bd1bb5e6ee5\",\"attributes\":{\"icon_name\":\"arrow-back\"}}}},{\"type\":\"object\",\"name\":\"panel.models.widgets.Button\",\"id\":\"e4125077-bb27-470b-8cdb-679474487c14\",\"attributes\":{\"subscribed_events\":{\"type\":\"set\",\"entries\":[\"button_click\"]},\"css_classes\":[\"solid\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"dd9b8770-149d-4ff3-b55e-dd87ee6b415a\"},{\"id\":\"2410c7c6-1024-4fdd-ae60-2235f91e04a8\"},{\"id\":\"96755a2b-0732-48a9-8fb9-3715b5ed2374\"},{\"id\":\"28ba4ebe-bcdc-4446-9b0f-9504f70d0c3f\"}],\"max_width\":90,\"max_height\":50,\"margin\":[0,5,0,0],\"sizing_mode\":\"stretch_width\",\"align\":\"center\",\"label\":\"Clear\",\"icon\":{\"type\":\"object\",\"name\":\"TablerIcon\",\"id\":\"44b19b75-23d0-419c-8d57-734a409fef5c\",\"attributes\":{\"icon_name\":\"trash\"}}}}]}}]}}],\"active_header_background\":\"\",\"button_css_classes\":[\"card-button\"],\"collapsed\":false,\"collapsible\":false,\"header_background\":\"\",\"header_color\":\"\",\"header_css_classes\":[\"chat-feed-header\"],\"hide_header\":true}},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"998c5cb4-fba9-4202-912b-ad33c910d387\",\"attributes\":{\"plot_id\":\"5712a6f3-4d6b-4925-a2b4-cbc677b0304e\",\"comm_id\":\"63ceeb6370064d7a87ac3240aef7eaf5\",\"client_comm_id\":\"ddbddff95857429e9d6c794a239bd420\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"copy_to_clipboard1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":null}]}]}};\n",
       "  var render_items = [{\"docid\":\"b8952d1d-4697-4a89-aa0f-c70ebf2c49f8\",\"roots\":{\"5712a6f3-4d6b-4925-a2b4-cbc677b0304e\":\"c3f776ed-d1bd-4869-b42d-730c8b16e30d\"},\"root_ids\":[\"5712a6f3-4d6b-4925-a2b4-cbc677b0304e\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  async function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    await Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && id_el.children[0].hasAttribute('data-root-id')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t  for (const child of root_el.children) {\n",
       "            // Ensure JupyterLab does not capture keyboard shortcuts\n",
       "            // see: https://jupyterlab.readthedocs.io/en/4.1.x/extension/notebook.html#keyboard-interaction-model\n",
       "\t    child.setAttribute('data-lm-suppress-shortcuts', 'true')\n",
       "\t  }\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined)\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ],
      "text/plain": [
       "ChatInterface(_button_data={'send': _ChatButtonData(i...}, _buttons={'send': Button(align='cen...}, _input_container=Row, _input_layout=Row, _placeholder=ChatMessage, _widgets={'ChatAreaInput': ChatArea...}, callback=<function run_mistral a..., callback_user='Mistral', show_button_name=True, sizing_mode='stretch_width', widgets=[ChatAreaInput(css_classes...])"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "5712a6f3-4d6b-4925-a2b4-cbc677b0304e"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_interface = pn.chat.ChatInterface(\n",
    "    callback=run_mistral, \n",
    "    callback_user=\"Mistral\"\n",
    ")\n",
    "\n",
    "chat_interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "client = MistralClient(api_key=\"ImsUHfFLA6OjlX6mARbnM1YcDOy7ujsq\")\n",
    "\n",
    "prompt = \"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{retrieved_chunk}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "def get_text_embedding(input):\n",
    "    embeddings_batch_response = client.embeddings(model=\"mistral-embed\", input=input)\n",
    "    return embeddings_batch_response.data[0].embedding\n",
    "\n",
    "def run_mistral(user_message, model=\"mistral-large-latest\"):\n",
    "    messages = [ChatMessage(role=\"user\", content=user_message)]\n",
    "    chat_response = client.chat(model=model, messages=messages)\n",
    "    return chat_response.choices[0].message.content\n",
    "\n",
    "def answer_question(question, index):\n",
    "    # create embeddings for a question\n",
    "    question_embeddings = np.array([get_text_embedding(question)])\n",
    "    # retrieve similar chunks from the vector database\n",
    "    D, I = index.search(question_embeddings, k=2)\n",
    "    retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
    "    # generate response based on the retrieved relevant text chunks\n",
    "    response = run_mistral(\n",
    "        prompt.format(retrieved_chunk=retrieved_chunk, question=question)\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='100e683e-602a-479d-b71c-d53532d773e0'>\n",
       "  <div id=\"d9b4d490-97ce-4e43-be2e-fa1b1502bb63\" data-root-id=\"100e683e-602a-479d-b71c-d53532d773e0\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"688af9f8-6b64-4465-8732-7cb4d8be192b\":{\"version\":\"3.4.1\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.layout.Card\",\"id\":\"100e683e-602a-479d-b71c-d53532d773e0\",\"attributes\":{\"name\":\"Card00207\",\"css_classes\":[\"chat-interface\"],\"styles\":{\"type\":\"map\",\"entries\":[[\"padding\",\"0px\"]]},\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.4.3/dist/css/loading.css\"}},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"b095b32e-1391-424b-9909-d97c62a62738\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.4.3/dist/css/listpanel.css\"}},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.4.3/dist/bundled/theme/default.css\"}},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.4.3/dist/bundled/theme/native.css\"}},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"3216c4d4-528b-4ad6-955e-562cb0a9e941\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.4.3/dist/css/chat_interface.css\"}}],\"min_width\":45,\"margin\":5,\"sizing_mode\":\"stretch_both\",\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"Row\",\"id\":\"b7ea297d-b845-43e9-9bc6-0b29d6d8c01b\",\"attributes\":{\"name\":\"Row00206\",\"css_classes\":[\"card-header-row\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"b095b32e-1391-424b-9909-d97c62a62738\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"}],\"margin\":0,\"sizing_mode\":\"stretch_width\",\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"panel.models.markup.HTML\",\"id\":\"90f106b0-fcf2-4211-9975-bfa602166cf5\",\"attributes\":{\"css_classes\":[\"chat-feed-title\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"}],\"margin\":[5,0],\"align\":\"start\",\"text\":\"&amp;#8203;\",\"disable_math\":true}}]}},{\"type\":\"object\",\"name\":\"panel.models.feed.Feed\",\"id\":\"6d8345d7-a3fe-4734-bf51-6212e51a65e1\",\"attributes\":{\"name\":\"Feed00204\",\"subscribed_events\":{\"type\":\"set\",\"entries\":[\"scroll_button_click\"]},\"css_classes\":[\"chat-feed-log\",\"scroll-vertical\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"b095b32e-1391-424b-9909-d97c62a62738\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"},{\"id\":\"3216c4d4-528b-4ad6-955e-562cb0a9e941\"}],\"min_width\":35,\"margin\":0,\"sizing_mode\":\"stretch_width\",\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"Row\",\"id\":\"d4dcd747-56bb-4357-b139-1636e93da1f1\",\"attributes\":{\"name\":\"Row00251\",\"css_classes\":[\"chat-message\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"b095b32e-1391-424b-9909-d97c62a62738\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"5dfa2ea9-0af6-4a63-835d-62ee72ec6c60\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.4.3/dist/css/chat_message.css\"}}],\"min_width\":35,\"max_width\":1200,\"margin\":[5,10],\"sizing_mode\":\"stretch_width\",\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"panel.models.layout.Column\",\"id\":\"1b3d4ebc-ffbe-41a3-b796-2f148d6b2450\",\"attributes\":{\"name\":\"Column00240\",\"css_classes\":[\"left\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"b095b32e-1391-424b-9909-d97c62a62738\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"},{\"id\":\"5dfa2ea9-0af6-4a63-835d-62ee72ec6c60\"}],\"height\":100,\"min_height\":100,\"max_width\":60,\"margin\":0,\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"panel.models.markup.HTML\",\"id\":\"04dd8df0-e57e-43e3-929f-aa8c413d5e37\",\"attributes\":{\"css_classes\":[\"avatar\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"}],\"margin\":[5,10],\"align\":\"start\",\"text\":\"\\u2699\\ufe0f\",\"disable_math\":true}}]}},{\"type\":\"object\",\"name\":\"panel.models.layout.Column\",\"id\":\"5d3e9929-c8f0-42ab-8594-52902167215b\",\"attributes\":{\"name\":\"Column00250\",\"css_classes\":[\"right\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"b095b32e-1391-424b-9909-d97c62a62738\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"},{\"id\":\"5dfa2ea9-0af6-4a63-835d-62ee72ec6c60\"}],\"min_width\":15,\"margin\":0,\"sizing_mode\":\"stretch_width\",\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"Row\",\"id\":\"d92cc6ac-aebd-48cc-878a-b6fb462f29af\",\"attributes\":{\"name\":\"Row00246\",\"css_classes\":[\"header\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"b095b32e-1391-424b-9909-d97c62a62738\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"},{\"id\":\"5dfa2ea9-0af6-4a63-835d-62ee72ec6c60\"}],\"min_width\":15,\"margin\":0,\"sizing_mode\":\"stretch_width\",\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"panel.models.markup.HTML\",\"id\":\"26cdba21-7df8-4983-a374-172ed81bdc8e\",\"attributes\":{\"css_classes\":[\"name\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"},{\"id\":\"5dfa2ea9-0af6-4a63-835d-62ee72ec6c60\"}],\"height\":20,\"min_height\":20,\"margin\":[5,10],\"align\":\"start\",\"text\":\"System\",\"disable_math\":true}},{\"type\":\"object\",\"name\":\"panel.models.reactive_html.ReactiveHTML\",\"id\":\"9794155e-9cf5-4f2d-bf8f-692199ae78ad\",\"attributes\":{\"subscribed_events\":{\"type\":\"set\",\"entries\":[\"dom_event\"]},\"css_classes\":[\"copy-icon\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"35ecd894-4ac9-4635-8501-3974820a4549\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.4.3/dist/css/chat_copy_icon.css\"}},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"},{\"id\":\"5dfa2ea9-0af6-4a63-835d-62ee72ec6c60\"}],\"width\":15,\"height\":15,\"margin\":0,\"sizing_mode\":\"fixed\",\"align\":\"start\",\"attrs\":{\"type\":\"map\",\"entries\":[[\"copy-button\",[[\"onclick\",[],\"{script('copy_to_clipboard')}\"]]],[\"copy-icon\",[[\"fill\",[\"fill\"],\"{fill}\"]]]]},\"callbacks\":{\"type\":\"map\",\"entries\":[[\"copy-button\",[[\"onclick\",\"script('copy_to_clipboard')\"]]]]},\"data\":{\"type\":\"object\",\"name\":\"copy_to_clipboard1\",\"id\":\"e1649bc1-3c68-49f9-940b-f1bb754d9e71\",\"attributes\":{\"name\":\"ChatCopyIcon00234\",\"value\":\"Send a message to get a reply from Mistral!\"}},\"html\":\"\\n&lt;div\\n    type=&quot;button&quot;\\n    id=&quot;copy-button-${id}&quot;\\n    onclick=&quot;${script(&#x27;copy_to_clipboard&#x27;)}&quot;\\n    style=&quot;cursor: pointer; width: ${model.width}px; height: ${model.height}px;&quot;\\n    title=&quot;Copy message to clipboard&quot;\\n&gt;\\n    &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; class=&quot;icon icon-tabler icon-tabler-copy&quot; id=&quot;copy-icon-${id}&quot;\\n        width=&quot;${model.width}&quot; height=&quot;${model.height}&quot; viewBox=&quot;0 0 24 24&quot;\\n        stroke-width=&quot;2&quot; stroke=&quot;currentColor&quot; fill=${fill} stroke-linecap=&quot;round&quot; stroke-linejoin=&quot;round&quot;\\n    &gt;\\n        &lt;path stroke=&quot;none&quot; d=&quot;M0 0h24v24H0z&quot; fill=&quot;none&quot;&gt;&lt;/path&gt;\\n        &lt;path d=&quot;M8 8m0 2a2 2 0 0 1 2 -2h8a2 2 0 0 1 2 2v8a2 2 0 0 1 -2 2h-8a2 2 0 0 1 -2 -2z&quot;&gt;&lt;/path&gt;\\n        &lt;path d=&quot;M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2&quot;&gt;&lt;/path&gt;\\n    &lt;/svg&gt;\\n&lt;/div&gt;\\n\",\"nodes\":[\"copy-button\",\"copy-icon\"],\"scripts\":{\"type\":\"map\",\"entries\":[[\"copy_to_clipboard\",[\"navigator.clipboard.writeText(`${data.value}`);\\ndata.fill = &quot;currentColor&quot;;\\nsetTimeout(() =&gt; data.fill = &quot;none&quot;, 50);\"]]]}}},{\"type\":\"object\",\"name\":\"panel.models.markup.HTML\",\"id\":\"56cbe2e5-0974-48ae-afe3-93c29381a6e7\",\"attributes\":{\"visible\":false,\"css_classes\":[\"activity-dot\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"},{\"id\":\"5dfa2ea9-0af6-4a63-835d-62ee72ec6c60\"}],\"margin\":[5,10],\"align\":\"start\",\"text\":\"\\u25cf\",\"disable_math\":true}}]}},{\"type\":\"object\",\"name\":\"Row\",\"id\":\"d767a8bf-9989-4768-b046-4b25669017f1\",\"attributes\":{\"name\":\"Row00243\",\"css_classes\":[\"center\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"b095b32e-1391-424b-9909-d97c62a62738\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"},{\"id\":\"5dfa2ea9-0af6-4a63-835d-62ee72ec6c60\"}],\"margin\":0,\"sizing_mode\":\"stretch_width\",\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"panel.models.markup.HTML\",\"id\":\"13d3727b-5dc5-4f96-8fe5-0a606fcbf8c1\",\"attributes\":{\"css_classes\":[\"markdown\",\"message\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"83a93052-bbe2-4e89-a3bf-308f8c811905\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.4.3/dist/css/markdown.css\"}},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"},{\"id\":\"5dfa2ea9-0af6-4a63-835d-62ee72ec6c60\"}],\"margin\":[5,10],\"align\":\"start\",\"text\":\"&lt;p&gt;Send a message to get a reply from Mistral!&lt;/p&gt;\\n\"}},{\"type\":\"object\",\"name\":\"panel.models.layout.Column\",\"id\":\"19b2d6bc-18d8-46dd-8dd5-b8967436f48f\",\"attributes\":{\"name\":\"Column00230\",\"css_classes\":[\"reaction-icons\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"b095b32e-1391-424b-9909-d97c62a62738\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"},{\"id\":\"5dfa2ea9-0af6-4a63-835d-62ee72ec6c60\"}],\"width\":15,\"height\":15,\"margin\":[5,10],\"sizing_mode\":\"stretch_width\",\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"panel.models.icon.ToggleIcon\",\"id\":\"16d38dc0-6727-4584-a5a3-5fe0ac2260a5\",\"attributes\":{\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"7a1ecdef-b364-46c9-88b9-f0f2f0a672ae\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.4.3/dist/css/icon.css\"}},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"}],\"margin\":0,\"align\":\"start\"}}]}}]}},{\"type\":\"object\",\"name\":\"panel.models.layout.Column\",\"id\":\"4c3fa839-b19d-4e30-b1a6-f50ab5445653\",\"attributes\":{\"name\":\"Column00249\",\"css_classes\":[\"footer\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"b095b32e-1391-424b-9909-d97c62a62738\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"},{\"id\":\"5dfa2ea9-0af6-4a63-835d-62ee72ec6c60\"}],\"margin\":0,\"sizing_mode\":\"stretch_width\",\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"panel.models.markup.HTML\",\"id\":\"80afcc42-08b1-4caa-a5dd-1fc31f421120\",\"attributes\":{\"css_classes\":[\"timestamp\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"}],\"margin\":[5,10],\"align\":\"start\",\"text\":\"19:45\",\"disable_math\":true}}]}}]}}]}}],\"auto_scroll_limit\":200,\"scroll_button_threshold\":100,\"view_latest\":true}},{\"type\":\"object\",\"name\":\"Spacer\",\"id\":\"19f7463c-89e5-479f-ac8c-e42fa4ea010a\",\"attributes\":{\"name\":\"VSpacer00205\",\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"}],\"margin\":0,\"sizing_mode\":\"stretch_height\",\"align\":\"start\"}},{\"type\":\"object\",\"name\":\"Row\",\"id\":\"0fc7940d-a3e3-44b5-afcc-fb662b4f94a5\",\"attributes\":{\"name\":\"Row00210\",\"css_classes\":[\"chat-interface-input-container\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"b095b32e-1391-424b-9909-d97c62a62738\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"},{\"id\":\"3216c4d4-528b-4ad6-955e-562cb0a9e941\"}],\"margin\":0,\"sizing_mode\":\"stretch_width\",\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"Row\",\"id\":\"140bbfbc-8a0d-4487-b18e-ab11797c5995\",\"attributes\":{\"name\":\"Row00227\",\"css_classes\":[\"chat-interface-input-row\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"b095b32e-1391-424b-9909-d97c62a62738\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"},{\"id\":\"3216c4d4-528b-4ad6-955e-562cb0a9e941\"}],\"margin\":0,\"sizing_mode\":\"stretch_width\",\"align\":\"start\",\"children\":[{\"type\":\"object\",\"name\":\"panel.models.chatarea_input.ChatAreaInput\",\"id\":\"51e3d0da-bbed-4a13-b306-a4b10627b469\",\"attributes\":{\"subscribed_events\":{\"type\":\"set\",\"entries\":[\"chat_message_event\"]},\"css_classes\":[\"chat-interface-input-widget\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"}],\"width\":300,\"margin\":[5,10],\"sizing_mode\":\"stretch_width\",\"align\":\"start\",\"resizable\":\"height\",\"placeholder\":\"Send a message\",\"max_length\":5000,\"rows\":1,\"auto_grow\":true,\"max_rows\":10}},{\"type\":\"object\",\"name\":\"panel.models.widgets.Button\",\"id\":\"cc4866c7-3a16-4d77-9a06-a73f15fc708b\",\"attributes\":{\"subscribed_events\":{\"type\":\"set\",\"entries\":[\"button_click\"]},\"css_classes\":[\"solid\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"type\":\"object\",\"name\":\"ImportedStyleSheet\",\"id\":\"8cbf0f8e-8022-4f08-bb44-8bf0735a33a5\",\"attributes\":{\"url\":\"https://cdn.holoviz.org/panel/1.4.3/dist/css/button.css\"}},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"}],\"max_width\":90,\"max_height\":50,\"margin\":[0,5,0,0],\"sizing_mode\":\"stretch_width\",\"align\":\"center\",\"label\":\"Send\",\"icon\":{\"type\":\"object\",\"name\":\"TablerIcon\",\"id\":\"43ab28bd-3bba-4251-9faa-c751bce2c9b9\",\"attributes\":{\"icon_name\":\"send\"}}}},{\"type\":\"object\",\"name\":\"panel.models.widgets.Button\",\"id\":\"c2301800-8594-434a-bc2a-0ddbbea7f6cc\",\"attributes\":{\"subscribed_events\":{\"type\":\"set\",\"entries\":[\"button_click\"]},\"visible\":false,\"css_classes\":[\"solid\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"8cbf0f8e-8022-4f08-bb44-8bf0735a33a5\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"}],\"max_width\":90,\"max_height\":50,\"margin\":[0,5,0,0],\"sizing_mode\":\"stretch_width\",\"align\":\"center\",\"label\":\"Stop\",\"icon\":{\"type\":\"object\",\"name\":\"TablerIcon\",\"id\":\"c7de2948-acb4-4eab-a4f0-c376841c762a\",\"attributes\":{\"icon_name\":\"player-stop\"}}}},{\"type\":\"object\",\"name\":\"panel.models.widgets.Button\",\"id\":\"6b858e68-e020-4bf7-ac89-17fbd50ea844\",\"attributes\":{\"subscribed_events\":{\"type\":\"set\",\"entries\":[\"button_click\"]},\"css_classes\":[\"solid\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"8cbf0f8e-8022-4f08-bb44-8bf0735a33a5\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"}],\"max_width\":90,\"max_height\":50,\"margin\":[0,5,0,0],\"sizing_mode\":\"stretch_width\",\"align\":\"center\",\"label\":\"Rerun\",\"icon\":{\"type\":\"object\",\"name\":\"TablerIcon\",\"id\":\"b69960ee-fdea-46e2-8e13-4ad2256f2171\",\"attributes\":{\"icon_name\":\"repeat\"}}}},{\"type\":\"object\",\"name\":\"panel.models.widgets.Button\",\"id\":\"f3e8ee4e-1dee-4bdf-b105-2d535748a859\",\"attributes\":{\"subscribed_events\":{\"type\":\"set\",\"entries\":[\"button_click\"]},\"css_classes\":[\"solid\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"8cbf0f8e-8022-4f08-bb44-8bf0735a33a5\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"}],\"max_width\":90,\"max_height\":50,\"margin\":[0,5,0,0],\"sizing_mode\":\"stretch_width\",\"align\":\"center\",\"label\":\"Undo\",\"icon\":{\"type\":\"object\",\"name\":\"TablerIcon\",\"id\":\"e4a1e739-ed37-4e79-8332-670e96e49ee3\",\"attributes\":{\"icon_name\":\"arrow-back\"}}}},{\"type\":\"object\",\"name\":\"panel.models.widgets.Button\",\"id\":\"1cc47c9d-1d1c-4229-bfde-cf2a48043768\",\"attributes\":{\"subscribed_events\":{\"type\":\"set\",\"entries\":[\"button_click\"]},\"css_classes\":[\"solid\"],\"stylesheets\":[\"\\n:host(.pn-loading):before, .pn-loading:before {\\n  background-color: #c3c3c3;\\n  mask-size: auto calc(min(50%, 400px));\\n  -webkit-mask-size: auto calc(min(50%, 400px));\\n}\",{\"id\":\"a1e938b3-7a13-45bf-9115-190ce31ec8c5\"},{\"id\":\"8cbf0f8e-8022-4f08-bb44-8bf0735a33a5\"},{\"id\":\"2d11c933-254d-42e3-8d96-b0610d687e79\"},{\"id\":\"f2b829f5-a254-4e54-9584-14c91a27f650\"}],\"max_width\":90,\"max_height\":50,\"margin\":[0,5,0,0],\"sizing_mode\":\"stretch_width\",\"align\":\"center\",\"label\":\"Clear\",\"icon\":{\"type\":\"object\",\"name\":\"TablerIcon\",\"id\":\"8e7f1514-871c-4c94-a72f-823ff8026b0a\",\"attributes\":{\"icon_name\":\"trash\"}}}}]}}]}}],\"active_header_background\":\"\",\"button_css_classes\":[\"card-button\"],\"collapsed\":false,\"collapsible\":false,\"header_background\":\"\",\"header_color\":\"\",\"header_css_classes\":[\"chat-feed-header\"],\"hide_header\":true}},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"a035a8e8-875a-4e7b-9835-e104a344c7f2\",\"attributes\":{\"plot_id\":\"100e683e-602a-479d-b71c-d53532d773e0\",\"comm_id\":\"8a35e5580c8243d485b4b70186ee7f9e\",\"client_comm_id\":\"4c0fa89db55d47eb8129501d8c9625dc\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"copy_to_clipboard1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":null}]}]}};\n",
       "  var render_items = [{\"docid\":\"688af9f8-6b64-4465-8732-7cb4d8be192b\",\"roots\":{\"100e683e-602a-479d-b71c-d53532d773e0\":\"d9b4d490-97ce-4e43-be2e-fa1b1502bb63\"},\"root_ids\":[\"100e683e-602a-479d-b71c-d53532d773e0\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  async function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    await Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && id_el.children[0].hasAttribute('data-root-id')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t  for (const child of root_el.children) {\n",
       "            // Ensure JupyterLab does not capture keyboard shortcuts\n",
       "            // see: https://jupyterlab.readthedocs.io/en/4.1.x/extension/notebook.html#keyboard-interaction-model\n",
       "\t    child.setAttribute('data-lm-suppress-shortcuts', 'true')\n",
       "\t  }\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined)\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ],
      "text/plain": [
       "ChatInterface(_button_data={'send': _ChatButtonData(i...}, _buttons={'send': Button(align='cen...}, _input_container=Row, _input_layout=Row, _placeholder=ChatMessage, _widgets={'ChatAreaInput': ChatArea...}, callback=<function answer_question ..., callback_user='Mistral', show_button_name=True, sizing_mode='stretch_width', widgets=[ChatAreaInput(css_classes...])\n",
       "    [0] ChatMessage(str, avatar='⚙️', reaction_icons=ChatReactionIcons, timestamp=datetime.datetime(2024, ..., user='System')"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "100e683e-602a-479d-b71c-d53532d773e0"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_interface = pn.chat.ChatInterface(\n",
    "    callback=answer_question,\n",
    "    callback_user=\"Mistral\",\n",
    ")\n",
    "chat_interface.send(\n",
    "    \"Send a message to get a reply from Mistral!\", \n",
    "    user=\"System\", \n",
    "    respond=False\n",
    ")\n",
    "chat_interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import param\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "# Replace with your actual API key\n",
    "client = MistralClient(api_key=\"ImsUHfFLA6OjlX6mARbnM1YcDOy7ujsq\")\n",
    "\n",
    "def get_text_embedding(txt):\n",
    "    client = MistralClient(api_key=api_key)\n",
    "    embeddings_batch_response = client.embeddings(model=\"mistral-embed\", input=txt)\n",
    "    return embeddings_batch_response.data[0].embedding\n",
    "\n",
    "class cbfs(param.Parameterized):\n",
    "    chat_history = param.List(default=[])\n",
    "    answer = param.String(default=\"\")\n",
    "    \n",
    "    def __init__(self, **params):\n",
    "        super(cbfs, self).__init__(**params)\n",
    "    \n",
    "    def convchain(self, query):\n",
    "        if not query:\n",
    "            return\n",
    "        question_embeddings = np.array([get_text_embedding(query)])\n",
    "        D, I = index.search(question_embeddings, k=2)\n",
    "        retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Context information is below.\n",
    "        ---------------------\n",
    "        {retrieved_chunk}\n",
    "        ---------------------\n",
    "        Given the context information and not prior knowledge, answer the query.\n",
    "        Query: {query}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "        messages = [ChatMessage(role=\"user\", content=prompt)]\n",
    "        chat_response = client.chat(model=\"mistral-small-latest\", messages=messages)\n",
    "        answer = chat_response.choices[0].message.content\n",
    "\n",
    "        self.chat_history.append((query, answer))\n",
    "        self.answer = answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatInterface:\n",
    "    def __init__(self, cbfs_instance):\n",
    "        self.cbfs_instance = cbfs_instance\n",
    "        self.user_input = widgets.Text(\n",
    "            placeholder='Type your question here...',\n",
    "            description='You:',\n",
    "            disabled=False\n",
    "        )\n",
    "        self.send_button = widgets.Button(description='Send')\n",
    "        self.chat_output = widgets.Output()\n",
    "\n",
    "        self.send_button.on_click(self.on_send)\n",
    "        self.user_input.on_submit(self.on_send_enter)  # Use on_submit for the enter key event\n",
    "        display(widgets.VBox([self.user_input, self.send_button, self.chat_output]))\n",
    "\n",
    "    def on_send(self, b):\n",
    "        self.process_user_input()\n",
    "\n",
    "    def on_send_enter(self, widget):  # Handler for the Enter key\n",
    "        self.process_user_input()\n",
    "\n",
    "    def process_user_input(self):\n",
    "        user_query = self.user_input.value.strip()\n",
    "        if user_query:\n",
    "            self.cbfs_instance.convchain(user_query)\n",
    "            with self.chat_output:\n",
    "                self.display_chat_history()\n",
    "            self.user_input.value = ''  # Clear input after sending\n",
    "\n",
    "    def display_chat_history(self):\n",
    "        self.chat_output.clear_output()\n",
    "        with self.chat_output:\n",
    "            for query, response in self.cbfs_instance.chat_history:\n",
    "                print(f'User: {query}')\n",
    "                print(f'AI: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30224/2273792851.py:13: DeprecationWarning: on_submit is deprecated. Instead, set the .continuous_update attribute to False and observe the value changing with: mywidget.observe(callback, 'value').\n",
      "  self.user_input.on_submit(self.on_send_enter)  # Use on_submit for the enter key event\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915643372afc4aeebb44ab49382e9012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='', description='You:', placeholder='Type your question here...'), Button(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize and display the chat interface with a cbfs instance\n",
    "cb_instance = cbfs()\n",
    "chat_interface = ChatInterface(cb_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models are crucial in today's digital landscape, and their importance can be summarized in the following ways:\n",
      "\n",
      "1. **Efficient Information Processing**: Fast language models enable swift processing of large amounts of text data, allowing for rapid analysis, generation, and summarization of information. This efficiency is vital in various applications, such as search engines, chatbots, and language translation systems.\n",
      "\n",
      "2. **Improved User Experience**: By providing instant responses, fast language models enhance user experience in applications like virtual assistants, customer support systems, and language translation apps, leading to increased user satisfaction and engagement.\n",
      "\n",
      "3. **Enhanced Productivity**: With fast language models, professionals can quickly analyze and generate text, boosting productivity in tasks like content creation, editing, and research. This acceleration of workflows can lead to significant time and cost savings.\n",
      "\n",
      "4. **Real-time Insights and Decision-Making**: Fast language models facilitate the rapid extraction of insights from vast amounts of text data, enabling businesses and organizations to make informed, data-driven decisions in a timely manner.\n",
      "\n",
      "5. **Competitive Advantage**: Organizations that leverage fast language models can gain a competitive edge in their respective industries, as they can respond more quickly to changing market conditions, customer needs, and trends.\n",
      "\n",
      "6. **Democratization of AI**: Fast language models can make AI technology more accessible to a broader range of developers, researchers, and organizations, promoting innovation and advancements in natural language processing.\n",
      "\n",
      "7. **Multilingual Support**: Fast language models can be trained on multiple languages, facilitating global communication, cultural exchange, and international business collaboration.\n",
      "\n",
      "8. **Improving Accessibility**: For individuals with disabilities, fast language models can enable more efficient communication, such as speedy language translation, text-to-speech synthesis, and speech-to-text recognition.\n",
      "\n",
      "9. **Cybersecurity and Threat Detection**: Fast language models can help detect and respond to cyber threats, such as phishing attacks, spam, and malware, by quickly analyzing large volumes of data.\n",
      "\n",
      "10. **Advancements in Research and Development**: Fast language models can accelerate research in fields like natural language processing, machine learning, and cognitive computing, driving innovation and breakthroughs.\n",
      "\n",
      "In summary, fast language models are essential for efficient information processing, improved user experiences, enhanced productivity, and gaining a competitive edge. They also have the potential to democratize AI, improve accessibility, and drive research and development in various fields.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "# Set the environment variable in the script\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "client = Groq(api_key=os.environ.get(\"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\"))\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-70b-8192\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
